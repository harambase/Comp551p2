Mean validation score: 0.7051442632468735
std: 0.006384755312844335
Parameters: {'clf__base_estimator': MultinomialNB(alpha=1.0, class_prior=None, fit_prior=True), 'clf__bootstrap': False, 'clf__bootstrap_features': False, 'clf__max_features': 0.9120442729722144, 'clf__max_samples': 0.7892022585496922, 'clf__n_estimators': 209, 'clf__oob_score': False, 'clf__warm_start': False, 'vect__max_df': 0.6333333333333333, 'vect__min_df': 0.00011618768297197917, 'vect__ngram_range': (1, 2), 'vect__stop_words': 'english', 'vect__token_pattern': '\\w{2,}'}
accruracy = 0.6699415825809878

{'clf__base_estimator': MultinomialNB(alpha=1.0, class_prior=None, fit_prior=True), 'clf__bootstrap': False, 'clf__bootstrap_features': False, 'clf__max_features': 0.9120442729722144, 'clf__max_samples': 0.7892022585496922, 'clf__n_estimators': 209, 'clf__oob_score': False, 'clf__warm_start': False, 'vect__max_df': 0.6333333333333333, 'vect__min_df': 0.00011618768297197917, 'vect__ngram_range': (1, 2), 'vect__stop_words': 'english', 'vect__token_pattern': '\\w{2,}'}

{'mean_fit_time': array([1.14463878e+00, 1.14639702e+00, 4.09430512e+01, 1.35936852e+00,
       7.15246806e+00, 5.56860500e+02, 7.23373951e+01, 1.18992126e+01,
       1.32209101e+01, 1.05764604e+01, 7.08542004e+00, 8.68748164e+00,
       5.50263513e+02, 2.86877618e+00, 2.13493195e+00, 8.32956591e+00,
       1.13521082e+03, 8.64060383e+00, 8.88059721e+00, 3.19260850e+00,
       1.16165459e+01, 7.60357975e+01, 1.09848490e+01, 9.50361266e+00,
       2.13406119e+00, 7.38799688e+02, 9.67975059e+00, 8.57699285e+00,
       7.70928675e+01, 1.20783525e+01, 2.17535381e+00, 2.43482566e+00,
       2.48862329e+00, 2.27916961e+00, 2.17812376e+00, 2.93068171e+02,
       3.14155397e+00, 3.13902402e+00, 4.81225396e+01, 9.79271922e+00,
       2.40791316e+00, 8.26904655e+00, 6.72705940e+02, 8.82596984e+00,
       1.12357642e+01, 2.49727998e+00, 8.30782681e+00, 1.53500528e+00,
       1.47131839e+00, 5.68895674e+00, 6.46891704e+00, 3.10810400e+03,
       1.09035042e+01, 2.58949938e+00, 2.21587181e+00, 6.84867082e+00,
       2.42821941e+00, 2.53801012e+00, 2.53217130e+00, 2.39096637e+00,
       2.67703815e+00, 8.91325827e+00, 1.03875253e+01, 3.37169646e+02,
       2.89439569e+00, 1.04455534e+01, 9.65527759e+00, 3.08014874e+00,
       9.31109462e+00, 1.57545612e+01, 2.20850110e+00, 2.10494242e+00,
       7.66517162e+00, 3.74992290e+01, 2.98708477e+00, 9.98016095e+00,
       8.50651693e+00, 3.37091541e+00, 4.77701497e+00, 1.47075195e+00,
       2.74871373e+00, 5.52681222e+00, 1.66226174e+02, 7.27027144e+00,
       2.64635286e+00, 6.91003504e+00, 7.24519854e+00, 1.54130845e+00,
       6.90824060e+00, 2.65693308e+01, 1.06090291e+01, 1.21149130e+01,
       1.54184168e+01, 5.38383493e+00, 5.61821156e+00, 1.62625542e+00,
       4.72209554e+00, 1.45556226e+00, 6.07224584e+00, 6.76943614e+02,
       7.59670005e+00, 9.12086315e+00, 6.96846509e+00, 2.72250652e+00,
       2.34854207e+00, 1.00867917e+01, 7.28504176e+00, 2.32140245e+00,
       2.27514114e+00, 2.23074465e+00, 2.56972876e+00, 2.51003580e+00,
       1.31722368e+01, 9.74139581e+00, 2.24443774e+00, 2.11824627e+00,
       7.55648494e+00, 2.26708999e+00, 2.47402844e+00, 7.42370138e+00,
       2.57290606e+00, 5.76239338e+00, 1.44273233e+00, 6.14734187e+00,
       6.59445310e+00, 6.05488687e+00, 5.92176070e+00, 5.86168656e+00,
       1.37615752e+00, 1.48505197e+00, 1.58003454e+00, 8.13689547e+00,
       1.47823057e+00, 5.81982327e+00, 1.74149766e+00, 6.24751267e+00,
       7.01004372e+00, 2.55074615e+00, 1.54229803e+00, 5.68416320e+02,
       2.58367243e+00, 2.88931665e+00, 6.92353492e+00, 9.15014906e+00,
       9.34865780e+00, 1.33586217e+01, 3.24588184e+00, 1.41221742e+01,
       8.57716393e+00, 8.33948345e+00, 8.14115605e+00, 2.53066611e+00,
       2.29868259e+00, 8.36217470e+00, 7.24721627e+00, 8.36526933e+00,
       8.08690758e+00, 2.96905298e+00, 2.22510781e+00, 1.02339022e+01,
       2.59555063e+00, 8.42648635e+00, 8.19444656e+00, 9.86334710e+00,
       2.26663032e+00, 2.39514399e+00, 9.50776205e+00, 2.28680186e+00,
       2.24610701e+00, 7.10336003e+00, 1.41857185e+00, 3.12682743e+02,
       2.52599626e+00, 2.91484666e+00, 8.84429293e+00, 7.15426326e+00,
       8.69772329e+00, 8.45137758e+00, 2.58151708e+00, 2.34931822e+00,
       2.32577767e+00, 2.86651273e+00, 8.64905629e+00, 9.60143476e+00,
       9.12744722e+00, 8.63709698e+00, 2.94821520e+00, 5.30382738e+00,
       6.83694243e+00, 5.67016273e+00, 1.17407703e+00, 1.04610605e+00,
       3.72375960e+00, 5.00236724e+01, 1.23942599e+00, 1.37758341e+00,
       1.37133250e+00, 4.09274983e+00, 8.23728223e+00, 4.40480452e+00]), 'std_fit_time': array([1.90177368e-02, 6.28695075e-02, 6.25360569e-01, 1.18130623e-01,
       3.87905095e-01, 3.05153417e+00, 1.55282677e+00, 8.69642338e-01,
       2.88439904e-01, 1.02624852e+00, 2.26262650e+00, 7.56786429e-01,
       5.68334751e+00, 1.22390921e-01, 1.06802456e-01, 9.15592242e-01,
       1.27854319e+01, 1.90866209e-01, 2.96825496e-01, 4.36219096e-02,
       1.83711554e-01, 6.23827633e-01, 1.86624591e-01, 1.07343191e+00,
       1.70356991e-01, 5.68232951e+00, 1.17890638e-01, 3.36623896e-01,
       6.17659815e-01, 2.27882530e+00, 5.41311823e-02, 4.44373757e-02,
       1.54394720e-01, 5.55816418e-02, 1.44875961e-01, 2.40961660e+00,
       6.26646605e-02, 7.31183258e-02, 1.35014098e+00, 2.56218300e-01,
       3.39284457e-01, 6.89827605e-01, 7.47833282e+00, 4.25481350e-01,
       2.49184755e-01, 2.70126152e-01, 2.93836524e-01, 2.45669864e-01,
       3.78972318e-02, 2.41107306e-01, 1.72373145e+00, 2.98942262e+01,
       1.06103607e-01, 3.36241194e-01, 6.69948367e-02, 3.36191513e-01,
       1.08051353e-01, 8.31128051e-02, 3.13103545e-02, 1.12875641e-01,
       7.98523634e-02, 6.36735893e-01, 4.52174496e-01, 1.02471514e+00,
       4.66493576e-02, 2.78020432e-01, 6.36235306e-02, 1.84662840e-02,
       2.05041182e-01, 9.43744029e-02, 5.33982807e-01, 6.74190720e-02,
       1.09568411e+00, 3.47214466e-01, 4.49830088e-02, 1.34331823e-01,
       3.43241723e-01, 8.28095143e-02, 1.84592732e-01, 5.32976737e-02,
       1.54728306e-01, 1.05912796e+00, 2.02610462e+00, 3.65746069e-01,
       4.76621954e-02, 2.35581305e-01, 3.89106857e-01, 1.72304746e-01,
       2.15070241e+00, 8.24401226e-01, 2.69845569e-01, 2.22582222e-01,
       9.22574418e-01, 1.44668875e-01, 2.30987491e-01, 1.31199934e-01,
       6.72363047e-02, 3.02908948e-02, 8.67868551e-01, 3.98409281e+00,
       2.77600367e-01, 1.26643785e-01, 4.18762186e-01, 8.66862459e-02,
       1.68997519e-01, 1.68585539e-01, 1.28844804e-01, 5.21043177e-02,
       6.42286707e-02, 7.12859498e-02, 9.37708933e-02, 1.20909336e-01,
       3.14927405e-01, 4.37114492e-01, 8.13728174e-02, 9.80464066e-02,
       1.43864375e-01, 1.20771579e-01, 9.40296159e-02, 2.93511169e-01,
       5.34280586e-02, 8.30221686e-01, 1.93024717e-02, 4.00655441e-01,
       3.65069788e-01, 3.89701224e-01, 2.56112756e-01, 1.28226658e-01,
       2.29212110e-02, 2.24191503e-02, 9.94255644e-02, 1.23149816e-01,
       1.26086217e-01, 3.43257018e-01, 3.98622072e-01, 5.93286572e-01,
       4.42672824e-01, 7.50687357e-02, 1.96731644e-01, 5.55410699e+00,
       1.30510656e-01, 1.36357713e-01, 7.38729829e-01, 3.17442331e-01,
       2.11748237e-01, 3.08201448e-01, 6.73414710e-02, 1.44432299e-01,
       3.91752580e-01, 3.46404587e-01, 1.39701724e-01, 8.27135167e-02,
       1.17076965e-01, 1.21667070e-01, 1.15654296e+00, 2.71395948e-01,
       4.31889567e-01, 1.36677626e-01, 1.89945137e-01, 1.85290127e-01,
       1.97931661e-01, 5.98690838e-01, 2.27452707e-01, 2.51107398e-01,
       1.67063214e-01, 6.44539207e-02, 1.55480811e-01, 1.59409614e-01,
       1.34755596e-01, 2.45044005e-01, 7.62217223e-02, 2.01063177e+00,
       1.92894371e-01, 5.26658678e-02, 4.42471059e-01, 3.06308734e-01,
       1.94296501e-01, 3.42227855e-01, 5.70061308e-02, 6.32960817e-02,
       1.26131080e-01, 8.35206191e-02, 1.07332402e+00, 1.64443421e-01,
       5.79409344e-01, 1.77598467e-01, 6.86043003e-02, 5.17298687e-01,
       1.83583607e-01, 3.41788030e-01, 4.18665956e-02, 8.35955555e-03,
       1.17824911e-01, 6.07746754e-01, 4.76785124e-02, 2.57922729e-02,
       2.13717658e-02, 5.65479501e-01, 2.11962200e-01, 8.66510229e-02]), 'mean_score_time': array([ 0.        ,  0.        ,  0.85983129,  0.        ,  0.        ,
        3.50325952, 26.99702077,  0.        ,  0.        ,  0.        ,
        0.        ,  0.        ,  5.37417936,  0.        ,  0.        ,
        0.        , 10.26375761,  0.        ,  0.        ,  0.        ,
        0.        ,  3.20247011,  0.        ,  0.        ,  0.        ,
        2.74986858,  0.        ,  0.        , 10.43618765,  0.        ,
        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,
        1.98884139,  0.        ,  0.        , 12.52133427,  0.        ,
        0.        ,  0.        ,  2.3224822 ,  0.        ,  0.        ,
        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,
        0.        , 10.89046822,  0.        ,  0.        ,  0.        ,
        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,
        0.        ,  0.        ,  0.        ,  0.85098538,  0.        ,
        0.        ,  0.        ,  0.        ,  0.        ,  2.8210948 ,
        0.        ,  0.        ,  0.        ,  5.06659017,  0.        ,
        0.        ,  0.        ,  0.79042468,  0.        ,  0.        ,
        0.68930168,  0.        ,  4.86286001,  0.        ,  0.        ,
        0.        ,  0.        ,  0.        ,  0.        ,  8.32991238,
        0.        ,  0.        ,  2.26442447,  0.        ,  0.        ,
        0.        ,  0.        ,  0.        ,  0.        ,  5.13294044,
        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,
        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,
        0.        ,  0.        ,  3.54153395,  0.        ,  0.        ,
        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,
        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,
        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,
        0.        ,  0.        ,  0.        ,  1.50224409,  0.        ,
        0.        ,  0.        ,  0.        ,  0.        ,  2.23879418,
        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,
        2.28233395,  0.        ,  3.73237758,  0.        ,  0.        ,
        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,
        1.99628587,  0.        ,  0.        ,  0.        ,  0.        ,
        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,
        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,
        0.        ,  1.25839086,  0.        ,  0.        ,  0.        ,
        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,
        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,
        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,
        0.        ,  0.        ,  0.        ,  0.43632684,  0.        ,
        0.        ,  0.        ,  0.        ,  1.16030455,  0.        ]), 'std_score_time': array([0.        , 0.        , 0.04648649, 0.        , 0.        ,
       0.11494449, 1.79369871, 0.        , 0.        , 0.        ,
       0.        , 0.        , 0.46703507, 0.        , 0.        ,
       0.        , 0.27325869, 0.        , 0.        , 0.        ,
       0.        , 0.19485447, 0.        , 0.        , 0.        ,
       0.40017446, 0.        , 0.        , 0.90231822, 0.        ,
       0.        , 0.        , 0.        , 0.        , 0.        ,
       0.12750712, 0.        , 0.        , 3.15040393, 0.        ,
       0.        , 0.        , 0.19926844, 0.        , 0.        ,
       0.        , 0.        , 0.        , 0.        , 0.        ,
       0.        , 2.90735664, 0.        , 0.        , 0.        ,
       0.        , 0.        , 0.        , 0.        , 0.        ,
       0.        , 0.        , 0.        , 0.11485834, 0.        ,
       0.        , 0.        , 0.        , 0.        , 0.14886662,
       0.        , 0.        , 0.        , 0.18644153, 0.        ,
       0.        , 0.        , 0.07573564, 0.        , 0.        ,
       0.14234076, 0.        , 0.44757176, 0.        , 0.        ,
       0.        , 0.        , 0.        , 0.        , 0.23474018,
       0.        , 0.        , 0.55057598, 0.        , 0.        ,
       0.        , 0.        , 0.        , 0.        , 0.62095247,
       0.        , 0.        , 0.        , 0.        , 0.        ,
       0.        , 0.        , 0.        , 0.        , 0.        ,
       0.        , 0.        , 0.12603765, 0.        , 0.        ,
       0.        , 0.        , 0.        , 0.        , 0.        ,
       0.        , 0.        , 0.        , 0.        , 0.        ,
       0.        , 0.        , 0.        , 0.        , 0.        ,
       0.        , 0.        , 0.        , 0.18271081, 0.        ,
       0.        , 0.        , 0.        , 0.        , 0.10587684,
       0.        , 0.        , 0.        , 0.        , 0.        ,
       0.095272  , 0.        , 0.11997277, 0.        , 0.        ,
       0.        , 0.        , 0.        , 0.        , 0.        ,
       0.20425852, 0.        , 0.        , 0.        , 0.        ,
       0.        , 0.        , 0.        , 0.        , 0.        ,
       0.        , 0.        , 0.        , 0.        , 0.        ,
       0.        , 0.27686477, 0.        , 0.        , 0.        ,
       0.        , 0.        , 0.        , 0.        , 0.        ,
       0.        , 0.        , 0.        , 0.        , 0.        ,
       0.        , 0.        , 0.        , 0.        , 0.        ,
       0.        , 0.        , 0.        , 0.04890587, 0.        ,
       0.        , 0.        , 0.        , 0.08018555, 0.        ]), 'param_clf__base_estimator': masked_array(data=[Pipeline(memory=None,
         steps=[('vect',
                 TfidfVectorizer(analyzer='word', binary=False,
                                 decode_error='strict',
                                 dtype=<class 'numpy.float64'>,
                                 encoding='utf-8', input='content',
                                 lowercase=True, max_df=0.6777777777777778,
                                 max_features=None,
                                 min_df=0.0012707570261625706,
                                 ngram_range=(1, 2), norm='l2',
                                 preprocessor=None, smooth_idf=True,
                                 stop_words=None, strip_accents=None,...
                                 vocabulary=None)),
                ('clf',
                 DecisionTreeClassifier(ccp_alpha=0.0, class_weight=None,
                                        criterion='gini', max_depth=49,
                                        max_features=0.56, max_leaf_nodes=316,
                                        min_impurity_decrease=1e-07,
                                        min_impurity_split=None,
                                        min_samples_leaf=4,
                                        min_samples_split=0.015198715678453873,
                                        min_weight_fraction_leaf=0.0,
                                        presort='deprecated', random_state=0,
                                        splitter='random'))],
         verbose=False),
                   Pipeline(memory=None,
         steps=[('vect',
                 TfidfVectorizer(analyzer='word', binary=False,
                                 decode_error='strict',
                                 dtype=<class 'numpy.float64'>,
                                 encoding='utf-8', input='content',
                                 lowercase=True, max_df=0.6333333333333333,
                                 max_features=None,
                                 min_df=0.0010083260374073057,
                                 ngram_range=(1, 1), norm='l2',
                                 preprocessor=None, smooth_idf=True,
                                 stop_words='english', strip_accents=...
                                 tokenizer=None, use_idf=True,
                                 vocabulary=None)),
                ('clf',
                 AdaBoostClassifier(algorithm='SAMME',
                                    base_estimator=LinearSVC(C=1.0,
                                                             class_weight=None,
                                                             dual=True,
                                                             fit_intercept=True,
                                                             intercept_scaling=1,
                                                             loss='squared_hinge',
                                                             max_iter=1000,
                                                             multi_class='ovr',
                                                             penalty='l2',
                                                             random_state=None,
                                                             tol=0.0001,
                                                             verbose=0),
                                    learning_rate=0.40053644159285917,
                                    n_estimators=190, random_state=0))],
         verbose=False),
                   None,
                   Pipeline(memory=None,
         steps=[('vect',
                 TfidfVectorizer(analyzer='word', binary=False,
                                 decode_error='strict',
                                 dtype=<class 'numpy.float64'>,
                                 encoding='utf-8', input='content',
                                 lowercase=True, max_df=0.7666666666666666,
                                 max_features=None,
                                 min_df=0.00010572508827796376,
                                 ngram_range=(1, 2), norm='l2',
                                 preprocessor=None, smooth_idf=True,
                                 stop_words='english', strip_accents=None,
                                 sublinear_tf=False, token_pattern='\\w{1,}',
                                 tokenizer=None, use_idf=True,
                                 vocabulary=None)),
                ('clf',
                 LinearSVC(C=99.37702864845689, class_weight=None, dual=False,
                           fit_intercept=False, intercept_scaling=1,
                           loss='squared_hinge', max_iter=1000,
                           multi_class='ovr', penalty='l2', random_state=0,
                           tol=3.8359242531998225e-06, verbose=0))],
         verbose=False),
                   MultinomialNB(alpha=1.0, class_prior=None, fit_prior=True),
                   None,
                   MultinomialNB(alpha=1.0, class_prior=None, fit_prior=True),
                   Pipeline(memory=None,
         steps=[('vect',
                 TfidfVectorizer(analyzer='word', binary=False,
                                 decode_error='strict',
                                 dtype=<class 'numpy.float64'>,
                                 encoding='utf-8', input='content',
                                 lowercase=True, max_df=0.6777777777777778,
                                 max_features=None,
                                 min_df=0.0012707570261625706,
                                 ngram_range=(1, 2), norm='l2',
                                 preprocessor=None, smooth_idf=True,
                                 stop_words=None, strip_accents=None,...
                                 vocabulary=None)),
                ('clf',
                 DecisionTreeClassifier(ccp_alpha=0.0, class_weight=None,
                                        criterion='gini', max_depth=49,
                                        max_features=0.56, max_leaf_nodes=316,
                                        min_impurity_decrease=1e-07,
                                        min_impurity_split=None,
                                        min_samples_leaf=4,
                                        min_samples_split=0.015198715678453873,
                                        min_weight_fraction_leaf=0.0,
                                        presort='deprecated', random_state=0,
                                        splitter='random'))],
         verbose=False),
                   Pipeline(memory=None,
         steps=[('vect',
                 TfidfVectorizer(analyzer='word', binary=False,
                                 decode_error='strict',
                                 dtype=<class 'numpy.float64'>,
                                 encoding='utf-8', input='content',
                                 lowercase=True, max_df=0.7222222222222222,
                                 max_features=None,
                                 min_df=0.00014705642569160768,
                                 ngram_range=(1, 1), norm='l2',
                                 preprocessor=None, smooth_idf=True,
                                 stop_words=None, strip_accents=None...
                                        max_features=0.01, max_leaf_nodes=None,
                                        max_samples=0.007808728196651904,
                                        min_impurity_decrease=3.1622776601683795e-10,
                                        min_impurity_split=None,
                                        min_samples_leaf=62,
                                        min_samples_split=0.0008966659727253451,
                                        min_weight_fraction_leaf=0.0007448459487372297,
                                        n_estimators=251, n_jobs=None,
                                        oob_score=False, random_state=0,
                                        verbose=True, warm_start=True))],
         verbose=False),
                   Pipeline(memory=None,
         steps=[('vect',
                 TfidfVectorizer(analyzer='word', binary=False,
                                 decode_error='strict',
                                 dtype=<class 'numpy.float64'>,
                                 encoding='utf-8', input='content',
                                 lowercase=True, max_df=0.7222222222222222,
                                 max_features=None,
                                 min_df=0.00014705642569160768,
                                 ngram_range=(1, 1), norm='l2',
                                 preprocessor=None, smooth_idf=True,
                                 stop_words=None, strip_accents=None...
                                        max_features=0.01, max_leaf_nodes=None,
                                        max_samples=0.007808728196651904,
                                        min_impurity_decrease=3.1622776601683795e-10,
                                        min_impurity_split=None,
                                        min_samples_leaf=62,
                                        min_samples_split=0.0008966659727253451,
                                        min_weight_fraction_leaf=0.0007448459487372297,
                                        n_estimators=251, n_jobs=None,
                                        oob_score=False, random_state=0,
                                        verbose=True, warm_start=True))],
         verbose=False),
                   Pipeline(memory=None,
         steps=[('vect',
                 TfidfVectorizer(analyzer='word', binary=False,
                                 decode_error='strict',
                                 dtype=<class 'numpy.float64'>,
                                 encoding='utf-8', input='content',
                                 lowercase=True, max_df=0.8555555555555556,
                                 max_features=None,
                                 min_df=0.00016483617040605902,
                                 ngram_range=(1, 2), norm='l2',
                                 preprocessor=None, smooth_idf=True,
                                 stop_words='english', strip_accents...
                                 tokenizer=None, use_idf=True,
                                 vocabulary=None)),
                ('clf',
                 LogisticRegression(C=0.47946121772409134, class_weight=None,
                                    dual=False, fit_intercept=True,
                                    intercept_scaling=1,
                                    l1_ratio=0.9168495407673116, max_iter=75,
                                    multi_class='multinomial', n_jobs=None,
                                    penalty='none', random_state=0,
                                    solver='sag', tol=6.514227791666363e-07,
                                    verbose=0, warm_start=False))],
         verbose=False),
                   MultinomialNB(alpha=1.0, class_prior=None, fit_prior=True),
                   None,
                   MultinomialNB(alpha=1.0, class_prior=None, fit_prior=True),
                   Pipeline(memory=None,
         steps=[('vect',
                 TfidfVectorizer(analyzer='word', binary=False,
                                 decode_error='strict',
                                 dtype=<class 'numpy.float64'>,
                                 encoding='utf-8', input='content',
                                 lowercase=True, max_df=0.6777777777777778,
                                 max_features=None,
                                 min_df=0.0012707570261625706,
                                 ngram_range=(1, 2), norm='l2',
                                 preprocessor=None, smooth_idf=True,
                                 stop_words=None, strip_accents=None,...
                                 vocabulary=None)),
                ('clf',
                 DecisionTreeClassifier(ccp_alpha=0.0, class_weight=None,
                                        criterion='gini', max_depth=49,
                                        max_features=0.56, max_leaf_nodes=316,
                                        min_impurity_decrease=1e-07,
                                        min_impurity_split=None,
                                        min_samples_leaf=4,
                                        min_samples_split=0.015198715678453873,
                                        min_weight_fraction_leaf=0.0,
                                        presort='deprecated', random_state=0,
                                        splitter='random'))],
         verbose=False),
                   Pipeline(memory=None,
         steps=[('vect',
                 TfidfVectorizer(analyzer='word', binary=False,
                                 decode_error='strict',
                                 dtype=<class 'numpy.float64'>,
                                 encoding='utf-8', input='content',
                                 lowercase=True, max_df=0.8555555555555556,
                                 max_features=None,
                                 min_df=0.00016483617040605902,
                                 ngram_range=(1, 2), norm='l2',
                                 preprocessor=None, smooth_idf=True,
                                 stop_words='english', strip_accents...
                                 tokenizer=None, use_idf=True,
                                 vocabulary=None)),
                ('clf',
                 LogisticRegression(C=0.47946121772409134, class_weight=None,
                                    dual=False, fit_intercept=True,
                                    intercept_scaling=1,
                                    l1_ratio=0.9168495407673116, max_iter=75,
                                    multi_class='multinomial', n_jobs=None,
                                    penalty='none', random_state=0,
                                    solver='sag', tol=6.514227791666363e-07,
                                    verbose=0, warm_start=False))],
         verbose=False),
                   None, None,
                   MultinomialNB(alpha=1.0, class_prior=None, fit_prior=True),
                   Pipeline(memory=None,
         steps=[('vect',
                 TfidfVectorizer(analyzer='word', binary=False,
                                 decode_error='strict',
                                 dtype=<class 'numpy.float64'>,
                                 encoding='utf-8', input='content',
                                 lowercase=True, max_df=0.7666666666666666,
                                 max_features=None,
                                 min_df=0.00010572508827796376,
                                 ngram_range=(1, 2), norm='l2',
                                 preprocessor=None, smooth_idf=True,
                                 stop_words='english', strip_accents=None,
                                 sublinear_tf=False, token_pattern='\\w{1,}',
                                 tokenizer=None, use_idf=True,
                                 vocabulary=None)),
                ('clf',
                 LinearSVC(C=99.37702864845689, class_weight=None, dual=False,
                           fit_intercept=False, intercept_scaling=1,
                           loss='squared_hinge', max_iter=1000,
                           multi_class='ovr', penalty='l2', random_state=0,
                           tol=3.8359242531998225e-06, verbose=0))],
         verbose=False),
                   Pipeline(memory=None,
         steps=[('vect',
                 TfidfVectorizer(analyzer='word', binary=False,
                                 decode_error='strict',
                                 dtype=<class 'numpy.float64'>,
                                 encoding='utf-8', input='content',
                                 lowercase=True, max_df=0.6777777777777778,
                                 max_features=None,
                                 min_df=0.0012707570261625706,
                                 ngram_range=(1, 2), norm='l2',
                                 preprocessor=None, smooth_idf=True,
                                 stop_words=None, strip_accents=None,...
                                 vocabulary=None)),
                ('clf',
                 DecisionTreeClassifier(ccp_alpha=0.0, class_weight=None,
                                        criterion='gini', max_depth=49,
                                        max_features=0.56, max_leaf_nodes=316,
                                        min_impurity_decrease=1e-07,
                                        min_impurity_split=None,
                                        min_samples_leaf=4,
                                        min_samples_split=0.015198715678453873,
                                        min_weight_fraction_leaf=0.0,
                                        presort='deprecated', random_state=0,
                                        splitter='random'))],
         verbose=False),
                   None,
                   Pipeline(memory=None,
         steps=[('vect',
                 TfidfVectorizer(analyzer='word', binary=False,
                                 decode_error='strict',
                                 dtype=<class 'numpy.float64'>,
                                 encoding='utf-8', input='content',
                                 lowercase=True, max_df=0.7222222222222222,
                                 max_features=None,
                                 min_df=0.00014705642569160768,
                                 ngram_range=(1, 1), norm='l2',
                                 preprocessor=None, smooth_idf=True,
                                 stop_words=None, strip_accents=None...
                                        max_features=0.01, max_leaf_nodes=None,
                                        max_samples=0.007808728196651904,
                                        min_impurity_decrease=3.1622776601683795e-10,
                                        min_impurity_split=None,
                                        min_samples_leaf=62,
                                        min_samples_split=0.0008966659727253451,
                                        min_weight_fraction_leaf=0.0007448459487372297,
                                        n_estimators=251, n_jobs=None,
                                        oob_score=False, random_state=0,
                                        verbose=True, warm_start=True))],
         verbose=False),
                   Pipeline(memory=None,
         steps=[('vect',
                 TfidfVectorizer(analyzer='word', binary=False,
                                 decode_error='strict',
                                 dtype=<class 'numpy.float64'>,
                                 encoding='utf-8', input='content',
                                 lowercase=True, max_df=0.8555555555555556,
                                 max_features=None,
                                 min_df=0.00016483617040605902,
                                 ngram_range=(1, 2), norm='l2',
                                 preprocessor=None, smooth_idf=True,
                                 stop_words='english', strip_accents...
                                 tokenizer=None, use_idf=True,
                                 vocabulary=None)),
                ('clf',
                 LogisticRegression(C=0.47946121772409134, class_weight=None,
                                    dual=False, fit_intercept=True,
                                    intercept_scaling=1,
                                    l1_ratio=0.9168495407673116, max_iter=75,
                                    multi_class='multinomial', n_jobs=None,
                                    penalty='none', random_state=0,
                                    solver='sag', tol=6.514227791666363e-07,
                                    verbose=0, warm_start=False))],
         verbose=False),
                   Pipeline(memory=None,
         steps=[('vect',
                 TfidfVectorizer(analyzer='word', binary=False,
                                 decode_error='strict',
                                 dtype=<class 'numpy.float64'>,
                                 encoding='utf-8', input='content',
                                 lowercase=True, max_df=0.6777777777777778,
                                 max_features=None,
                                 min_df=0.0012707570261625706,
                                 ngram_range=(1, 2), norm='l2',
                                 preprocessor=None, smooth_idf=True,
                                 stop_words=None, strip_accents=None,...
                                 vocabulary=None)),
                ('clf',
                 DecisionTreeClassifier(ccp_alpha=0.0, class_weight=None,
                                        criterion='gini', max_depth=49,
                                        max_features=0.56, max_leaf_nodes=316,
                                        min_impurity_decrease=1e-07,
                                        min_impurity_split=None,
                                        min_samples_leaf=4,
                                        min_samples_split=0.015198715678453873,
                                        min_weight_fraction_leaf=0.0,
                                        presort='deprecated', random_state=0,
                                        splitter='random'))],
         verbose=False),
                   None, None,
                   Pipeline(memory=None,
         steps=[('vect',
                 TfidfVectorizer(analyzer='word', binary=False,
                                 decode_error='strict',
                                 dtype=<class 'numpy.float64'>,
                                 encoding='utf-8', input='content',
                                 lowercase=True, max_df=0.6333333333333333,
                                 max_features=None,
                                 min_df=0.0010083260374073057,
                                 ngram_range=(1, 1), norm='l2',
                                 preprocessor=None, smooth_idf=True,
                                 stop_words='english', strip_accents=...
                                 tokenizer=None, use_idf=True,
                                 vocabulary=None)),
                ('clf',
                 AdaBoostClassifier(algorithm='SAMME',
                                    base_estimator=LinearSVC(C=1.0,
                                                             class_weight=None,
                                                             dual=True,
                                                             fit_intercept=True,
                                                             intercept_scaling=1,
                                                             loss='squared_hinge',
                                                             max_iter=1000,
                                                             multi_class='ovr',
                                                             penalty='l2',
                                                             random_state=None,
                                                             tol=0.0001,
                                                             verbose=0),
                                    learning_rate=0.40053644159285917,
                                    n_estimators=190, random_state=0))],
         verbose=False),
                   MultinomialNB(alpha=1.0, class_prior=None, fit_prior=True),
                   Pipeline(memory=None,
         steps=[('vect',
                 TfidfVectorizer(analyzer='word', binary=False,
                                 decode_error='strict',
                                 dtype=<class 'numpy.float64'>,
                                 encoding='utf-8', input='content',
                                 lowercase=True, max_df=0.8555555555555556,
                                 max_features=None,
                                 min_df=0.00016483617040605902,
                                 ngram_range=(1, 2), norm='l2',
                                 preprocessor=None, smooth_idf=True,
                                 stop_words='english', strip_accents...
                                 tokenizer=None, use_idf=True,
                                 vocabulary=None)),
                ('clf',
                 LogisticRegression(C=0.47946121772409134, class_weight=None,
                                    dual=False, fit_intercept=True,
                                    intercept_scaling=1,
                                    l1_ratio=0.9168495407673116, max_iter=75,
                                    multi_class='multinomial', n_jobs=None,
                                    penalty='none', random_state=0,
                                    solver='sag', tol=6.514227791666363e-07,
                                    verbose=0, warm_start=False))],
         verbose=False),
                   Pipeline(memory=None,
         steps=[('vect',
                 TfidfVectorizer(analyzer='word', binary=False,
                                 decode_error='strict',
                                 dtype=<class 'numpy.float64'>,
                                 encoding='utf-8', input='content',
                                 lowercase=True, max_df=0.7666666666666666,
                                 max_features=None,
                                 min_df=0.00010572508827796376,
                                 ngram_range=(1, 2), norm='l2',
                                 preprocessor=None, smooth_idf=True,
                                 stop_words='english', strip_accents=None,
                                 sublinear_tf=False, token_pattern='\\w{1,}',
                                 tokenizer=None, use_idf=True,
                                 vocabulary=None)),
                ('clf',
                 LinearSVC(C=99.37702864845689, class_weight=None, dual=False,
                           fit_intercept=False, intercept_scaling=1,
                           loss='squared_hinge', max_iter=1000,
                           multi_class='ovr', penalty='l2', random_state=0,
                           tol=3.8359242531998225e-06, verbose=0))],
         verbose=False),
                   Pipeline(memory=None,
         steps=[('vect',
                 TfidfVectorizer(analyzer='word', binary=False,
                                 decode_error='strict',
                                 dtype=<class 'numpy.float64'>,
                                 encoding='utf-8', input='content',
                                 lowercase=True, max_df=0.7222222222222222,
                                 max_features=None,
                                 min_df=0.00014705642569160768,
                                 ngram_range=(1, 1), norm='l2',
                                 preprocessor=None, smooth_idf=True,
                                 stop_words=None, strip_accents=None...
                                        max_features=0.01, max_leaf_nodes=None,
                                        max_samples=0.007808728196651904,
                                        min_impurity_decrease=3.1622776601683795e-10,
                                        min_impurity_split=None,
                                        min_samples_leaf=62,
                                        min_samples_split=0.0008966659727253451,
                                        min_weight_fraction_leaf=0.0007448459487372297,
                                        n_estimators=251, n_jobs=None,
                                        oob_score=False, random_state=0,
                                        verbose=True, warm_start=True))],
         verbose=False),
                   Pipeline(memory=None,
         steps=[('vect',
                 TfidfVectorizer(analyzer='word', binary=False,
                                 decode_error='strict',
                                 dtype=<class 'numpy.float64'>,
                                 encoding='utf-8', input='content',
                                 lowercase=True, max_df=0.6333333333333333,
                                 max_features=None,
                                 min_df=0.0010083260374073057,
                                 ngram_range=(1, 1), norm='l2',
                                 preprocessor=None, smooth_idf=True,
                                 stop_words='english', strip_accents=...
                                 tokenizer=None, use_idf=True,
                                 vocabulary=None)),
                ('clf',
                 AdaBoostClassifier(algorithm='SAMME',
                                    base_estimator=LinearSVC(C=1.0,
                                                             class_weight=None,
                                                             dual=True,
                                                             fit_intercept=True,
                                                             intercept_scaling=1,
                                                             loss='squared_hinge',
                                                             max_iter=1000,
                                                             multi_class='ovr',
                                                             penalty='l2',
                                                             random_state=None,
                                                             tol=0.0001,
                                                             verbose=0),
                                    learning_rate=0.40053644159285917,
                                    n_estimators=190, random_state=0))],
         verbose=False),
                   Pipeline(memory=None,
         steps=[('vect',
                 TfidfVectorizer(analyzer='word', binary=False,
                                 decode_error='strict',
                                 dtype=<class 'numpy.float64'>,
                                 encoding='utf-8', input='content',
                                 lowercase=True, max_df=0.6333333333333333,
                                 max_features=None,
                                 min_df=0.0010083260374073057,
                                 ngram_range=(1, 1), norm='l2',
                                 preprocessor=None, smooth_idf=True,
                                 stop_words='english', strip_accents=...
                                 tokenizer=None, use_idf=True,
                                 vocabulary=None)),
                ('clf',
                 AdaBoostClassifier(algorithm='SAMME',
                                    base_estimator=LinearSVC(C=1.0,
                                                             class_weight=None,
                                                             dual=True,
                                                             fit_intercept=True,
                                                             intercept_scaling=1,
                                                             loss='squared_hinge',
                                                             max_iter=1000,
                                                             multi_class='ovr',
                                                             penalty='l2',
                                                             random_state=None,
                                                             tol=0.0001,
                                                             verbose=0),
                                    learning_rate=0.40053644159285917,
                                    n_estimators=190, random_state=0))],
         verbose=False),
                   Pipeline(memory=None,
         steps=[('vect',
                 TfidfVectorizer(analyzer='word', binary=False,
                                 decode_error='strict',
                                 dtype=<class 'numpy.float64'>,
                                 encoding='utf-8', input='content',
                                 lowercase=True, max_df=0.6777777777777778,
                                 max_features=None,
                                 min_df=0.0012707570261625706,
                                 ngram_range=(1, 2), norm='l2',
                                 preprocessor=None, smooth_idf=True,
                                 stop_words=None, strip_accents=None,...
                                 vocabulary=None)),
                ('clf',
                 DecisionTreeClassifier(ccp_alpha=0.0, class_weight=None,
                                        criterion='gini', max_depth=49,
                                        max_features=0.56, max_leaf_nodes=316,
                                        min_impurity_decrease=1e-07,
                                        min_impurity_split=None,
                                        min_samples_leaf=4,
                                        min_samples_split=0.015198715678453873,
                                        min_weight_fraction_leaf=0.0,
                                        presort='deprecated', random_state=0,
                                        splitter='random'))],
         verbose=False),
                   None,
                   Pipeline(memory=None,
         steps=[('vect',
                 TfidfVectorizer(analyzer='word', binary=False,
                                 decode_error='strict',
                                 dtype=<class 'numpy.float64'>,
                                 encoding='utf-8', input='content',
                                 lowercase=True, max_df=0.7666666666666666,
                                 max_features=None,
                                 min_df=0.00010572508827796376,
                                 ngram_range=(1, 2), norm='l2',
                                 preprocessor=None, smooth_idf=True,
                                 stop_words='english', strip_accents=None,
                                 sublinear_tf=False, token_pattern='\\w{1,}',
                                 tokenizer=None, use_idf=True,
                                 vocabulary=None)),
                ('clf',
                 LinearSVC(C=99.37702864845689, class_weight=None, dual=False,
                           fit_intercept=False, intercept_scaling=1,
                           loss='squared_hinge', max_iter=1000,
                           multi_class='ovr', penalty='l2', random_state=0,
                           tol=3.8359242531998225e-06, verbose=0))],
         verbose=False),
                   Pipeline(memory=None,
         steps=[('vect',
                 TfidfVectorizer(analyzer='word', binary=False,
                                 decode_error='strict',
                                 dtype=<class 'numpy.float64'>,
                                 encoding='utf-8', input='content',
                                 lowercase=True, max_df=0.6777777777777778,
                                 max_features=None,
                                 min_df=0.0012707570261625706,
                                 ngram_range=(1, 2), norm='l2',
                                 preprocessor=None, smooth_idf=True,
                                 stop_words=None, strip_accents=None,...
                                 vocabulary=None)),
                ('clf',
                 DecisionTreeClassifier(ccp_alpha=0.0, class_weight=None,
                                        criterion='gini', max_depth=49,
                                        max_features=0.56, max_leaf_nodes=316,
                                        min_impurity_decrease=1e-07,
                                        min_impurity_split=None,
                                        min_samples_leaf=4,
                                        min_samples_split=0.015198715678453873,
                                        min_weight_fraction_leaf=0.0,
                                        presort='deprecated', random_state=0,
                                        splitter='random'))],
         verbose=False),
                   MultinomialNB(alpha=1.0, class_prior=None, fit_prior=True),
                   None,
                   Pipeline(memory=None,
         steps=[('vect',
                 TfidfVectorizer(analyzer='word', binary=False,
                                 decode_error='strict',
                                 dtype=<class 'numpy.float64'>,
                                 encoding='utf-8', input='content',
                                 lowercase=True, max_df=0.7666666666666666,
                                 max_features=None,
                                 min_df=0.00010572508827796376,
                                 ngram_range=(1, 2), norm='l2',
                                 preprocessor=None, smooth_idf=True,
                                 stop_words='english', strip_accents=None,
                                 sublinear_tf=False, token_pattern='\\w{1,}',
                                 tokenizer=None, use_idf=True,
                                 vocabulary=None)),
                ('clf',
                 LinearSVC(C=99.37702864845689, class_weight=None, dual=False,
                           fit_intercept=False, intercept_scaling=1,
                           loss='squared_hinge', max_iter=1000,
                           multi_class='ovr', penalty='l2', random_state=0,
                           tol=3.8359242531998225e-06, verbose=0))],
         verbose=False),
                   Pipeline(memory=None,
         steps=[('vect',
                 TfidfVectorizer(analyzer='word', binary=False,
                                 decode_error='strict',
                                 dtype=<class 'numpy.float64'>,
                                 encoding='utf-8', input='content',
                                 lowercase=True, max_df=0.8555555555555556,
                                 max_features=None,
                                 min_df=0.00016483617040605902,
                                 ngram_range=(1, 2), norm='l2',
                                 preprocessor=None, smooth_idf=True,
                                 stop_words='english', strip_accents...
                                 tokenizer=None, use_idf=True,
                                 vocabulary=None)),
                ('clf',
                 LogisticRegression(C=0.47946121772409134, class_weight=None,
                                    dual=False, fit_intercept=True,
                                    intercept_scaling=1,
                                    l1_ratio=0.9168495407673116, max_iter=75,
                                    multi_class='multinomial', n_jobs=None,
                                    penalty='none', random_state=0,
                                    solver='sag', tol=6.514227791666363e-07,
                                    verbose=0, warm_start=False))],
         verbose=False),
                   None,
                   Pipeline(memory=None,
         steps=[('vect',
                 TfidfVectorizer(analyzer='word', binary=False,
                                 decode_error='strict',
                                 dtype=<class 'numpy.float64'>,
                                 encoding='utf-8', input='content',
                                 lowercase=True, max_df=0.6333333333333333,
                                 max_features=None,
                                 min_df=0.0010083260374073057,
                                 ngram_range=(1, 1), norm='l2',
                                 preprocessor=None, smooth_idf=True,
                                 stop_words='english', strip_accents=...
                                 tokenizer=None, use_idf=True,
                                 vocabulary=None)),
                ('clf',
                 AdaBoostClassifier(algorithm='SAMME',
                                    base_estimator=LinearSVC(C=1.0,
                                                             class_weight=None,
                                                             dual=True,
                                                             fit_intercept=True,
                                                             intercept_scaling=1,
                                                             loss='squared_hinge',
                                                             max_iter=1000,
                                                             multi_class='ovr',
                                                             penalty='l2',
                                                             random_state=None,
                                                             tol=0.0001,
                                                             verbose=0),
                                    learning_rate=0.40053644159285917,
                                    n_estimators=190, random_state=0))],
         verbose=False),
                   Pipeline(memory=None,
         steps=[('vect',
                 TfidfVectorizer(analyzer='word', binary=False,
                                 decode_error='strict',
                                 dtype=<class 'numpy.float64'>,
                                 encoding='utf-8', input='content',
                                 lowercase=True, max_df=0.7222222222222222,
                                 max_features=None,
                                 min_df=0.00014705642569160768,
                                 ngram_range=(1, 1), norm='l2',
                                 preprocessor=None, smooth_idf=True,
                                 stop_words=None, strip_accents=None...
                                        max_features=0.01, max_leaf_nodes=None,
                                        max_samples=0.007808728196651904,
                                        min_impurity_decrease=3.1622776601683795e-10,
                                        min_impurity_split=None,
                                        min_samples_leaf=62,
                                        min_samples_split=0.0008966659727253451,
                                        min_weight_fraction_leaf=0.0007448459487372297,
                                        n_estimators=251, n_jobs=None,
                                        oob_score=False, random_state=0,
                                        verbose=True, warm_start=True))],
         verbose=False),
                   Pipeline(memory=None,
         steps=[('vect',
                 TfidfVectorizer(analyzer='word', binary=False,
                                 decode_error='strict',
                                 dtype=<class 'numpy.float64'>,
                                 encoding='utf-8', input='content',
                                 lowercase=True, max_df=0.7222222222222222,
                                 max_features=None,
                                 min_df=0.00014705642569160768,
                                 ngram_range=(1, 1), norm='l2',
                                 preprocessor=None, smooth_idf=True,
                                 stop_words=None, strip_accents=None...
                                        max_features=0.01, max_leaf_nodes=None,
                                        max_samples=0.007808728196651904,
                                        min_impurity_decrease=3.1622776601683795e-10,
                                        min_impurity_split=None,
                                        min_samples_leaf=62,
                                        min_samples_split=0.0008966659727253451,
                                        min_weight_fraction_leaf=0.0007448459487372297,
                                        n_estimators=251, n_jobs=None,
                                        oob_score=False, random_state=0,
                                        verbose=True, warm_start=True))],
         verbose=False),
                   Pipeline(memory=None,
         steps=[('vect',
                 TfidfVectorizer(analyzer='word', binary=False,
                                 decode_error='strict',
                                 dtype=<class 'numpy.float64'>,
                                 encoding='utf-8', input='content',
                                 lowercase=True, max_df=0.6333333333333333,
                                 max_features=None,
                                 min_df=0.0010083260374073057,
                                 ngram_range=(1, 1), norm='l2',
                                 preprocessor=None, smooth_idf=True,
                                 stop_words='english', strip_accents=...
                                 tokenizer=None, use_idf=True,
                                 vocabulary=None)),
                ('clf',
                 AdaBoostClassifier(algorithm='SAMME',
                                    base_estimator=LinearSVC(C=1.0,
                                                             class_weight=None,
                                                             dual=True,
                                                             fit_intercept=True,
                                                             intercept_scaling=1,
                                                             loss='squared_hinge',
                                                             max_iter=1000,
                                                             multi_class='ovr',
                                                             penalty='l2',
                                                             random_state=None,
                                                             tol=0.0001,
                                                             verbose=0),
                                    learning_rate=0.40053644159285917,
                                    n_estimators=190, random_state=0))],
         verbose=False),
                   Pipeline(memory=None,
         steps=[('vect',
                 TfidfVectorizer(analyzer='word', binary=False,
                                 decode_error='strict',
                                 dtype=<class 'numpy.float64'>,
                                 encoding='utf-8', input='content',
                                 lowercase=True, max_df=0.8555555555555556,
                                 max_features=None,
                                 min_df=0.00016483617040605902,
                                 ngram_range=(1, 2), norm='l2',
                                 preprocessor=None, smooth_idf=True,
                                 stop_words='english', strip_accents...
                                 tokenizer=None, use_idf=True,
                                 vocabulary=None)),
                ('clf',
                 LogisticRegression(C=0.47946121772409134, class_weight=None,
                                    dual=False, fit_intercept=True,
                                    intercept_scaling=1,
                                    l1_ratio=0.9168495407673116, max_iter=75,
                                    multi_class='multinomial', n_jobs=None,
                                    penalty='none', random_state=0,
                                    solver='sag', tol=6.514227791666363e-07,
                                    verbose=0, warm_start=False))],
         verbose=False),
                   Pipeline(memory=None,
         steps=[('vect',
                 TfidfVectorizer(analyzer='word', binary=False,
                                 decode_error='strict',
                                 dtype=<class 'numpy.float64'>,
                                 encoding='utf-8', input='content',
                                 lowercase=True, max_df=0.7222222222222222,
                                 max_features=None,
                                 min_df=0.00014705642569160768,
                                 ngram_range=(1, 1), norm='l2',
                                 preprocessor=None, smooth_idf=True,
                                 stop_words=None, strip_accents=None...
                                        max_features=0.01, max_leaf_nodes=None,
                                        max_samples=0.007808728196651904,
                                        min_impurity_decrease=3.1622776601683795e-10,
                                        min_impurity_split=None,
                                        min_samples_leaf=62,
                                        min_samples_split=0.0008966659727253451,
                                        min_weight_fraction_leaf=0.0007448459487372297,
                                        n_estimators=251, n_jobs=None,
                                        oob_score=False, random_state=0,
                                        verbose=True, warm_start=True))],
         verbose=False),
                   Pipeline(memory=None,
         steps=[('vect',
                 TfidfVectorizer(analyzer='word', binary=False,
                                 decode_error='strict',
                                 dtype=<class 'numpy.float64'>,
                                 encoding='utf-8', input='content',
                                 lowercase=True, max_df=0.6333333333333333,
                                 max_features=None,
                                 min_df=0.0010083260374073057,
                                 ngram_range=(1, 1), norm='l2',
                                 preprocessor=None, smooth_idf=True,
                                 stop_words='english', strip_accents=...
                                 tokenizer=None, use_idf=True,
                                 vocabulary=None)),
                ('clf',
                 AdaBoostClassifier(algorithm='SAMME',
                                    base_estimator=LinearSVC(C=1.0,
                                                             class_weight=None,
                                                             dual=True,
                                                             fit_intercept=True,
                                                             intercept_scaling=1,
                                                             loss='squared_hinge',
                                                             max_iter=1000,
                                                             multi_class='ovr',
                                                             penalty='l2',
                                                             random_state=None,
                                                             tol=0.0001,
                                                             verbose=0),
                                    learning_rate=0.40053644159285917,
                                    n_estimators=190, random_state=0))],
         verbose=False),
                   Pipeline(memory=None,
         steps=[('vect',
                 TfidfVectorizer(analyzer='word', binary=False,
                                 decode_error='strict',
                                 dtype=<class 'numpy.float64'>,
                                 encoding='utf-8', input='content',
                                 lowercase=True, max_df=0.7666666666666666,
                                 max_features=None,
                                 min_df=0.00010572508827796376,
                                 ngram_range=(1, 2), norm='l2',
                                 preprocessor=None, smooth_idf=True,
                                 stop_words='english', strip_accents=None,
                                 sublinear_tf=False, token_pattern='\\w{1,}',
                                 tokenizer=None, use_idf=True,
                                 vocabulary=None)),
                ('clf',
                 LinearSVC(C=99.37702864845689, class_weight=None, dual=False,
                           fit_intercept=False, intercept_scaling=1,
                           loss='squared_hinge', max_iter=1000,
                           multi_class='ovr', penalty='l2', random_state=0,
                           tol=3.8359242531998225e-06, verbose=0))],
         verbose=False),
                   None,
                   MultinomialNB(alpha=1.0, class_prior=None, fit_prior=True),
                   Pipeline(memory=None,
         steps=[('vect',
                 TfidfVectorizer(analyzer='word', binary=False,
                                 decode_error='strict',
                                 dtype=<class 'numpy.float64'>,
                                 encoding='utf-8', input='content',
                                 lowercase=True, max_df=0.6333333333333333,
                                 max_features=None,
                                 min_df=0.0010083260374073057,
                                 ngram_range=(1, 1), norm='l2',
                                 preprocessor=None, smooth_idf=True,
                                 stop_words='english', strip_accents=...
                                 tokenizer=None, use_idf=True,
                                 vocabulary=None)),
                ('clf',
                 AdaBoostClassifier(algorithm='SAMME',
                                    base_estimator=LinearSVC(C=1.0,
                                                             class_weight=None,
                                                             dual=True,
                                                             fit_intercept=True,
                                                             intercept_scaling=1,
                                                             loss='squared_hinge',
                                                             max_iter=1000,
                                                             multi_class='ovr',
                                                             penalty='l2',
                                                             random_state=None,
                                                             tol=0.0001,
                                                             verbose=0),
                                    learning_rate=0.40053644159285917,
                                    n_estimators=190, random_state=0))],
         verbose=False),
                   Pipeline(memory=None,
         steps=[('vect',
                 TfidfVectorizer(analyzer='word', binary=False,
                                 decode_error='strict',
                                 dtype=<class 'numpy.float64'>,
                                 encoding='utf-8', input='content',
                                 lowercase=True, max_df=0.8555555555555556,
                                 max_features=None,
                                 min_df=0.00016483617040605902,
                                 ngram_range=(1, 2), norm='l2',
                                 preprocessor=None, smooth_idf=True,
                                 stop_words='english', strip_accents...
                                 tokenizer=None, use_idf=True,
                                 vocabulary=None)),
                ('clf',
                 LogisticRegression(C=0.47946121772409134, class_weight=None,
                                    dual=False, fit_intercept=True,
                                    intercept_scaling=1,
                                    l1_ratio=0.9168495407673116, max_iter=75,
                                    multi_class='multinomial', n_jobs=None,
                                    penalty='none', random_state=0,
                                    solver='sag', tol=6.514227791666363e-07,
                                    verbose=0, warm_start=False))],
         verbose=False),
                   Pipeline(memory=None,
         steps=[('vect',
                 TfidfVectorizer(analyzer='word', binary=False,
                                 decode_error='strict',
                                 dtype=<class 'numpy.float64'>,
                                 encoding='utf-8', input='content',
                                 lowercase=True, max_df=0.8555555555555556,
                                 max_features=None,
                                 min_df=0.00016483617040605902,
                                 ngram_range=(1, 2), norm='l2',
                                 preprocessor=None, smooth_idf=True,
                                 stop_words='english', strip_accents...
                                 tokenizer=None, use_idf=True,
                                 vocabulary=None)),
                ('clf',
                 LogisticRegression(C=0.47946121772409134, class_weight=None,
                                    dual=False, fit_intercept=True,
                                    intercept_scaling=1,
                                    l1_ratio=0.9168495407673116, max_iter=75,
                                    multi_class='multinomial', n_jobs=None,
                                    penalty='none', random_state=0,
                                    solver='sag', tol=6.514227791666363e-07,
                                    verbose=0, warm_start=False))],
         verbose=False),
                   Pipeline(memory=None,
         steps=[('vect',
                 TfidfVectorizer(analyzer='word', binary=False,
                                 decode_error='strict',
                                 dtype=<class 'numpy.float64'>,
                                 encoding='utf-8', input='content',
                                 lowercase=True, max_df=0.7222222222222222,
                                 max_features=None,
                                 min_df=0.00014705642569160768,
                                 ngram_range=(1, 1), norm='l2',
                                 preprocessor=None, smooth_idf=True,
                                 stop_words=None, strip_accents=None...
                                        max_features=0.01, max_leaf_nodes=None,
                                        max_samples=0.007808728196651904,
                                        min_impurity_decrease=3.1622776601683795e-10,
                                        min_impurity_split=None,
                                        min_samples_leaf=62,
                                        min_samples_split=0.0008966659727253451,
                                        min_weight_fraction_leaf=0.0007448459487372297,
                                        n_estimators=251, n_jobs=None,
                                        oob_score=False, random_state=0,
                                        verbose=True, warm_start=True))],
         verbose=False),
                   MultinomialNB(alpha=1.0, class_prior=None, fit_prior=True),
                   None,
                   Pipeline(memory=None,
         steps=[('vect',
                 TfidfVectorizer(analyzer='word', binary=False,
                                 decode_error='strict',
                                 dtype=<class 'numpy.float64'>,
                                 encoding='utf-8', input='content',
                                 lowercase=True, max_df=0.6333333333333333,
                                 max_features=None,
                                 min_df=0.0010083260374073057,
                                 ngram_range=(1, 1), norm='l2',
                                 preprocessor=None, smooth_idf=True,
                                 stop_words='english', strip_accents=...
                                 tokenizer=None, use_idf=True,
                                 vocabulary=None)),
                ('clf',
                 AdaBoostClassifier(algorithm='SAMME',
                                    base_estimator=LinearSVC(C=1.0,
                                                             class_weight=None,
                                                             dual=True,
                                                             fit_intercept=True,
                                                             intercept_scaling=1,
                                                             loss='squared_hinge',
                                                             max_iter=1000,
                                                             multi_class='ovr',
                                                             penalty='l2',
                                                             random_state=None,
                                                             tol=0.0001,
                                                             verbose=0),
                                    learning_rate=0.40053644159285917,
                                    n_estimators=190, random_state=0))],
         verbose=False),
                   None,
                   Pipeline(memory=None,
         steps=[('vect',
                 TfidfVectorizer(analyzer='word', binary=False,
                                 decode_error='strict',
                                 dtype=<class 'numpy.float64'>,
                                 encoding='utf-8', input='content',
                                 lowercase=True, max_df=0.6777777777777778,
                                 max_features=None,
                                 min_df=0.0012707570261625706,
                                 ngram_range=(1, 2), norm='l2',
                                 preprocessor=None, smooth_idf=True,
                                 stop_words=None, strip_accents=None,...
                                 vocabulary=None)),
                ('clf',
                 DecisionTreeClassifier(ccp_alpha=0.0, class_weight=None,
                                        criterion='gini', max_depth=49,
                                        max_features=0.56, max_leaf_nodes=316,
                                        min_impurity_decrease=1e-07,
                                        min_impurity_split=None,
                                        min_samples_leaf=4,
                                        min_samples_split=0.015198715678453873,
                                        min_weight_fraction_leaf=0.0,
                                        presort='deprecated', random_state=0,
                                        splitter='random'))],
         verbose=False),
                   Pipeline(memory=None,
         steps=[('vect',
                 TfidfVectorizer(analyzer='word', binary=False,
                                 decode_error='strict',
                                 dtype=<class 'numpy.float64'>,
                                 encoding='utf-8', input='content',
                                 lowercase=True, max_df=0.7222222222222222,
                                 max_features=None,
                                 min_df=0.00014705642569160768,
                                 ngram_range=(1, 1), norm='l2',
                                 preprocessor=None, smooth_idf=True,
                                 stop_words=None, strip_accents=None...
                                        max_features=0.01, max_leaf_nodes=None,
                                        max_samples=0.007808728196651904,
                                        min_impurity_decrease=3.1622776601683795e-10,
                                        min_impurity_split=None,
                                        min_samples_leaf=62,
                                        min_samples_split=0.0008966659727253451,
                                        min_weight_fraction_leaf=0.0007448459487372297,
                                        n_estimators=251, n_jobs=None,
                                        oob_score=False, random_state=0,
                                        verbose=True, warm_start=True))],
         verbose=False),
                   None,
                   Pipeline(memory=None,
         steps=[('vect',
                 TfidfVectorizer(analyzer='word', binary=False,
                                 decode_error='strict',
                                 dtype=<class 'numpy.float64'>,
                                 encoding='utf-8', input='content',
                                 lowercase=True, max_df=0.6333333333333333,
                                 max_features=None,
                                 min_df=0.0010083260374073057,
                                 ngram_range=(1, 1), norm='l2',
                                 preprocessor=None, smooth_idf=True,
                                 stop_words='english', strip_accents=...
                                 tokenizer=None, use_idf=True,
                                 vocabulary=None)),
                ('clf',
                 AdaBoostClassifier(algorithm='SAMME',
                                    base_estimator=LinearSVC(C=1.0,
                                                             class_weight=None,
                                                             dual=True,
                                                             fit_intercept=True,
                                                             intercept_scaling=1,
                                                             loss='squared_hinge',
                                                             max_iter=1000,
                                                             multi_class='ovr',
                                                             penalty='l2',
                                                             random_state=None,
                                                             tol=0.0001,
                                                             verbose=0),
                                    learning_rate=0.40053644159285917,
                                    n_estimators=190, random_state=0))],
         verbose=False),
                   MultinomialNB(alpha=1.0, class_prior=None, fit_prior=True),
                   Pipeline(memory=None,
         steps=[('vect',
                 TfidfVectorizer(analyzer='word', binary=False,
                                 decode_error='strict',
                                 dtype=<class 'numpy.float64'>,
                                 encoding='utf-8', input='content',
                                 lowercase=True, max_df=0.8555555555555556,
                                 max_features=None,
                                 min_df=0.00016483617040605902,
                                 ngram_range=(1, 2), norm='l2',
                                 preprocessor=None, smooth_idf=True,
                                 stop_words='english', strip_accents...
                                 tokenizer=None, use_idf=True,
                                 vocabulary=None)),
                ('clf',
                 LogisticRegression(C=0.47946121772409134, class_weight=None,
                                    dual=False, fit_intercept=True,
                                    intercept_scaling=1,
                                    l1_ratio=0.9168495407673116, max_iter=75,
                                    multi_class='multinomial', n_jobs=None,
                                    penalty='none', random_state=0,
                                    solver='sag', tol=6.514227791666363e-07,
                                    verbose=0, warm_start=False))],
         verbose=False),
                   Pipeline(memory=None,
         steps=[('vect',
                 TfidfVectorizer(analyzer='word', binary=False,
                                 decode_error='strict',
                                 dtype=<class 'numpy.float64'>,
                                 encoding='utf-8', input='content',
                                 lowercase=True, max_df=0.6777777777777778,
                                 max_features=None,
                                 min_df=0.0012707570261625706,
                                 ngram_range=(1, 2), norm='l2',
                                 preprocessor=None, smooth_idf=True,
                                 stop_words=None, strip_accents=None,...
                                 vocabulary=None)),
                ('clf',
                 DecisionTreeClassifier(ccp_alpha=0.0, class_weight=None,
                                        criterion='gini', max_depth=49,
                                        max_features=0.56, max_leaf_nodes=316,
                                        min_impurity_decrease=1e-07,
                                        min_impurity_split=None,
                                        min_samples_leaf=4,
                                        min_samples_split=0.015198715678453873,
                                        min_weight_fraction_leaf=0.0,
                                        presort='deprecated', random_state=0,
                                        splitter='random'))],
         verbose=False),
                   Pipeline(memory=None,
         steps=[('vect',
                 TfidfVectorizer(analyzer='word', binary=False,
                                 decode_error='strict',
                                 dtype=<class 'numpy.float64'>,
                                 encoding='utf-8', input='content',
                                 lowercase=True, max_df=0.7222222222222222,
                                 max_features=None,
                                 min_df=0.00014705642569160768,
                                 ngram_range=(1, 1), norm='l2',
                                 preprocessor=None, smooth_idf=True,
                                 stop_words=None, strip_accents=None...
                                        max_features=0.01, max_leaf_nodes=None,
                                        max_samples=0.007808728196651904,
                                        min_impurity_decrease=3.1622776601683795e-10,
                                        min_impurity_split=None,
                                        min_samples_leaf=62,
                                        min_samples_split=0.0008966659727253451,
                                        min_weight_fraction_leaf=0.0007448459487372297,
                                        n_estimators=251, n_jobs=None,
                                        oob_score=False, random_state=0,
                                        verbose=True, warm_start=True))],
         verbose=False),
                   MultinomialNB(alpha=1.0, class_prior=None, fit_prior=True),
                   Pipeline(memory=None,
         steps=[('vect',
                 TfidfVectorizer(analyzer='word', binary=False,
                                 decode_error='strict',
                                 dtype=<class 'numpy.float64'>,
                                 encoding='utf-8', input='content',
                                 lowercase=True, max_df=0.8555555555555556,
                                 max_features=None,
                                 min_df=0.00016483617040605902,
                                 ngram_range=(1, 2), norm='l2',
                                 preprocessor=None, smooth_idf=True,
                                 stop_words='english', strip_accents...
                                 tokenizer=None, use_idf=True,
                                 vocabulary=None)),
                ('clf',
                 LogisticRegression(C=0.47946121772409134, class_weight=None,
                                    dual=False, fit_intercept=True,
                                    intercept_scaling=1,
                                    l1_ratio=0.9168495407673116, max_iter=75,
                                    multi_class='multinomial', n_jobs=None,
                                    penalty='none', random_state=0,
                                    solver='sag', tol=6.514227791666363e-07,
                                    verbose=0, warm_start=False))],
         verbose=False),
                   MultinomialNB(alpha=1.0, class_prior=None, fit_prior=True),
                   Pipeline(memory=None,
         steps=[('vect',
                 TfidfVectorizer(analyzer='word', binary=False,
                                 decode_error='strict',
                                 dtype=<class 'numpy.float64'>,
                                 encoding='utf-8', input='content',
                                 lowercase=True, max_df=0.6333333333333333,
                                 max_features=None,
                                 min_df=0.0010083260374073057,
                                 ngram_range=(1, 1), norm='l2',
                                 preprocessor=None, smooth_idf=True,
                                 stop_words='english', strip_accents=...
                                 tokenizer=None, use_idf=True,
                                 vocabulary=None)),
                ('clf',
                 AdaBoostClassifier(algorithm='SAMME',
                                    base_estimator=LinearSVC(C=1.0,
                                                             class_weight=None,
                                                             dual=True,
                                                             fit_intercept=True,
                                                             intercept_scaling=1,
                                                             loss='squared_hinge',
                                                             max_iter=1000,
                                                             multi_class='ovr',
                                                             penalty='l2',
                                                             random_state=None,
                                                             tol=0.0001,
                                                             verbose=0),
                                    learning_rate=0.40053644159285917,
                                    n_estimators=190, random_state=0))],
         verbose=False),
                   MultinomialNB(alpha=1.0, class_prior=None, fit_prior=True),
                   None,
                   MultinomialNB(alpha=1.0, class_prior=None, fit_prior=True),
                   Pipeline(memory=None,
         steps=[('vect',
                 TfidfVectorizer(analyzer='word', binary=False,
                                 decode_error='strict',
                                 dtype=<class 'numpy.float64'>,
                                 encoding='utf-8', input='content',
                                 lowercase=True, max_df=0.7222222222222222,
                                 max_features=None,
                                 min_df=0.00014705642569160768,
                                 ngram_range=(1, 1), norm='l2',
                                 preprocessor=None, smooth_idf=True,
                                 stop_words=None, strip_accents=None...
                                        max_features=0.01, max_leaf_nodes=None,
                                        max_samples=0.007808728196651904,
                                        min_impurity_decrease=3.1622776601683795e-10,
                                        min_impurity_split=None,
                                        min_samples_leaf=62,
                                        min_samples_split=0.0008966659727253451,
                                        min_weight_fraction_leaf=0.0007448459487372297,
                                        n_estimators=251, n_jobs=None,
                                        oob_score=False, random_state=0,
                                        verbose=True, warm_start=True))],
         verbose=False),
                   MultinomialNB(alpha=1.0, class_prior=None, fit_prior=True),
                   Pipeline(memory=None,
         steps=[('vect',
                 TfidfVectorizer(analyzer='word', binary=False,
                                 decode_error='strict',
                                 dtype=<class 'numpy.float64'>,
                                 encoding='utf-8', input='content',
                                 lowercase=True, max_df=0.7666666666666666,
                                 max_features=None,
                                 min_df=0.00010572508827796376,
                                 ngram_range=(1, 2), norm='l2',
                                 preprocessor=None, smooth_idf=True,
                                 stop_words='english', strip_accents=None,
                                 sublinear_tf=False, token_pattern='\\w{1,}',
                                 tokenizer=None, use_idf=True,
                                 vocabulary=None)),
                ('clf',
                 LinearSVC(C=99.37702864845689, class_weight=None, dual=False,
                           fit_intercept=False, intercept_scaling=1,
                           loss='squared_hinge', max_iter=1000,
                           multi_class='ovr', penalty='l2', random_state=0,
                           tol=3.8359242531998225e-06, verbose=0))],
         verbose=False),
                   Pipeline(memory=None,
         steps=[('vect',
                 TfidfVectorizer(analyzer='word', binary=False,
                                 decode_error='strict',
                                 dtype=<class 'numpy.float64'>,
                                 encoding='utf-8', input='content',
                                 lowercase=True, max_df=0.7666666666666666,
                                 max_features=None,
                                 min_df=0.00010572508827796376,
                                 ngram_range=(1, 2), norm='l2',
                                 preprocessor=None, smooth_idf=True,
                                 stop_words='english', strip_accents=None,
                                 sublinear_tf=False, token_pattern='\\w{1,}',
                                 tokenizer=None, use_idf=True,
                                 vocabulary=None)),
                ('clf',
                 LinearSVC(C=99.37702864845689, class_weight=None, dual=False,
                           fit_intercept=False, intercept_scaling=1,
                           loss='squared_hinge', max_iter=1000,
                           multi_class='ovr', penalty='l2', random_state=0,
                           tol=3.8359242531998225e-06, verbose=0))],
         verbose=False),
                   MultinomialNB(alpha=1.0, class_prior=None, fit_prior=True),
                   Pipeline(memory=None,
         steps=[('vect',
                 TfidfVectorizer(analyzer='word', binary=False,
                                 decode_error='strict',
                                 dtype=<class 'numpy.float64'>,
                                 encoding='utf-8', input='content',
                                 lowercase=True, max_df=0.7666666666666666,
                                 max_features=None,
                                 min_df=0.00010572508827796376,
                                 ngram_range=(1, 2), norm='l2',
                                 preprocessor=None, smooth_idf=True,
                                 stop_words='english', strip_accents=None,
                                 sublinear_tf=False, token_pattern='\\w{1,}',
                                 tokenizer=None, use_idf=True,
                                 vocabulary=None)),
                ('clf',
                 LinearSVC(C=99.37702864845689, class_weight=None, dual=False,
                           fit_intercept=False, intercept_scaling=1,
                           loss='squared_hinge', max_iter=1000,
                           multi_class='ovr', penalty='l2', random_state=0,
                           tol=3.8359242531998225e-06, verbose=0))],
         verbose=False),
                   None,
                   Pipeline(memory=None,
         steps=[('vect',
                 TfidfVectorizer(analyzer='word', binary=False,
                                 decode_error='strict',
                                 dtype=<class 'numpy.float64'>,
                                 encoding='utf-8', input='content',
                                 lowercase=True, max_df=0.7666666666666666,
                                 max_features=None,
                                 min_df=0.00010572508827796376,
                                 ngram_range=(1, 2), norm='l2',
                                 preprocessor=None, smooth_idf=True,
                                 stop_words='english', strip_accents=None,
                                 sublinear_tf=False, token_pattern='\\w{1,}',
                                 tokenizer=None, use_idf=True,
                                 vocabulary=None)),
                ('clf',
                 LinearSVC(C=99.37702864845689, class_weight=None, dual=False,
                           fit_intercept=False, intercept_scaling=1,
                           loss='squared_hinge', max_iter=1000,
                           multi_class='ovr', penalty='l2', random_state=0,
                           tol=3.8359242531998225e-06, verbose=0))],
         verbose=False),
                   MultinomialNB(alpha=1.0, class_prior=None, fit_prior=True),
                   Pipeline(memory=None,
         steps=[('vect',
                 TfidfVectorizer(analyzer='word', binary=False,
                                 decode_error='strict',
                                 dtype=<class 'numpy.float64'>,
                                 encoding='utf-8', input='content',
                                 lowercase=True, max_df=0.7666666666666666,
                                 max_features=None,
                                 min_df=0.00010572508827796376,
                                 ngram_range=(1, 2), norm='l2',
                                 preprocessor=None, smooth_idf=True,
                                 stop_words='english', strip_accents=None,
                                 sublinear_tf=False, token_pattern='\\w{1,}',
                                 tokenizer=None, use_idf=True,
                                 vocabulary=None)),
                ('clf',
                 LinearSVC(C=99.37702864845689, class_weight=None, dual=False,
                           fit_intercept=False, intercept_scaling=1,
                           loss='squared_hinge', max_iter=1000,
                           multi_class='ovr', penalty='l2', random_state=0,
                           tol=3.8359242531998225e-06, verbose=0))],
         verbose=False),
                   Pipeline(memory=None,
         steps=[('vect',
                 TfidfVectorizer(analyzer='word', binary=False,
                                 decode_error='strict',
                                 dtype=<class 'numpy.float64'>,
                                 encoding='utf-8', input='content',
                                 lowercase=True, max_df=0.6777777777777778,
                                 max_features=None,
                                 min_df=0.0012707570261625706,
                                 ngram_range=(1, 2), norm='l2',
                                 preprocessor=None, smooth_idf=True,
                                 stop_words=None, strip_accents=None,...
                                 vocabulary=None)),
                ('clf',
                 DecisionTreeClassifier(ccp_alpha=0.0, class_weight=None,
                                        criterion='gini', max_depth=49,
                                        max_features=0.56, max_leaf_nodes=316,
                                        min_impurity_decrease=1e-07,
                                        min_impurity_split=None,
                                        min_samples_leaf=4,
                                        min_samples_split=0.015198715678453873,
                                        min_weight_fraction_leaf=0.0,
                                        presort='deprecated', random_state=0,
                                        splitter='random'))],
         verbose=False),
                   Pipeline(memory=None,
         steps=[('vect',
                 TfidfVectorizer(analyzer='word', binary=False,
                                 decode_error='strict',
                                 dtype=<class 'numpy.float64'>,
                                 encoding='utf-8', input='content',
                                 lowercase=True, max_df=0.7666666666666666,
                                 max_features=None,
                                 min_df=0.00010572508827796376,
                                 ngram_range=(1, 2), norm='l2',
                                 preprocessor=None, smooth_idf=True,
                                 stop_words='english', strip_accents=None,
                                 sublinear_tf=False, token_pattern='\\w{1,}',
                                 tokenizer=None, use_idf=True,
                                 vocabulary=None)),
                ('clf',
                 LinearSVC(C=99.37702864845689, class_weight=None, dual=False,
                           fit_intercept=False, intercept_scaling=1,
                           loss='squared_hinge', max_iter=1000,
                           multi_class='ovr', penalty='l2', random_state=0,
                           tol=3.8359242531998225e-06, verbose=0))],
         verbose=False),
                   Pipeline(memory=None,
         steps=[('vect',
                 TfidfVectorizer(analyzer='word', binary=False,
                                 decode_error='strict',
                                 dtype=<class 'numpy.float64'>,
                                 encoding='utf-8', input='content',
                                 lowercase=True, max_df=0.7666666666666666,
                                 max_features=None,
                                 min_df=0.00010572508827796376,
                                 ngram_range=(1, 2), norm='l2',
                                 preprocessor=None, smooth_idf=True,
                                 stop_words='english', strip_accents=None,
                                 sublinear_tf=False, token_pattern='\\w{1,}',
                                 tokenizer=None, use_idf=True,
                                 vocabulary=None)),
                ('clf',
                 LinearSVC(C=99.37702864845689, class_weight=None, dual=False,
                           fit_intercept=False, intercept_scaling=1,
                           loss='squared_hinge', max_iter=1000,
                           multi_class='ovr', penalty='l2', random_state=0,
                           tol=3.8359242531998225e-06, verbose=0))],
         verbose=False),
                   MultinomialNB(alpha=1.0, class_prior=None, fit_prior=True),
                   None, None,
                   MultinomialNB(alpha=1.0, class_prior=None, fit_prior=True),
                   Pipeline(memory=None,
         steps=[('vect',
                 TfidfVectorizer(analyzer='word', binary=False,
                                 decode_error='strict',
                                 dtype=<class 'numpy.float64'>,
                                 encoding='utf-8', input='content',
                                 lowercase=True, max_df=0.8555555555555556,
                                 max_features=None,
                                 min_df=0.00016483617040605902,
                                 ngram_range=(1, 2), norm='l2',
                                 preprocessor=None, smooth_idf=True,
                                 stop_words='english', strip_accents...
                                 tokenizer=None, use_idf=True,
                                 vocabulary=None)),
                ('clf',
                 LogisticRegression(C=0.47946121772409134, class_weight=None,
                                    dual=False, fit_intercept=True,
                                    intercept_scaling=1,
                                    l1_ratio=0.9168495407673116, max_iter=75,
                                    multi_class='multinomial', n_jobs=None,
                                    penalty='none', random_state=0,
                                    solver='sag', tol=6.514227791666363e-07,
                                    verbose=0, warm_start=False))],
         verbose=False),
                   Pipeline(memory=None,
         steps=[('vect',
                 TfidfVectorizer(analyzer='word', binary=False,
                                 decode_error='strict',
                                 dtype=<class 'numpy.float64'>,
                                 encoding='utf-8', input='content',
                                 lowercase=True, max_df=0.7666666666666666,
                                 max_features=None,
                                 min_df=0.00010572508827796376,
                                 ngram_range=(1, 2), norm='l2',
                                 preprocessor=None, smooth_idf=True,
                                 stop_words='english', strip_accents=None,
                                 sublinear_tf=False, token_pattern='\\w{1,}',
                                 tokenizer=None, use_idf=True,
                                 vocabulary=None)),
                ('clf',
                 LinearSVC(C=99.37702864845689, class_weight=None, dual=False,
                           fit_intercept=False, intercept_scaling=1,
                           loss='squared_hinge', max_iter=1000,
                           multi_class='ovr', penalty='l2', random_state=0,
                           tol=3.8359242531998225e-06, verbose=0))],
         verbose=False),
                   Pipeline(memory=None,
         steps=[('vect',
                 TfidfVectorizer(analyzer='word', binary=False,
                                 decode_error='strict',
                                 dtype=<class 'numpy.float64'>,
                                 encoding='utf-8', input='content',
                                 lowercase=True, max_df=0.6333333333333333,
                                 max_features=None,
                                 min_df=0.0010083260374073057,
                                 ngram_range=(1, 1), norm='l2',
                                 preprocessor=None, smooth_idf=True,
                                 stop_words='english', strip_accents=...
                                 tokenizer=None, use_idf=True,
                                 vocabulary=None)),
                ('clf',
                 AdaBoostClassifier(algorithm='SAMME',
                                    base_estimator=LinearSVC(C=1.0,
                                                             class_weight=None,
                                                             dual=True,
                                                             fit_intercept=True,
                                                             intercept_scaling=1,
                                                             loss='squared_hinge',
                                                             max_iter=1000,
                                                             multi_class='ovr',
                                                             penalty='l2',
                                                             random_state=None,
                                                             tol=0.0001,
                                                             verbose=0),
                                    learning_rate=0.40053644159285917,
                                    n_estimators=190, random_state=0))],
         verbose=False),
                   Pipeline(memory=None,
         steps=[('vect',
                 TfidfVectorizer(analyzer='word', binary=False,
                                 decode_error='strict',
                                 dtype=<class 'numpy.float64'>,
                                 encoding='utf-8', input='content',
                                 lowercase=True, max_df=0.8555555555555556,
                                 max_features=None,
                                 min_df=0.00016483617040605902,
                                 ngram_range=(1, 2), norm='l2',
                                 preprocessor=None, smooth_idf=True,
                                 stop_words='english', strip_accents...
                                 tokenizer=None, use_idf=True,
                                 vocabulary=None)),
                ('clf',
                 LogisticRegression(C=0.47946121772409134, class_weight=None,
                                    dual=False, fit_intercept=True,
                                    intercept_scaling=1,
                                    l1_ratio=0.9168495407673116, max_iter=75,
                                    multi_class='multinomial', n_jobs=None,
                                    penalty='none', random_state=0,
                                    solver='sag', tol=6.514227791666363e-07,
                                    verbose=0, warm_start=False))],
         verbose=False),
                   Pipeline(memory=None,
         steps=[('vect',
                 TfidfVectorizer(analyzer='word', binary=False,
                                 decode_error='strict',
                                 dtype=<class 'numpy.float64'>,
                                 encoding='utf-8', input='content',
                                 lowercase=True, max_df=0.6777777777777778,
                                 max_features=None,
                                 min_df=0.0012707570261625706,
                                 ngram_range=(1, 2), norm='l2',
                                 preprocessor=None, smooth_idf=True,
                                 stop_words=None, strip_accents=None,...
                                 vocabulary=None)),
                ('clf',
                 DecisionTreeClassifier(ccp_alpha=0.0, class_weight=None,
                                        criterion='gini', max_depth=49,
                                        max_features=0.56, max_leaf_nodes=316,
                                        min_impurity_decrease=1e-07,
                                        min_impurity_split=None,
                                        min_samples_leaf=4,
                                        min_samples_split=0.015198715678453873,
                                        min_weight_fraction_leaf=0.0,
                                        presort='deprecated', random_state=0,
                                        splitter='random'))],
         verbose=False),
                   Pipeline(memory=None,
         steps=[('vect',
                 TfidfVectorizer(analyzer='word', binary=False,
                                 decode_error='strict',
                                 dtype=<class 'numpy.float64'>,
                                 encoding='utf-8', input='content',
                                 lowercase=True, max_df=0.6777777777777778,
                                 max_features=None,
                                 min_df=0.0012707570261625706,
                                 ngram_range=(1, 2), norm='l2',
                                 preprocessor=None, smooth_idf=True,
                                 stop_words=None, strip_accents=None,...
                                 vocabulary=None)),
                ('clf',
                 DecisionTreeClassifier(ccp_alpha=0.0, class_weight=None,
                                        criterion='gini', max_depth=49,
                                        max_features=0.56, max_leaf_nodes=316,
                                        min_impurity_decrease=1e-07,
                                        min_impurity_split=None,
                                        min_samples_leaf=4,
                                        min_samples_split=0.015198715678453873,
                                        min_weight_fraction_leaf=0.0,
                                        presort='deprecated', random_state=0,
                                        splitter='random'))],
         verbose=False),
                   None,
                   Pipeline(memory=None,
         steps=[('vect',
                 TfidfVectorizer(analyzer='word', binary=False,
                                 decode_error='strict',
                                 dtype=<class 'numpy.float64'>,
                                 encoding='utf-8', input='content',
                                 lowercase=True, max_df=0.7222222222222222,
                                 max_features=None,
                                 min_df=0.00014705642569160768,
                                 ngram_range=(1, 1), norm='l2',
                                 preprocessor=None, smooth_idf=True,
                                 stop_words=None, strip_accents=None...
                                        max_features=0.01, max_leaf_nodes=None,
                                        max_samples=0.007808728196651904,
                                        min_impurity_decrease=3.1622776601683795e-10,
                                        min_impurity_split=None,
                                        min_samples_leaf=62,
                                        min_samples_split=0.0008966659727253451,
                                        min_weight_fraction_leaf=0.0007448459487372297,
                                        n_estimators=251, n_jobs=None,
                                        oob_score=False, random_state=0,
                                        verbose=True, warm_start=True))],
         verbose=False),
                   Pipeline(memory=None,
         steps=[('vect',
                 TfidfVectorizer(analyzer='word', binary=False,
                                 decode_error='strict',
                                 dtype=<class 'numpy.float64'>,
                                 encoding='utf-8', input='content',
                                 lowercase=True, max_df=0.7222222222222222,
                                 max_features=None,
                                 min_df=0.00014705642569160768,
                                 ngram_range=(1, 1), norm='l2',
                                 preprocessor=None, smooth_idf=True,
                                 stop_words=None, strip_accents=None...
                                        max_features=0.01, max_leaf_nodes=None,
                                        max_samples=0.007808728196651904,
                                        min_impurity_decrease=3.1622776601683795e-10,
                                        min_impurity_split=None,
                                        min_samples_leaf=62,
                                        min_samples_split=0.0008966659727253451,
                                        min_weight_fraction_leaf=0.0007448459487372297,
                                        n_estimators=251, n_jobs=None,
                                        oob_score=False, random_state=0,
                                        verbose=True, warm_start=True))],
         verbose=False),
                   Pipeline(memory=None,
         steps=[('vect',
                 TfidfVectorizer(analyzer='word', binary=False,
                                 decode_error='strict',
                                 dtype=<class 'numpy.float64'>,
                                 encoding='utf-8', input='content',
                                 lowercase=True, max_df=0.6777777777777778,
                                 max_features=None,
                                 min_df=0.0012707570261625706,
                                 ngram_range=(1, 2), norm='l2',
                                 preprocessor=None, smooth_idf=True,
                                 stop_words=None, strip_accents=None,...
                                 vocabulary=None)),
                ('clf',
                 DecisionTreeClassifier(ccp_alpha=0.0, class_weight=None,
                                        criterion='gini', max_depth=49,
                                        max_features=0.56, max_leaf_nodes=316,
                                        min_impurity_decrease=1e-07,
                                        min_impurity_split=None,
                                        min_samples_leaf=4,
                                        min_samples_split=0.015198715678453873,
                                        min_weight_fraction_leaf=0.0,
                                        presort='deprecated', random_state=0,
                                        splitter='random'))],
         verbose=False),
                   Pipeline(memory=None,
         steps=[('vect',
                 TfidfVectorizer(analyzer='word', binary=False,
                                 decode_error='strict',
                                 dtype=<class 'numpy.float64'>,
                                 encoding='utf-8', input='content',
                                 lowercase=True, max_df=0.6333333333333333,
                                 max_features=None,
                                 min_df=0.0010083260374073057,
                                 ngram_range=(1, 1), norm='l2',
                                 preprocessor=None, smooth_idf=True,
                                 stop_words='english', strip_accents=...
                                 tokenizer=None, use_idf=True,
                                 vocabulary=None)),
                ('clf',
                 AdaBoostClassifier(algorithm='SAMME',
                                    base_estimator=LinearSVC(C=1.0,
                                                             class_weight=None,
                                                             dual=True,
                                                             fit_intercept=True,
                                                             intercept_scaling=1,
                                                             loss='squared_hinge',
                                                             max_iter=1000,
                                                             multi_class='ovr',
                                                             penalty='l2',
                                                             random_state=None,
                                                             tol=0.0001,
                                                             verbose=0),
                                    learning_rate=0.40053644159285917,
                                    n_estimators=190, random_state=0))],
         verbose=False),
                   Pipeline(memory=None,
         steps=[('vect',
                 TfidfVectorizer(analyzer='word', binary=False,
                                 decode_error='strict',
                                 dtype=<class 'numpy.float64'>,
                                 encoding='utf-8', input='content',
                                 lowercase=True, max_df=0.7666666666666666,
                                 max_features=None,
                                 min_df=0.00010572508827796376,
                                 ngram_range=(1, 2), norm='l2',
                                 preprocessor=None, smooth_idf=True,
                                 stop_words='english', strip_accents=None,
                                 sublinear_tf=False, token_pattern='\\w{1,}',
                                 tokenizer=None, use_idf=True,
                                 vocabulary=None)),
                ('clf',
                 LinearSVC(C=99.37702864845689, class_weight=None, dual=False,
                           fit_intercept=False, intercept_scaling=1,
                           loss='squared_hinge', max_iter=1000,
                           multi_class='ovr', penalty='l2', random_state=0,
                           tol=3.8359242531998225e-06, verbose=0))],
         verbose=False),
                   Pipeline(memory=None,
         steps=[('vect',
                 TfidfVectorizer(analyzer='word', binary=False,
                                 decode_error='strict',
                                 dtype=<class 'numpy.float64'>,
                                 encoding='utf-8', input='content',
                                 lowercase=True, max_df=0.6333333333333333,
                                 max_features=None,
                                 min_df=0.0010083260374073057,
                                 ngram_range=(1, 1), norm='l2',
                                 preprocessor=None, smooth_idf=True,
                                 stop_words='english', strip_accents=...
                                 tokenizer=None, use_idf=True,
                                 vocabulary=None)),
                ('clf',
                 AdaBoostClassifier(algorithm='SAMME',
                                    base_estimator=LinearSVC(C=1.0,
                                                             class_weight=None,
                                                             dual=True,
                                                             fit_intercept=True,
                                                             intercept_scaling=1,
                                                             loss='squared_hinge',
                                                             max_iter=1000,
                                                             multi_class='ovr',
                                                             penalty='l2',
                                                             random_state=None,
                                                             tol=0.0001,
                                                             verbose=0),
                                    learning_rate=0.40053644159285917,
                                    n_estimators=190, random_state=0))],
         verbose=False),
                   Pipeline(memory=None,
         steps=[('vect',
                 TfidfVectorizer(analyzer='word', binary=False,
                                 decode_error='strict',
                                 dtype=<class 'numpy.float64'>,
                                 encoding='utf-8', input='content',
                                 lowercase=True, max_df=0.8555555555555556,
                                 max_features=None,
                                 min_df=0.00016483617040605902,
                                 ngram_range=(1, 2), norm='l2',
                                 preprocessor=None, smooth_idf=True,
                                 stop_words='english', strip_accents...
                                 tokenizer=None, use_idf=True,
                                 vocabulary=None)),
                ('clf',
                 LogisticRegression(C=0.47946121772409134, class_weight=None,
                                    dual=False, fit_intercept=True,
                                    intercept_scaling=1,
                                    l1_ratio=0.9168495407673116, max_iter=75,
                                    multi_class='multinomial', n_jobs=None,
                                    penalty='none', random_state=0,
                                    solver='sag', tol=6.514227791666363e-07,
                                    verbose=0, warm_start=False))],
         verbose=False),
                   Pipeline(memory=None,
         steps=[('vect',
                 TfidfVectorizer(analyzer='word', binary=False,
                                 decode_error='strict',
                                 dtype=<class 'numpy.float64'>,
                                 encoding='utf-8', input='content',
                                 lowercase=True, max_df=0.8555555555555556,
                                 max_features=None,
                                 min_df=0.00016483617040605902,
                                 ngram_range=(1, 2), norm='l2',
                                 preprocessor=None, smooth_idf=True,
                                 stop_words='english', strip_accents...
                                 tokenizer=None, use_idf=True,
                                 vocabulary=None)),
                ('clf',
                 LogisticRegression(C=0.47946121772409134, class_weight=None,
                                    dual=False, fit_intercept=True,
                                    intercept_scaling=1,
                                    l1_ratio=0.9168495407673116, max_iter=75,
                                    multi_class='multinomial', n_jobs=None,
                                    penalty='none', random_state=0,
                                    solver='sag', tol=6.514227791666363e-07,
                                    verbose=0, warm_start=False))],
         verbose=False),
                   Pipeline(memory=None,
         steps=[('vect',
                 TfidfVectorizer(analyzer='word', binary=False,
                                 decode_error='strict',
                                 dtype=<class 'numpy.float64'>,
                                 encoding='utf-8', input='content',
                                 lowercase=True, max_df=0.7666666666666666,
                                 max_features=None,
                                 min_df=0.00010572508827796376,
                                 ngram_range=(1, 2), norm='l2',
                                 preprocessor=None, smooth_idf=True,
                                 stop_words='english', strip_accents=None,
                                 sublinear_tf=False, token_pattern='\\w{1,}',
                                 tokenizer=None, use_idf=True,
                                 vocabulary=None)),
                ('clf',
                 LinearSVC(C=99.37702864845689, class_weight=None, dual=False,
                           fit_intercept=False, intercept_scaling=1,
                           loss='squared_hinge', max_iter=1000,
                           multi_class='ovr', penalty='l2', random_state=0,
                           tol=3.8359242531998225e-06, verbose=0))],
         verbose=False),
                   Pipeline(memory=None,
         steps=[('vect',
                 TfidfVectorizer(analyzer='word', binary=False,
                                 decode_error='strict',
                                 dtype=<class 'numpy.float64'>,
                                 encoding='utf-8', input='content',
                                 lowercase=True, max_df=0.7222222222222222,
                                 max_features=None,
                                 min_df=0.00014705642569160768,
                                 ngram_range=(1, 1), norm='l2',
                                 preprocessor=None, smooth_idf=True,
                                 stop_words=None, strip_accents=None...
                                        max_features=0.01, max_leaf_nodes=None,
                                        max_samples=0.007808728196651904,
                                        min_impurity_decrease=3.1622776601683795e-10,
                                        min_impurity_split=None,
                                        min_samples_leaf=62,
                                        min_samples_split=0.0008966659727253451,
                                        min_weight_fraction_leaf=0.0007448459487372297,
                                        n_estimators=251, n_jobs=None,
                                        oob_score=False, random_state=0,
                                        verbose=True, warm_start=True))],
         verbose=False),
                   Pipeline(memory=None,
         steps=[('vect',
                 TfidfVectorizer(analyzer='word', binary=False,
                                 decode_error='strict',
                                 dtype=<class 'numpy.float64'>,
                                 encoding='utf-8', input='content',
                                 lowercase=True, max_df=0.8555555555555556,
                                 max_features=None,
                                 min_df=0.00016483617040605902,
                                 ngram_range=(1, 2), norm='l2',
                                 preprocessor=None, smooth_idf=True,
                                 stop_words='english', strip_accents...
                                 tokenizer=None, use_idf=True,
                                 vocabulary=None)),
                ('clf',
                 LogisticRegression(C=0.47946121772409134, class_weight=None,
                                    dual=False, fit_intercept=True,
                                    intercept_scaling=1,
                                    l1_ratio=0.9168495407673116, max_iter=75,
                                    multi_class='multinomial', n_jobs=None,
                                    penalty='none', random_state=0,
                                    solver='sag', tol=6.514227791666363e-07,
                                    verbose=0, warm_start=False))],
         verbose=False),
                   Pipeline(memory=None,
         steps=[('vect',
                 TfidfVectorizer(analyzer='word', binary=False,
                                 decode_error='strict',
                                 dtype=<class 'numpy.float64'>,
                                 encoding='utf-8', input='content',
                                 lowercase=True, max_df=0.6333333333333333,
                                 max_features=None,
                                 min_df=0.0010083260374073057,
                                 ngram_range=(1, 1), norm='l2',
                                 preprocessor=None, smooth_idf=True,
                                 stop_words='english', strip_accents=...
                                 tokenizer=None, use_idf=True,
                                 vocabulary=None)),
                ('clf',
                 AdaBoostClassifier(algorithm='SAMME',
                                    base_estimator=LinearSVC(C=1.0,
                                                             class_weight=None,
                                                             dual=True,
                                                             fit_intercept=True,
                                                             intercept_scaling=1,
                                                             loss='squared_hinge',
                                                             max_iter=1000,
                                                             multi_class='ovr',
                                                             penalty='l2',
                                                             random_state=None,
                                                             tol=0.0001,
                                                             verbose=0),
                                    learning_rate=0.40053644159285917,
                                    n_estimators=190, random_state=0))],
         verbose=False),
                   MultinomialNB(alpha=1.0, class_prior=None, fit_prior=True),
                   Pipeline(memory=None,
         steps=[('vect',
                 TfidfVectorizer(analyzer='word', binary=False,
                                 decode_error='strict',
                                 dtype=<class 'numpy.float64'>,
                                 encoding='utf-8', input='content',
                                 lowercase=True, max_df=0.6333333333333333,
                                 max_features=None,
                                 min_df=0.0010083260374073057,
                                 ngram_range=(1, 1), norm='l2',
                                 preprocessor=None, smooth_idf=True,
                                 stop_words='english', strip_accents=...
                                 tokenizer=None, use_idf=True,
                                 vocabulary=None)),
                ('clf',
                 AdaBoostClassifier(algorithm='SAMME',
                                    base_estimator=LinearSVC(C=1.0,
                                                             class_weight=None,
                                                             dual=True,
                                                             fit_intercept=True,
                                                             intercept_scaling=1,
                                                             loss='squared_hinge',
                                                             max_iter=1000,
                                                             multi_class='ovr',
                                                             penalty='l2',
                                                             random_state=None,
                                                             tol=0.0001,
                                                             verbose=0),
                                    learning_rate=0.40053644159285917,
                                    n_estimators=190, random_state=0))],
         verbose=False),
                   Pipeline(memory=None,
         steps=[('vect',
                 TfidfVectorizer(analyzer='word', binary=False,
                                 decode_error='strict',
                                 dtype=<class 'numpy.float64'>,
                                 encoding='utf-8', input='content',
                                 lowercase=True, max_df=0.7666666666666666,
                                 max_features=None,
                                 min_df=0.00010572508827796376,
                                 ngram_range=(1, 2), norm='l2',
                                 preprocessor=None, smooth_idf=True,
                                 stop_words='english', strip_accents=None,
                                 sublinear_tf=False, token_pattern='\\w{1,}',
                                 tokenizer=None, use_idf=True,
                                 vocabulary=None)),
                ('clf',
                 LinearSVC(C=99.37702864845689, class_weight=None, dual=False,
                           fit_intercept=False, intercept_scaling=1,
                           loss='squared_hinge', max_iter=1000,
                           multi_class='ovr', penalty='l2', random_state=0,
                           tol=3.8359242531998225e-06, verbose=0))],
         verbose=False),
                   Pipeline(memory=None,
         steps=[('vect',
                 TfidfVectorizer(analyzer='word', binary=False,
                                 decode_error='strict',
                                 dtype=<class 'numpy.float64'>,
                                 encoding='utf-8', input='content',
                                 lowercase=True, max_df=0.6777777777777778,
                                 max_features=None,
                                 min_df=0.0012707570261625706,
                                 ngram_range=(1, 2), norm='l2',
                                 preprocessor=None, smooth_idf=True,
                                 stop_words=None, strip_accents=None,...
                                 vocabulary=None)),
                ('clf',
                 DecisionTreeClassifier(ccp_alpha=0.0, class_weight=None,
                                        criterion='gini', max_depth=49,
                                        max_features=0.56, max_leaf_nodes=316,
                                        min_impurity_decrease=1e-07,
                                        min_impurity_split=None,
                                        min_samples_leaf=4,
                                        min_samples_split=0.015198715678453873,
                                        min_weight_fraction_leaf=0.0,
                                        presort='deprecated', random_state=0,
                                        splitter='random'))],
         verbose=False),
                   Pipeline(memory=None,
         steps=[('vect',
                 TfidfVectorizer(analyzer='word', binary=False,
                                 decode_error='strict',
                                 dtype=<class 'numpy.float64'>,
                                 encoding='utf-8', input='content',
                                 lowercase=True, max_df=0.7222222222222222,
                                 max_features=None,
                                 min_df=0.00014705642569160768,
                                 ngram_range=(1, 1), norm='l2',
                                 preprocessor=None, smooth_idf=True,
                                 stop_words=None, strip_accents=None...
                                        max_features=0.01, max_leaf_nodes=None,
                                        max_samples=0.007808728196651904,
                                        min_impurity_decrease=3.1622776601683795e-10,
                                        min_impurity_split=None,
                                        min_samples_leaf=62,
                                        min_samples_split=0.0008966659727253451,
                                        min_weight_fraction_leaf=0.0007448459487372297,
                                        n_estimators=251, n_jobs=None,
                                        oob_score=False, random_state=0,
                                        verbose=True, warm_start=True))],
         verbose=False),
                   Pipeline(memory=None,
         steps=[('vect',
                 TfidfVectorizer(analyzer='word', binary=False,
                                 decode_error='strict',
                                 dtype=<class 'numpy.float64'>,
                                 encoding='utf-8', input='content',
                                 lowercase=True, max_df=0.8555555555555556,
                                 max_features=None,
                                 min_df=0.00016483617040605902,
                                 ngram_range=(1, 2), norm='l2',
                                 preprocessor=None, smooth_idf=True,
                                 stop_words='english', strip_accents...
                                 tokenizer=None, use_idf=True,
                                 vocabulary=None)),
                ('clf',
                 LogisticRegression(C=0.47946121772409134, class_weight=None,
                                    dual=False, fit_intercept=True,
                                    intercept_scaling=1,
                                    l1_ratio=0.9168495407673116, max_iter=75,
                                    multi_class='multinomial', n_jobs=None,
                                    penalty='none', random_state=0,
                                    solver='sag', tol=6.514227791666363e-07,
                                    verbose=0, warm_start=False))],
         verbose=False),
                   Pipeline(memory=None,
         steps=[('vect',
                 TfidfVectorizer(analyzer='word', binary=False,
                                 decode_error='strict',
                                 dtype=<class 'numpy.float64'>,
                                 encoding='utf-8', input='content',
                                 lowercase=True, max_df=0.7666666666666666,
                                 max_features=None,
                                 min_df=0.00010572508827796376,
                                 ngram_range=(1, 2), norm='l2',
                                 preprocessor=None, smooth_idf=True,
                                 stop_words='english', strip_accents=None,
                                 sublinear_tf=False, token_pattern='\\w{1,}',
                                 tokenizer=None, use_idf=True,
                                 vocabulary=None)),
                ('clf',
                 LinearSVC(C=99.37702864845689, class_weight=None, dual=False,
                           fit_intercept=False, intercept_scaling=1,
                           loss='squared_hinge', max_iter=1000,
                           multi_class='ovr', penalty='l2', random_state=0,
                           tol=3.8359242531998225e-06, verbose=0))],
         verbose=False),
                   Pipeline(memory=None,
         steps=[('vect',
                 TfidfVectorizer(analyzer='word', binary=False,
                                 decode_error='strict',
                                 dtype=<class 'numpy.float64'>,
                                 encoding='utf-8', input='content',
                                 lowercase=True, max_df=0.6777777777777778,
                                 max_features=None,
                                 min_df=0.0012707570261625706,
                                 ngram_range=(1, 2), norm='l2',
                                 preprocessor=None, smooth_idf=True,
                                 stop_words=None, strip_accents=None,...
                                 vocabulary=None)),
                ('clf',
                 DecisionTreeClassifier(ccp_alpha=0.0, class_weight=None,
                                        criterion='gini', max_depth=49,
                                        max_features=0.56, max_leaf_nodes=316,
                                        min_impurity_decrease=1e-07,
                                        min_impurity_split=None,
                                        min_samples_leaf=4,
                                        min_samples_split=0.015198715678453873,
                                        min_weight_fraction_leaf=0.0,
                                        presort='deprecated', random_state=0,
                                        splitter='random'))],
         verbose=False),
                   None,
                   Pipeline(memory=None,
         steps=[('vect',
                 TfidfVectorizer(analyzer='word', binary=False,
                                 decode_error='strict',
                                 dtype=<class 'numpy.float64'>,
                                 encoding='utf-8', input='content',
                                 lowercase=True, max_df=0.6777777777777778,
                                 max_features=None,
                                 min_df=0.0012707570261625706,
                                 ngram_range=(1, 2), norm='l2',
                                 preprocessor=None, smooth_idf=True,
                                 stop_words=None, strip_accents=None,...
                                 vocabulary=None)),
                ('clf',
                 DecisionTreeClassifier(ccp_alpha=0.0, class_weight=None,
                                        criterion='gini', max_depth=49,
                                        max_features=0.56, max_leaf_nodes=316,
                                        min_impurity_decrease=1e-07,
                                        min_impurity_split=None,
                                        min_samples_leaf=4,
                                        min_samples_split=0.015198715678453873,
                                        min_weight_fraction_leaf=0.0,
                                        presort='deprecated', random_state=0,
                                        splitter='random'))],
         verbose=False),
                   Pipeline(memory=None,
         steps=[('vect',
                 TfidfVectorizer(analyzer='word', binary=False,
                                 decode_error='strict',
                                 dtype=<class 'numpy.float64'>,
                                 encoding='utf-8', input='content',
                                 lowercase=True, max_df=0.8555555555555556,
                                 max_features=None,
                                 min_df=0.00016483617040605902,
                                 ngram_range=(1, 2), norm='l2',
                                 preprocessor=None, smooth_idf=True,
                                 stop_words='english', strip_accents...
                                 tokenizer=None, use_idf=True,
                                 vocabulary=None)),
                ('clf',
                 LogisticRegression(C=0.47946121772409134, class_weight=None,
                                    dual=False, fit_intercept=True,
                                    intercept_scaling=1,
                                    l1_ratio=0.9168495407673116, max_iter=75,
                                    multi_class='multinomial', n_jobs=None,
                                    penalty='none', random_state=0,
                                    solver='sag', tol=6.514227791666363e-07,
                                    verbose=0, warm_start=False))],
         verbose=False),
                   Pipeline(memory=None,
         steps=[('vect',
                 TfidfVectorizer(analyzer='word', binary=False,
                                 decode_error='strict',
                                 dtype=<class 'numpy.float64'>,
                                 encoding='utf-8', input='content',
                                 lowercase=True, max_df=0.6777777777777778,
                                 max_features=None,
                                 min_df=0.0012707570261625706,
                                 ngram_range=(1, 2), norm='l2',
                                 preprocessor=None, smooth_idf=True,
                                 stop_words=None, strip_accents=None,...
                                 vocabulary=None)),
                ('clf',
                 DecisionTreeClassifier(ccp_alpha=0.0, class_weight=None,
                                        criterion='gini', max_depth=49,
                                        max_features=0.56, max_leaf_nodes=316,
                                        min_impurity_decrease=1e-07,
                                        min_impurity_split=None,
                                        min_samples_leaf=4,
                                        min_samples_split=0.015198715678453873,
                                        min_weight_fraction_leaf=0.0,
                                        presort='deprecated', random_state=0,
                                        splitter='random'))],
         verbose=False),
                   Pipeline(memory=None,
         steps=[('vect',
                 TfidfVectorizer(analyzer='word', binary=False,
                                 decode_error='strict',
                                 dtype=<class 'numpy.float64'>,
                                 encoding='utf-8', input='content',
                                 lowercase=True, max_df=0.7222222222222222,
                                 max_features=None,
                                 min_df=0.00014705642569160768,
                                 ngram_range=(1, 1), norm='l2',
                                 preprocessor=None, smooth_idf=True,
                                 stop_words=None, strip_accents=None...
                                        max_features=0.01, max_leaf_nodes=None,
                                        max_samples=0.007808728196651904,
                                        min_impurity_decrease=3.1622776601683795e-10,
                                        min_impurity_split=None,
                                        min_samples_leaf=62,
                                        min_samples_split=0.0008966659727253451,
                                        min_weight_fraction_leaf=0.0007448459487372297,
                                        n_estimators=251, n_jobs=None,
                                        oob_score=False, random_state=0,
                                        verbose=True, warm_start=True))],
         verbose=False),
                   Pipeline(memory=None,
         steps=[('vect',
                 TfidfVectorizer(analyzer='word', binary=False,
                                 decode_error='strict',
                                 dtype=<class 'numpy.float64'>,
                                 encoding='utf-8', input='content',
                                 lowercase=True, max_df=0.8555555555555556,
                                 max_features=None,
                                 min_df=0.00016483617040605902,
                                 ngram_range=(1, 2), norm='l2',
                                 preprocessor=None, smooth_idf=True,
                                 stop_words='english', strip_accents...
                                 tokenizer=None, use_idf=True,
                                 vocabulary=None)),
                ('clf',
                 LogisticRegression(C=0.47946121772409134, class_weight=None,
                                    dual=False, fit_intercept=True,
                                    intercept_scaling=1,
                                    l1_ratio=0.9168495407673116, max_iter=75,
                                    multi_class='multinomial', n_jobs=None,
                                    penalty='none', random_state=0,
                                    solver='sag', tol=6.514227791666363e-07,
                                    verbose=0, warm_start=False))],
         verbose=False),
                   Pipeline(memory=None,
         steps=[('vect',
                 TfidfVectorizer(analyzer='word', binary=False,
                                 decode_error='strict',
                                 dtype=<class 'numpy.float64'>,
                                 encoding='utf-8', input='content',
                                 lowercase=True, max_df=0.7222222222222222,
                                 max_features=None,
                                 min_df=0.00014705642569160768,
                                 ngram_range=(1, 1), norm='l2',
                                 preprocessor=None, smooth_idf=True,
                                 stop_words=None, strip_accents=None...
                                        max_features=0.01, max_leaf_nodes=None,
                                        max_samples=0.007808728196651904,
                                        min_impurity_decrease=3.1622776601683795e-10,
                                        min_impurity_split=None,
                                        min_samples_leaf=62,
                                        min_samples_split=0.0008966659727253451,
                                        min_weight_fraction_leaf=0.0007448459487372297,
                                        n_estimators=251, n_jobs=None,
                                        oob_score=False, random_state=0,
                                        verbose=True, warm_start=True))],
         verbose=False),
                   Pipeline(memory=None,
         steps=[('vect',
                 TfidfVectorizer(analyzer='word', binary=False,
                                 decode_error='strict',
                                 dtype=<class 'numpy.float64'>,
                                 encoding='utf-8', input='content',
                                 lowercase=True, max_df=0.7666666666666666,
                                 max_features=None,
                                 min_df=0.00010572508827796376,
                                 ngram_range=(1, 2), norm='l2',
                                 preprocessor=None, smooth_idf=True,
                                 stop_words='english', strip_accents=None,
                                 sublinear_tf=False, token_pattern='\\w{1,}',
                                 tokenizer=None, use_idf=True,
                                 vocabulary=None)),
                ('clf',
                 LinearSVC(C=99.37702864845689, class_weight=None, dual=False,
                           fit_intercept=False, intercept_scaling=1,
                           loss='squared_hinge', max_iter=1000,
                           multi_class='ovr', penalty='l2', random_state=0,
                           tol=3.8359242531998225e-06, verbose=0))],
         verbose=False),
                   Pipeline(memory=None,
         steps=[('vect',
                 TfidfVectorizer(analyzer='word', binary=False,
                                 decode_error='strict',
                                 dtype=<class 'numpy.float64'>,
                                 encoding='utf-8', input='content',
                                 lowercase=True, max_df=0.8555555555555556,
                                 max_features=None,
                                 min_df=0.00016483617040605902,
                                 ngram_range=(1, 2), norm='l2',
                                 preprocessor=None, smooth_idf=True,
                                 stop_words='english', strip_accents...
                                 tokenizer=None, use_idf=True,
                                 vocabulary=None)),
                ('clf',
                 LogisticRegression(C=0.47946121772409134, class_weight=None,
                                    dual=False, fit_intercept=True,
                                    intercept_scaling=1,
                                    l1_ratio=0.9168495407673116, max_iter=75,
                                    multi_class='multinomial', n_jobs=None,
                                    penalty='none', random_state=0,
                                    solver='sag', tol=6.514227791666363e-07,
                                    verbose=0, warm_start=False))],
         verbose=False),
                   Pipeline(memory=None,
         steps=[('vect',
                 TfidfVectorizer(analyzer='word', binary=False,
                                 decode_error='strict',
                                 dtype=<class 'numpy.float64'>,
                                 encoding='utf-8', input='content',
                                 lowercase=True, max_df=0.7222222222222222,
                                 max_features=None,
                                 min_df=0.00014705642569160768,
                                 ngram_range=(1, 1), norm='l2',
                                 preprocessor=None, smooth_idf=True,
                                 stop_words=None, strip_accents=None...
                                        max_features=0.01, max_leaf_nodes=None,
                                        max_samples=0.007808728196651904,
                                        min_impurity_decrease=3.1622776601683795e-10,
                                        min_impurity_split=None,
                                        min_samples_leaf=62,
                                        min_samples_split=0.0008966659727253451,
                                        min_weight_fraction_leaf=0.0007448459487372297,
                                        n_estimators=251, n_jobs=None,
                                        oob_score=False, random_state=0,
                                        verbose=True, warm_start=True))],
         verbose=False),
                   Pipeline(memory=None,
         steps=[('vect',
                 TfidfVectorizer(analyzer='word', binary=False,
                                 decode_error='strict',
                                 dtype=<class 'numpy.float64'>,
                                 encoding='utf-8', input='content',
                                 lowercase=True, max_df=0.6333333333333333,
                                 max_features=None,
                                 min_df=0.0010083260374073057,
                                 ngram_range=(1, 1), norm='l2',
                                 preprocessor=None, smooth_idf=True,
                                 stop_words='english', strip_accents=...
                                 tokenizer=None, use_idf=True,
                                 vocabulary=None)),
                ('clf',
                 AdaBoostClassifier(algorithm='SAMME',
                                    base_estimator=LinearSVC(C=1.0,
                                                             class_weight=None,
                                                             dual=True,
                                                             fit_intercept=True,
                                                             intercept_scaling=1,
                                                             loss='squared_hinge',
                                                             max_iter=1000,
                                                             multi_class='ovr',
                                                             penalty='l2',
                                                             random_state=None,
                                                             tol=0.0001,
                                                             verbose=0),
                                    learning_rate=0.40053644159285917,
                                    n_estimators=190, random_state=0))],
         verbose=False),
                   None,
                   Pipeline(memory=None,
         steps=[('vect',
                 TfidfVectorizer(analyzer='word', binary=False,
                                 decode_error='strict',
                                 dtype=<class 'numpy.float64'>,
                                 encoding='utf-8', input='content',
                                 lowercase=True, max_df=0.8555555555555556,
                                 max_features=None,
                                 min_df=0.00016483617040605902,
                                 ngram_range=(1, 2), norm='l2',
                                 preprocessor=None, smooth_idf=True,
                                 stop_words='english', strip_accents...
                                 tokenizer=None, use_idf=True,
                                 vocabulary=None)),
                ('clf',
                 LogisticRegression(C=0.47946121772409134, class_weight=None,
                                    dual=False, fit_intercept=True,
                                    intercept_scaling=1,
                                    l1_ratio=0.9168495407673116, max_iter=75,
                                    multi_class='multinomial', n_jobs=None,
                                    penalty='none', random_state=0,
                                    solver='sag', tol=6.514227791666363e-07,
                                    verbose=0, warm_start=False))],
         verbose=False),
                   MultinomialNB(alpha=1.0, class_prior=None, fit_prior=True),
                   Pipeline(memory=None,
         steps=[('vect',
                 TfidfVectorizer(analyzer='word', binary=False,
                                 decode_error='strict',
                                 dtype=<class 'numpy.float64'>,
                                 encoding='utf-8', input='content',
                                 lowercase=True, max_df=0.7222222222222222,
                                 max_features=None,
                                 min_df=0.00014705642569160768,
                                 ngram_range=(1, 1), norm='l2',
                                 preprocessor=None, smooth_idf=True,
                                 stop_words=None, strip_accents=None...
                                        max_features=0.01, max_leaf_nodes=None,
                                        max_samples=0.007808728196651904,
                                        min_impurity_decrease=3.1622776601683795e-10,
                                        min_impurity_split=None,
                                        min_samples_leaf=62,
                                        min_samples_split=0.0008966659727253451,
                                        min_weight_fraction_leaf=0.0007448459487372297,
                                        n_estimators=251, n_jobs=None,
                                        oob_score=False, random_state=0,
                                        verbose=True, warm_start=True))],
         verbose=False),
                   Pipeline(memory=None,
         steps=[('vect',
                 TfidfVectorizer(analyzer='word', binary=False,
                                 decode_error='strict',
                                 dtype=<class 'numpy.float64'>,
                                 encoding='utf-8', input='content',
                                 lowercase=True, max_df=0.6777777777777778,
                                 max_features=None,
                                 min_df=0.0012707570261625706,
                                 ngram_range=(1, 2), norm='l2',
                                 preprocessor=None, smooth_idf=True,
                                 stop_words=None, strip_accents=None,...
                                 vocabulary=None)),
                ('clf',
                 DecisionTreeClassifier(ccp_alpha=0.0, class_weight=None,
                                        criterion='gini', max_depth=49,
                                        max_features=0.56, max_leaf_nodes=316,
                                        min_impurity_decrease=1e-07,
                                        min_impurity_split=None,
                                        min_samples_leaf=4,
                                        min_samples_split=0.015198715678453873,
                                        min_weight_fraction_leaf=0.0,
                                        presort='deprecated', random_state=0,
                                        splitter='random'))],
         verbose=False),
                   Pipeline(memory=None,
         steps=[('vect',
                 TfidfVectorizer(analyzer='word', binary=False,
                                 decode_error='strict',
                                 dtype=<class 'numpy.float64'>,
                                 encoding='utf-8', input='content',
                                 lowercase=True, max_df=0.6333333333333333,
                                 max_features=None,
                                 min_df=0.0010083260374073057,
                                 ngram_range=(1, 1), norm='l2',
                                 preprocessor=None, smooth_idf=True,
                                 stop_words='english', strip_accents=...
                                 tokenizer=None, use_idf=True,
                                 vocabulary=None)),
                ('clf',
                 AdaBoostClassifier(algorithm='SAMME',
                                    base_estimator=LinearSVC(C=1.0,
                                                             class_weight=None,
                                                             dual=True,
                                                             fit_intercept=True,
                                                             intercept_scaling=1,
                                                             loss='squared_hinge',
                                                             max_iter=1000,
                                                             multi_class='ovr',
                                                             penalty='l2',
                                                             random_state=None,
                                                             tol=0.0001,
                                                             verbose=0),
                                    learning_rate=0.40053644159285917,
                                    n_estimators=190, random_state=0))],
         verbose=False),
                   None,
                   Pipeline(memory=None,
         steps=[('vect',
                 TfidfVectorizer(analyzer='word', binary=False,
                                 decode_error='strict',
                                 dtype=<class 'numpy.float64'>,
                                 encoding='utf-8', input='content',
                                 lowercase=True, max_df=0.7666666666666666,
                                 max_features=None,
                                 min_df=0.00010572508827796376,
                                 ngram_range=(1, 2), norm='l2',
                                 preprocessor=None, smooth_idf=True,
                                 stop_words='english', strip_accents=None,
                                 sublinear_tf=False, token_pattern='\\w{1,}',
                                 tokenizer=None, use_idf=True,
                                 vocabulary=None)),
                ('clf',
                 LinearSVC(C=99.37702864845689, class_weight=None, dual=False,
                           fit_intercept=False, intercept_scaling=1,
                           loss='squared_hinge', max_iter=1000,
                           multi_class='ovr', penalty='l2', random_state=0,
                           tol=3.8359242531998225e-06, verbose=0))],
         verbose=False),
                   None, None, None,
                   Pipeline(memory=None,
         steps=[('vect',
                 TfidfVectorizer(analyzer='word', binary=False,
                                 decode_error='strict',
                                 dtype=<class 'numpy.float64'>,
                                 encoding='utf-8', input='content',
                                 lowercase=True, max_df=0.7666666666666666,
                                 max_features=None,
                                 min_df=0.00010572508827796376,
                                 ngram_range=(1, 2), norm='l2',
                                 preprocessor=None, smooth_idf=True,
                                 stop_words='english', strip_accents=None,
                                 sublinear_tf=False, token_pattern='\\w{1,}',
                                 tokenizer=None, use_idf=True,
                                 vocabulary=None)),
                ('clf',
                 LinearSVC(C=99.37702864845689, class_weight=None, dual=False,
                           fit_intercept=False, intercept_scaling=1,
                           loss='squared_hinge', max_iter=1000,
                           multi_class='ovr', penalty='l2', random_state=0,
                           tol=3.8359242531998225e-06, verbose=0))],
         verbose=False),
                   None,
                   MultinomialNB(alpha=1.0, class_prior=None, fit_prior=True),
                   MultinomialNB(alpha=1.0, class_prior=None, fit_prior=True),
                   Pipeline(memory=None,
         steps=[('vect',
                 TfidfVectorizer(analyzer='word', binary=False,
                                 decode_error='strict',
                                 dtype=<class 'numpy.float64'>,
                                 encoding='utf-8', input='content',
                                 lowercase=True, max_df=0.6333333333333333,
                                 max_features=None,
                                 min_df=0.0010083260374073057,
                                 ngram_range=(1, 1), norm='l2',
                                 preprocessor=None, smooth_idf=True,
                                 stop_words='english', strip_accents=...
                                 tokenizer=None, use_idf=True,
                                 vocabulary=None)),
                ('clf',
                 AdaBoostClassifier(algorithm='SAMME',
                                    base_estimator=LinearSVC(C=1.0,
                                                             class_weight=None,
                                                             dual=True,
                                                             fit_intercept=True,
                                                             intercept_scaling=1,
                                                             loss='squared_hinge',
                                                             max_iter=1000,
                                                             multi_class='ovr',
                                                             penalty='l2',
                                                             random_state=None,
                                                             tol=0.0001,
                                                             verbose=0),
                                    learning_rate=0.40053644159285917,
                                    n_estimators=190, random_state=0))],
         verbose=False),
                   MultinomialNB(alpha=1.0, class_prior=None, fit_prior=True),
                   Pipeline(memory=None,
         steps=[('vect',
                 TfidfVectorizer(analyzer='word', binary=False,
                                 decode_error='strict',
                                 dtype=<class 'numpy.float64'>,
                                 encoding='utf-8', input='content',
                                 lowercase=True, max_df=0.6333333333333333,
                                 max_features=None,
                                 min_df=0.0010083260374073057,
                                 ngram_range=(1, 1), norm='l2',
                                 preprocessor=None, smooth_idf=True,
                                 stop_words='english', strip_accents=...
                                 tokenizer=None, use_idf=True,
                                 vocabulary=None)),
                ('clf',
                 AdaBoostClassifier(algorithm='SAMME',
                                    base_estimator=LinearSVC(C=1.0,
                                                             class_weight=None,
                                                             dual=True,
                                                             fit_intercept=True,
                                                             intercept_scaling=1,
                                                             loss='squared_hinge',
                                                             max_iter=1000,
                                                             multi_class='ovr',
                                                             penalty='l2',
                                                             random_state=None,
                                                             tol=0.0001,
                                                             verbose=0),
                                    learning_rate=0.40053644159285917,
                                    n_estimators=190, random_state=0))],
         verbose=False),
                   Pipeline(memory=None,
         steps=[('vect',
                 TfidfVectorizer(analyzer='word', binary=False,
                                 decode_error='strict',
                                 dtype=<class 'numpy.float64'>,
                                 encoding='utf-8', input='content',
                                 lowercase=True, max_df=0.6777777777777778,
                                 max_features=None,
                                 min_df=0.0012707570261625706,
                                 ngram_range=(1, 2), norm='l2',
                                 preprocessor=None, smooth_idf=True,
                                 stop_words=None, strip_accents=None,...
                                 vocabulary=None)),
                ('clf',
                 DecisionTreeClassifier(ccp_alpha=0.0, class_weight=None,
                                        criterion='gini', max_depth=49,
                                        max_features=0.56, max_leaf_nodes=316,
                                        min_impurity_decrease=1e-07,
                                        min_impurity_split=None,
                                        min_samples_leaf=4,
                                        min_samples_split=0.015198715678453873,
                                        min_weight_fraction_leaf=0.0,
                                        presort='deprecated', random_state=0,
                                        splitter='random'))],
         verbose=False),
                   Pipeline(memory=None,
         steps=[('vect',
                 TfidfVectorizer(analyzer='word', binary=False,
                                 decode_error='strict',
                                 dtype=<class 'numpy.float64'>,
                                 encoding='utf-8', input='content',
                                 lowercase=True, max_df=0.7222222222222222,
                                 max_features=None,
                                 min_df=0.00014705642569160768,
                                 ngram_range=(1, 1), norm='l2',
                                 preprocessor=None, smooth_idf=True,
                                 stop_words=None, strip_accents=None...
                                        max_features=0.01, max_leaf_nodes=None,
                                        max_samples=0.007808728196651904,
                                        min_impurity_decrease=3.1622776601683795e-10,
                                        min_impurity_split=None,
                                        min_samples_leaf=62,
                                        min_samples_split=0.0008966659727253451,
                                        min_weight_fraction_leaf=0.0007448459487372297,
                                        n_estimators=251, n_jobs=None,
                                        oob_score=False, random_state=0,
                                        verbose=True, warm_start=True))],
         verbose=False),
                   Pipeline(memory=None,
         steps=[('vect',
                 TfidfVectorizer(analyzer='word', binary=False,
                                 decode_error='strict',
                                 dtype=<class 'numpy.float64'>,
                                 encoding='utf-8', input='content',
                                 lowercase=True, max_df=0.7666666666666666,
                                 max_features=None,
                                 min_df=0.00010572508827796376,
                                 ngram_range=(1, 2), norm='l2',
                                 preprocessor=None, smooth_idf=True,
                                 stop_words='english', strip_accents=None,
                                 sublinear_tf=False, token_pattern='\\w{1,}',
                                 tokenizer=None, use_idf=True,
                                 vocabulary=None)),
                ('clf',
                 LinearSVC(C=99.37702864845689, class_weight=None, dual=False,
                           fit_intercept=False, intercept_scaling=1,
                           loss='squared_hinge', max_iter=1000,
                           multi_class='ovr', penalty='l2', random_state=0,
                           tol=3.8359242531998225e-06, verbose=0))],
         verbose=False),
                   Pipeline(memory=None,
         steps=[('vect',
                 TfidfVectorizer(analyzer='word', binary=False,
                                 decode_error='strict',
                                 dtype=<class 'numpy.float64'>,
                                 encoding='utf-8', input='content',
                                 lowercase=True, max_df=0.6333333333333333,
                                 max_features=None,
                                 min_df=0.0010083260374073057,
                                 ngram_range=(1, 1), norm='l2',
                                 preprocessor=None, smooth_idf=True,
                                 stop_words='english', strip_accents=...
                                 tokenizer=None, use_idf=True,
                                 vocabulary=None)),
                ('clf',
                 AdaBoostClassifier(algorithm='SAMME',
                                    base_estimator=LinearSVC(C=1.0,
                                                             class_weight=None,
                                                             dual=True,
                                                             fit_intercept=True,
                                                             intercept_scaling=1,
                                                             loss='squared_hinge',
                                                             max_iter=1000,
                                                             multi_class='ovr',
                                                             penalty='l2',
                                                             random_state=None,
                                                             tol=0.0001,
                                                             verbose=0),
                                    learning_rate=0.40053644159285917,
                                    n_estimators=190, random_state=0))],
         verbose=False),
                   MultinomialNB(alpha=1.0, class_prior=None, fit_prior=True),
                   Pipeline(memory=None,
         steps=[('vect',
                 TfidfVectorizer(analyzer='word', binary=False,
                                 decode_error='strict',
                                 dtype=<class 'numpy.float64'>,
                                 encoding='utf-8', input='content',
                                 lowercase=True, max_df=0.6777777777777778,
                                 max_features=None,
                                 min_df=0.0012707570261625706,
                                 ngram_range=(1, 2), norm='l2',
                                 preprocessor=None, smooth_idf=True,
                                 stop_words=None, strip_accents=None,...
                                 vocabulary=None)),
                ('clf',
                 DecisionTreeClassifier(ccp_alpha=0.0, class_weight=None,
                                        criterion='gini', max_depth=49,
                                        max_features=0.56, max_leaf_nodes=316,
                                        min_impurity_decrease=1e-07,
                                        min_impurity_split=None,
                                        min_samples_leaf=4,
                                        min_samples_split=0.015198715678453873,
                                        min_weight_fraction_leaf=0.0,
                                        presort='deprecated', random_state=0,
                                        splitter='random'))],
         verbose=False),
                   MultinomialNB(alpha=1.0, class_prior=None, fit_prior=True),
                   Pipeline(memory=None,
         steps=[('vect',
                 TfidfVectorizer(analyzer='word', binary=False,
                                 decode_error='strict',
                                 dtype=<class 'numpy.float64'>,
                                 encoding='utf-8', input='content',
                                 lowercase=True, max_df=0.6777777777777778,
                                 max_features=None,
                                 min_df=0.0012707570261625706,
                                 ngram_range=(1, 2), norm='l2',
                                 preprocessor=None, smooth_idf=True,
                                 stop_words=None, strip_accents=None,...
                                 vocabulary=None)),
                ('clf',
                 DecisionTreeClassifier(ccp_alpha=0.0, class_weight=None,
                                        criterion='gini', max_depth=49,
                                        max_features=0.56, max_leaf_nodes=316,
                                        min_impurity_decrease=1e-07,
                                        min_impurity_split=None,
                                        min_samples_leaf=4,
                                        min_samples_split=0.015198715678453873,
                                        min_weight_fraction_leaf=0.0,
                                        presort='deprecated', random_state=0,
                                        splitter='random'))],
         verbose=False),
                   None,
                   Pipeline(memory=None,
         steps=[('vect',
                 TfidfVectorizer(analyzer='word', binary=False,
                                 decode_error='strict',
                                 dtype=<class 'numpy.float64'>,
                                 encoding='utf-8', input='content',
                                 lowercase=True, max_df=0.8555555555555556,
                                 max_features=None,
                                 min_df=0.00016483617040605902,
                                 ngram_range=(1, 2), norm='l2',
                                 preprocessor=None, smooth_idf=True,
                                 stop_words='english', strip_accents...
                                 tokenizer=None, use_idf=True,
                                 vocabulary=None)),
                ('clf',
                 LogisticRegression(C=0.47946121772409134, class_weight=None,
                                    dual=False, fit_intercept=True,
                                    intercept_scaling=1,
                                    l1_ratio=0.9168495407673116, max_iter=75,
                                    multi_class='multinomial', n_jobs=None,
                                    penalty='none', random_state=0,
                                    solver='sag', tol=6.514227791666363e-07,
                                    verbose=0, warm_start=False))],
         verbose=False),
                   MultinomialNB(alpha=1.0, class_prior=None, fit_prior=True),
                   Pipeline(memory=None,
         steps=[('vect',
                 TfidfVectorizer(analyzer='word', binary=False,
                                 decode_error='strict',
                                 dtype=<class 'numpy.float64'>,
                                 encoding='utf-8', input='content',
                                 lowercase=True, max_df=0.6333333333333333,
                                 max_features=None,
                                 min_df=0.0010083260374073057,
                                 ngram_range=(1, 1), norm='l2',
                                 preprocessor=None, smooth_idf=True,
                                 stop_words='english', strip_accents=...
                                 tokenizer=None, use_idf=True,
                                 vocabulary=None)),
                ('clf',
                 AdaBoostClassifier(algorithm='SAMME',
                                    base_estimator=LinearSVC(C=1.0,
                                                             class_weight=None,
                                                             dual=True,
                                                             fit_intercept=True,
                                                             intercept_scaling=1,
                                                             loss='squared_hinge',
                                                             max_iter=1000,
                                                             multi_class='ovr',
                                                             penalty='l2',
                                                             random_state=None,
                                                             tol=0.0001,
                                                             verbose=0),
                                    learning_rate=0.40053644159285917,
                                    n_estimators=190, random_state=0))],
         verbose=False),
                   Pipeline(memory=None,
         steps=[('vect',
                 TfidfVectorizer(analyzer='word', binary=False,
                                 decode_error='strict',
                                 dtype=<class 'numpy.float64'>,
                                 encoding='utf-8', input='content',
                                 lowercase=True, max_df=0.6777777777777778,
                                 max_features=None,
                                 min_df=0.0012707570261625706,
                                 ngram_range=(1, 2), norm='l2',
                                 preprocessor=None, smooth_idf=True,
                                 stop_words=None, strip_accents=None,...
                                 vocabulary=None)),
                ('clf',
                 DecisionTreeClassifier(ccp_alpha=0.0, class_weight=None,
                                        criterion='gini', max_depth=49,
                                        max_features=0.56, max_leaf_nodes=316,
                                        min_impurity_decrease=1e-07,
                                        min_impurity_split=None,
                                        min_samples_leaf=4,
                                        min_samples_split=0.015198715678453873,
                                        min_weight_fraction_leaf=0.0,
                                        presort='deprecated', random_state=0,
                                        splitter='random'))],
         verbose=False),
                   Pipeline(memory=None,
         steps=[('vect',
                 TfidfVectorizer(analyzer='word', binary=False,
                                 decode_error='strict',
                                 dtype=<class 'numpy.float64'>,
                                 encoding='utf-8', input='content',
                                 lowercase=True, max_df=0.6333333333333333,
                                 max_features=None,
                                 min_df=0.0010083260374073057,
                                 ngram_range=(1, 1), norm='l2',
                                 preprocessor=None, smooth_idf=True,
                                 stop_words='english', strip_accents=...
                                 tokenizer=None, use_idf=True,
                                 vocabulary=None)),
                ('clf',
                 AdaBoostClassifier(algorithm='SAMME',
                                    base_estimator=LinearSVC(C=1.0,
                                                             class_weight=None,
                                                             dual=True,
                                                             fit_intercept=True,
                                                             intercept_scaling=1,
                                                             loss='squared_hinge',
                                                             max_iter=1000,
                                                             multi_class='ovr',
                                                             penalty='l2',
                                                             random_state=None,
                                                             tol=0.0001,
                                                             verbose=0),
                                    learning_rate=0.40053644159285917,
                                    n_estimators=190, random_state=0))],
         verbose=False),
                   MultinomialNB(alpha=1.0, class_prior=None, fit_prior=True),
                   Pipeline(memory=None,
         steps=[('vect',
                 TfidfVectorizer(analyzer='word', binary=False,
                                 decode_error='strict',
                                 dtype=<class 'numpy.float64'>,
                                 encoding='utf-8', input='content',
                                 lowercase=True, max_df=0.7666666666666666,
                                 max_features=None,
                                 min_df=0.00010572508827796376,
                                 ngram_range=(1, 2), norm='l2',
                                 preprocessor=None, smooth_idf=True,
                                 stop_words='english', strip_accents=None,
                                 sublinear_tf=False, token_pattern='\\w{1,}',
                                 tokenizer=None, use_idf=True,
                                 vocabulary=None)),
                ('clf',
                 LinearSVC(C=99.37702864845689, class_weight=None, dual=False,
                           fit_intercept=False, intercept_scaling=1,
                           loss='squared_hinge', max_iter=1000,
                           multi_class='ovr', penalty='l2', random_state=0,
                           tol=3.8359242531998225e-06, verbose=0))],
         verbose=False),
                   Pipeline(memory=None,
         steps=[('vect',
                 TfidfVectorizer(analyzer='word', binary=False,
                                 decode_error='strict',
                                 dtype=<class 'numpy.float64'>,
                                 encoding='utf-8', input='content',
                                 lowercase=True, max_df=0.7222222222222222,
                                 max_features=None,
                                 min_df=0.00014705642569160768,
                                 ngram_range=(1, 1), norm='l2',
                                 preprocessor=None, smooth_idf=True,
                                 stop_words=None, strip_accents=None...
                                        max_features=0.01, max_leaf_nodes=None,
                                        max_samples=0.007808728196651904,
                                        min_impurity_decrease=3.1622776601683795e-10,
                                        min_impurity_split=None,
                                        min_samples_leaf=62,
                                        min_samples_split=0.0008966659727253451,
                                        min_weight_fraction_leaf=0.0007448459487372297,
                                        n_estimators=251, n_jobs=None,
                                        oob_score=False, random_state=0,
                                        verbose=True, warm_start=True))],
         verbose=False),
                   Pipeline(memory=None,
         steps=[('vect',
                 TfidfVectorizer(analyzer='word', binary=False,
                                 decode_error='strict',
                                 dtype=<class 'numpy.float64'>,
                                 encoding='utf-8', input='content',
                                 lowercase=True, max_df=0.6333333333333333,
                                 max_features=None,
                                 min_df=0.0010083260374073057,
                                 ngram_range=(1, 1), norm='l2',
                                 preprocessor=None, smooth_idf=True,
                                 stop_words='english', strip_accents=...
                                 tokenizer=None, use_idf=True,
                                 vocabulary=None)),
                ('clf',
                 AdaBoostClassifier(algorithm='SAMME',
                                    base_estimator=LinearSVC(C=1.0,
                                                             class_weight=None,
                                                             dual=True,
                                                             fit_intercept=True,
                                                             intercept_scaling=1,
                                                             loss='squared_hinge',
                                                             max_iter=1000,
                                                             multi_class='ovr',
                                                             penalty='l2',
                                                             random_state=None,
                                                             tol=0.0001,
                                                             verbose=0),
                                    learning_rate=0.40053644159285917,
                                    n_estimators=190, random_state=0))],
         verbose=False),
                   Pipeline(memory=None,
         steps=[('vect',
                 TfidfVectorizer(analyzer='word', binary=False,
                                 decode_error='strict',
                                 dtype=<class 'numpy.float64'>,
                                 encoding='utf-8', input='content',
                                 lowercase=True, max_df=0.7666666666666666,
                                 max_features=None,
                                 min_df=0.00010572508827796376,
                                 ngram_range=(1, 2), norm='l2',
                                 preprocessor=None, smooth_idf=True,
                                 stop_words='english', strip_accents=None,
                                 sublinear_tf=False, token_pattern='\\w{1,}',
                                 tokenizer=None, use_idf=True,
                                 vocabulary=None)),
                ('clf',
                 LinearSVC(C=99.37702864845689, class_weight=None, dual=False,
                           fit_intercept=False, intercept_scaling=1,
                           loss='squared_hinge', max_iter=1000,
                           multi_class='ovr', penalty='l2', random_state=0,
                           tol=3.8359242531998225e-06, verbose=0))],
         verbose=False),
                   Pipeline(memory=None,
         steps=[('vect',
                 TfidfVectorizer(analyzer='word', binary=False,
                                 decode_error='strict',
                                 dtype=<class 'numpy.float64'>,
                                 encoding='utf-8', input='content',
                                 lowercase=True, max_df=0.6333333333333333,
                                 max_features=None,
                                 min_df=0.0010083260374073057,
                                 ngram_range=(1, 1), norm='l2',
                                 preprocessor=None, smooth_idf=True,
                                 stop_words='english', strip_accents=...
                                 tokenizer=None, use_idf=True,
                                 vocabulary=None)),
                ('clf',
                 AdaBoostClassifier(algorithm='SAMME',
                                    base_estimator=LinearSVC(C=1.0,
                                                             class_weight=None,
                                                             dual=True,
                                                             fit_intercept=True,
                                                             intercept_scaling=1,
                                                             loss='squared_hinge',
                                                             max_iter=1000,
                                                             multi_class='ovr',
                                                             penalty='l2',
                                                             random_state=None,
                                                             tol=0.0001,
                                                             verbose=0),
                                    learning_rate=0.40053644159285917,
                                    n_estimators=190, random_state=0))],
         verbose=False),
                   Pipeline(memory=None,
         steps=[('vect',
                 TfidfVectorizer(analyzer='word', binary=False,
                                 decode_error='strict',
                                 dtype=<class 'numpy.float64'>,
                                 encoding='utf-8', input='content',
                                 lowercase=True, max_df=0.6333333333333333,
                                 max_features=None,
                                 min_df=0.0010083260374073057,
                                 ngram_range=(1, 1), norm='l2',
                                 preprocessor=None, smooth_idf=True,
                                 stop_words='english', strip_accents=...
                                 tokenizer=None, use_idf=True,
                                 vocabulary=None)),
                ('clf',
                 AdaBoostClassifier(algorithm='SAMME',
                                    base_estimator=LinearSVC(C=1.0,
                                                             class_weight=None,
                                                             dual=True,
                                                             fit_intercept=True,
                                                             intercept_scaling=1,
                                                             loss='squared_hinge',
                                                             max_iter=1000,
                                                             multi_class='ovr',
                                                             penalty='l2',
                                                             random_state=None,
                                                             tol=0.0001,
                                                             verbose=0),
                                    learning_rate=0.40053644159285917,
                                    n_estimators=190, random_state=0))],
         verbose=False),
                   Pipeline(memory=None,
         steps=[('vect',
                 TfidfVectorizer(analyzer='word', binary=False,
                                 decode_error='strict',
                                 dtype=<class 'numpy.float64'>,
                                 encoding='utf-8', input='content',
                                 lowercase=True, max_df=0.8555555555555556,
                                 max_features=None,
                                 min_df=0.00016483617040605902,
                                 ngram_range=(1, 2), norm='l2',
                                 preprocessor=None, smooth_idf=True,
                                 stop_words='english', strip_accents...
                                 tokenizer=None, use_idf=True,
                                 vocabulary=None)),
                ('clf',
                 LogisticRegression(C=0.47946121772409134, class_weight=None,
                                    dual=False, fit_intercept=True,
                                    intercept_scaling=1,
                                    l1_ratio=0.9168495407673116, max_iter=75,
                                    multi_class='multinomial', n_jobs=None,
                                    penalty='none', random_state=0,
                                    solver='sag', tol=6.514227791666363e-07,
                                    verbose=0, warm_start=False))],
         verbose=False),
                   None,
                   Pipeline(memory=None,
         steps=[('vect',
                 TfidfVectorizer(analyzer='word', binary=False,
                                 decode_error='strict',
                                 dtype=<class 'numpy.float64'>,
                                 encoding='utf-8', input='content',
                                 lowercase=True, max_df=0.6777777777777778,
                                 max_features=None,
                                 min_df=0.0012707570261625706,
                                 ngram_range=(1, 2), norm='l2',
                                 preprocessor=None, smooth_idf=True,
                                 stop_words=None, strip_accents=None,...
                                 vocabulary=None)),
                ('clf',
                 DecisionTreeClassifier(ccp_alpha=0.0, class_weight=None,
                                        criterion='gini', max_depth=49,
                                        max_features=0.56, max_leaf_nodes=316,
                                        min_impurity_decrease=1e-07,
                                        min_impurity_split=None,
                                        min_samples_leaf=4,
                                        min_samples_split=0.015198715678453873,
                                        min_weight_fraction_leaf=0.0,
                                        presort='deprecated', random_state=0,
                                        splitter='random'))],
         verbose=False),
                   MultinomialNB(alpha=1.0, class_prior=None, fit_prior=True),
                   Pipeline(memory=None,
         steps=[('vect',
                 TfidfVectorizer(analyzer='word', binary=False,
                                 decode_error='strict',
                                 dtype=<class 'numpy.float64'>,
                                 encoding='utf-8', input='content',
                                 lowercase=True, max_df=0.7222222222222222,
                                 max_features=None,
                                 min_df=0.00014705642569160768,
                                 ngram_range=(1, 1), norm='l2',
                                 preprocessor=None, smooth_idf=True,
                                 stop_words=None, strip_accents=None...
                                        max_features=0.01, max_leaf_nodes=None,
                                        max_samples=0.007808728196651904,
                                        min_impurity_decrease=3.1622776601683795e-10,
                                        min_impurity_split=None,
                                        min_samples_leaf=62,
                                        min_samples_split=0.0008966659727253451,
                                        min_weight_fraction_leaf=0.0007448459487372297,
                                        n_estimators=251, n_jobs=None,
                                        oob_score=False, random_state=0,
                                        verbose=True, warm_start=True))],
         verbose=False),
                   Pipeline(memory=None,
         steps=[('vect',
                 TfidfVectorizer(analyzer='word', binary=False,
                                 decode_error='strict',
                                 dtype=<class 'numpy.float64'>,
                                 encoding='utf-8', input='content',
                                 lowercase=True, max_df=0.6777777777777778,
                                 max_features=None,
                                 min_df=0.0012707570261625706,
                                 ngram_range=(1, 2), norm='l2',
                                 preprocessor=None, smooth_idf=True,
                                 stop_words=None, strip_accents=None,...
                                 vocabulary=None)),
                ('clf',
                 DecisionTreeClassifier(ccp_alpha=0.0, class_weight=None,
                                        criterion='gini', max_depth=49,
                                        max_features=0.56, max_leaf_nodes=316,
                                        min_impurity_decrease=1e-07,
                                        min_impurity_split=None,
                                        min_samples_leaf=4,
                                        min_samples_split=0.015198715678453873,
                                        min_weight_fraction_leaf=0.0,
                                        presort='deprecated', random_state=0,
                                        splitter='random'))],
         verbose=False),
                   Pipeline(memory=None,
         steps=[('vect',
                 TfidfVectorizer(analyzer='word', binary=False,
                                 decode_error='strict',
                                 dtype=<class 'numpy.float64'>,
                                 encoding='utf-8', input='content',
                                 lowercase=True, max_df=0.6777777777777778,
                                 max_features=None,
                                 min_df=0.0012707570261625706,
                                 ngram_range=(1, 2), norm='l2',
                                 preprocessor=None, smooth_idf=True,
                                 stop_words=None, strip_accents=None,...
                                 vocabulary=None)),
                ('clf',
                 DecisionTreeClassifier(ccp_alpha=0.0, class_weight=None,
                                        criterion='gini', max_depth=49,
                                        max_features=0.56, max_leaf_nodes=316,
                                        min_impurity_decrease=1e-07,
                                        min_impurity_split=None,
                                        min_samples_leaf=4,
                                        min_samples_split=0.015198715678453873,
                                        min_weight_fraction_leaf=0.0,
                                        presort='deprecated', random_state=0,
                                        splitter='random'))],
         verbose=False),
                   Pipeline(memory=None,
         steps=[('vect',
                 TfidfVectorizer(analyzer='word', binary=False,
                                 decode_error='strict',
                                 dtype=<class 'numpy.float64'>,
                                 encoding='utf-8', input='content',
                                 lowercase=True, max_df=0.6777777777777778,
                                 max_features=None,
                                 min_df=0.0012707570261625706,
                                 ngram_range=(1, 2), norm='l2',
                                 preprocessor=None, smooth_idf=True,
                                 stop_words=None, strip_accents=None,...
                                 vocabulary=None)),
                ('clf',
                 DecisionTreeClassifier(ccp_alpha=0.0, class_weight=None,
                                        criterion='gini', max_depth=49,
                                        max_features=0.56, max_leaf_nodes=316,
                                        min_impurity_decrease=1e-07,
                                        min_impurity_split=None,
                                        min_samples_leaf=4,
                                        min_samples_split=0.015198715678453873,
                                        min_weight_fraction_leaf=0.0,
                                        presort='deprecated', random_state=0,
                                        splitter='random'))],
         verbose=False),
                   Pipeline(memory=None,
         steps=[('vect',
                 TfidfVectorizer(analyzer='word', binary=False,
                                 decode_error='strict',
                                 dtype=<class 'numpy.float64'>,
                                 encoding='utf-8', input='content',
                                 lowercase=True, max_df=0.6333333333333333,
                                 max_features=None,
                                 min_df=0.0010083260374073057,
                                 ngram_range=(1, 1), norm='l2',
                                 preprocessor=None, smooth_idf=True,
                                 stop_words='english', strip_accents=...
                                 tokenizer=None, use_idf=True,
                                 vocabulary=None)),
                ('clf',
                 AdaBoostClassifier(algorithm='SAMME',
                                    base_estimator=LinearSVC(C=1.0,
                                                             class_weight=None,
                                                             dual=True,
                                                             fit_intercept=True,
                                                             intercept_scaling=1,
                                                             loss='squared_hinge',
                                                             max_iter=1000,
                                                             multi_class='ovr',
                                                             penalty='l2',
                                                             random_state=None,
                                                             tol=0.0001,
                                                             verbose=0),
                                    learning_rate=0.40053644159285917,
                                    n_estimators=190, random_state=0))],
         verbose=False),
                   Pipeline(memory=None,
         steps=[('vect',
                 TfidfVectorizer(analyzer='word', binary=False,
                                 decode_error='strict',
                                 dtype=<class 'numpy.float64'>,
                                 encoding='utf-8', input='content',
                                 lowercase=True, max_df=0.6777777777777778,
                                 max_features=None,
                                 min_df=0.0012707570261625706,
                                 ngram_range=(1, 2), norm='l2',
                                 preprocessor=None, smooth_idf=True,
                                 stop_words=None, strip_accents=None,...
                                 vocabulary=None)),
                ('clf',
                 DecisionTreeClassifier(ccp_alpha=0.0, class_weight=None,
                                        criterion='gini', max_depth=49,
                                        max_features=0.56, max_leaf_nodes=316,
                                        min_impurity_decrease=1e-07,
                                        min_impurity_split=None,
                                        min_samples_leaf=4,
                                        min_samples_split=0.015198715678453873,
                                        min_weight_fraction_leaf=0.0,
                                        presort='deprecated', random_state=0,
                                        splitter='random'))],
         verbose=False),
                   Pipeline(memory=None,
         steps=[('vect',
                 TfidfVectorizer(analyzer='word', binary=False,
                                 decode_error='strict',
                                 dtype=<class 'numpy.float64'>,
                                 encoding='utf-8', input='content',
                                 lowercase=True, max_df=0.7222222222222222,
                                 max_features=None,
                                 min_df=0.00014705642569160768,
                                 ngram_range=(1, 1), norm='l2',
                                 preprocessor=None, smooth_idf=True,
                                 stop_words=None, strip_accents=None...
                                        max_features=0.01, max_leaf_nodes=None,
                                        max_samples=0.007808728196651904,
                                        min_impurity_decrease=3.1622776601683795e-10,
                                        min_impurity_split=None,
                                        min_samples_leaf=62,
                                        min_samples_split=0.0008966659727253451,
                                        min_weight_fraction_leaf=0.0007448459487372297,
                                        n_estimators=251, n_jobs=None,
                                        oob_score=False, random_state=0,
                                        verbose=True, warm_start=True))],
         verbose=False),
                   None,
                   Pipeline(memory=None,
         steps=[('vect',
                 TfidfVectorizer(analyzer='word', binary=False,
                                 decode_error='strict',
                                 dtype=<class 'numpy.float64'>,
                                 encoding='utf-8', input='content',
                                 lowercase=True, max_df=0.8555555555555556,
                                 max_features=None,
                                 min_df=0.00016483617040605902,
                                 ngram_range=(1, 2), norm='l2',
                                 preprocessor=None, smooth_idf=True,
                                 stop_words='english', strip_accents...
                                 tokenizer=None, use_idf=True,
                                 vocabulary=None)),
                ('clf',
                 LogisticRegression(C=0.47946121772409134, class_weight=None,
                                    dual=False, fit_intercept=True,
                                    intercept_scaling=1,
                                    l1_ratio=0.9168495407673116, max_iter=75,
                                    multi_class='multinomial', n_jobs=None,
                                    penalty='none', random_state=0,
                                    solver='sag', tol=6.514227791666363e-07,
                                    verbose=0, warm_start=False))],
         verbose=False),
                   None,
                   Pipeline(memory=None,
         steps=[('vect',
                 TfidfVectorizer(analyzer='word', binary=False,
                                 decode_error='strict',
                                 dtype=<class 'numpy.float64'>,
                                 encoding='utf-8', input='content',
                                 lowercase=True, max_df=0.6777777777777778,
                                 max_features=None,
                                 min_df=0.0012707570261625706,
                                 ngram_range=(1, 2), norm='l2',
                                 preprocessor=None, smooth_idf=True,
                                 stop_words=None, strip_accents=None,...
                                 vocabulary=None)),
                ('clf',
                 DecisionTreeClassifier(ccp_alpha=0.0, class_weight=None,
                                        criterion='gini', max_depth=49,
                                        max_features=0.56, max_leaf_nodes=316,
                                        min_impurity_decrease=1e-07,
                                        min_impurity_split=None,
                                        min_samples_leaf=4,
                                        min_samples_split=0.015198715678453873,
                                        min_weight_fraction_leaf=0.0,
                                        presort='deprecated', random_state=0,
                                        splitter='random'))],
         verbose=False),
                   Pipeline(memory=None,
         steps=[('vect',
                 TfidfVectorizer(analyzer='word', binary=False,
                                 decode_error='strict',
                                 dtype=<class 'numpy.float64'>,
                                 encoding='utf-8', input='content',
                                 lowercase=True, max_df=0.6333333333333333,
                                 max_features=None,
                                 min_df=0.0010083260374073057,
                                 ngram_range=(1, 1), norm='l2',
                                 preprocessor=None, smooth_idf=True,
                                 stop_words='english', strip_accents=...
                                 tokenizer=None, use_idf=True,
                                 vocabulary=None)),
                ('clf',
                 AdaBoostClassifier(algorithm='SAMME',
                                    base_estimator=LinearSVC(C=1.0,
                                                             class_weight=None,
                                                             dual=True,
                                                             fit_intercept=True,
                                                             intercept_scaling=1,
                                                             loss='squared_hinge',
                                                             max_iter=1000,
                                                             multi_class='ovr',
                                                             penalty='l2',
                                                             random_state=None,
                                                             tol=0.0001,
                                                             verbose=0),
                                    learning_rate=0.40053644159285917,
                                    n_estimators=190, random_state=0))],
         verbose=False),
                   MultinomialNB(alpha=1.0, class_prior=None, fit_prior=True),
                   Pipeline(memory=None,
         steps=[('vect',
                 TfidfVectorizer(analyzer='word', binary=False,
                                 decode_error='strict',
                                 dtype=<class 'numpy.float64'>,
                                 encoding='utf-8', input='content',
                                 lowercase=True, max_df=0.6777777777777778,
                                 max_features=None,
                                 min_df=0.0012707570261625706,
                                 ngram_range=(1, 2), norm='l2',
                                 preprocessor=None, smooth_idf=True,
                                 stop_words=None, strip_accents=None,...
                                 vocabulary=None)),
                ('clf',
                 DecisionTreeClassifier(ccp_alpha=0.0, class_weight=None,
                                        criterion='gini', max_depth=49,
                                        max_features=0.56, max_leaf_nodes=316,
                                        min_impurity_decrease=1e-07,
                                        min_impurity_split=None,
                                        min_samples_leaf=4,
                                        min_samples_split=0.015198715678453873,
                                        min_weight_fraction_leaf=0.0,
                                        presort='deprecated', random_state=0,
                                        splitter='random'))],
         verbose=False),
                   Pipeline(memory=None,
         steps=[('vect',
                 TfidfVectorizer(analyzer='word', binary=False,
                                 decode_error='strict',
                                 dtype=<class 'numpy.float64'>,
                                 encoding='utf-8', input='content',
                                 lowercase=True, max_df=0.6333333333333333,
                                 max_features=None,
                                 min_df=0.0010083260374073057,
                                 ngram_range=(1, 1), norm='l2',
                                 preprocessor=None, smooth_idf=True,
                                 stop_words='english', strip_accents=...
                                 tokenizer=None, use_idf=True,
                                 vocabulary=None)),
                ('clf',
                 AdaBoostClassifier(algorithm='SAMME',
                                    base_estimator=LinearSVC(C=1.0,
                                                             class_weight=None,
                                                             dual=True,
                                                             fit_intercept=True,
                                                             intercept_scaling=1,
                                                             loss='squared_hinge',
                                                             max_iter=1000,
                                                             multi_class='ovr',
                                                             penalty='l2',
                                                             random_state=None,
                                                             tol=0.0001,
                                                             verbose=0),
                                    learning_rate=0.40053644159285917,
                                    n_estimators=190, random_state=0))],
         verbose=False),
                   Pipeline(memory=None,
         steps=[('vect',
                 TfidfVectorizer(analyzer='word', binary=False,
                                 decode_error='strict',
                                 dtype=<class 'numpy.float64'>,
                                 encoding='utf-8', input='content',
                                 lowercase=True, max_df=0.7222222222222222,
                                 max_features=None,
                                 min_df=0.00014705642569160768,
                                 ngram_range=(1, 1), norm='l2',
                                 preprocessor=None, smooth_idf=True,
                                 stop_words=None, strip_accents=None...
                                        max_features=0.01, max_leaf_nodes=None,
                                        max_samples=0.007808728196651904,
                                        min_impurity_decrease=3.1622776601683795e-10,
                                        min_impurity_split=None,
                                        min_samples_leaf=62,
                                        min_samples_split=0.0008966659727253451,
                                        min_weight_fraction_leaf=0.0007448459487372297,
                                        n_estimators=251, n_jobs=None,
                                        oob_score=False, random_state=0,
                                        verbose=True, warm_start=True))],
         verbose=False),
                   Pipeline(memory=None,
         steps=[('vect',
                 TfidfVectorizer(analyzer='word', binary=False,
                                 decode_error='strict',
                                 dtype=<class 'numpy.float64'>,
                                 encoding='utf-8', input='content',
                                 lowercase=True, max_df=0.8555555555555556,
                                 max_features=None,
                                 min_df=0.00016483617040605902,
                                 ngram_range=(1, 2), norm='l2',
                                 preprocessor=None, smooth_idf=True,
                                 stop_words='english', strip_accents...
                                 tokenizer=None, use_idf=True,
                                 vocabulary=None)),
                ('clf',
                 LogisticRegression(C=0.47946121772409134, class_weight=None,
                                    dual=False, fit_intercept=True,
                                    intercept_scaling=1,
                                    l1_ratio=0.9168495407673116, max_iter=75,
                                    multi_class='multinomial', n_jobs=None,
                                    penalty='none', random_state=0,
                                    solver='sag', tol=6.514227791666363e-07,
                                    verbose=0, warm_start=False))],
         verbose=False),
                   Pipeline(memory=None,
         steps=[('vect',
                 TfidfVectorizer(analyzer='word', binary=False,
                                 decode_error='strict',
                                 dtype=<class 'numpy.float64'>,
                                 encoding='utf-8', input='content',
                                 lowercase=True, max_df=0.7666666666666666,
                                 max_features=None,
                                 min_df=0.00010572508827796376,
                                 ngram_range=(1, 2), norm='l2',
                                 preprocessor=None, smooth_idf=True,
                                 stop_words='english', strip_accents=None,
                                 sublinear_tf=False, token_pattern='\\w{1,}',
                                 tokenizer=None, use_idf=True,
                                 vocabulary=None)),
                ('clf',
                 LinearSVC(C=99.37702864845689, class_weight=None, dual=False,
                           fit_intercept=False, intercept_scaling=1,
                           loss='squared_hinge', max_iter=1000,
                           multi_class='ovr', penalty='l2', random_state=0,
                           tol=3.8359242531998225e-06, verbose=0))],
         verbose=False),
                   Pipeline(memory=None,
         steps=[('vect',
                 TfidfVectorizer(analyzer='word', binary=False,
                                 decode_error='strict',
                                 dtype=<class 'numpy.float64'>,
                                 encoding='utf-8', input='content',
                                 lowercase=True, max_df=0.6777777777777778,
                                 max_features=None,
                                 min_df=0.0012707570261625706,
                                 ngram_range=(1, 2), norm='l2',
                                 preprocessor=None, smooth_idf=True,
                                 stop_words=None, strip_accents=None,...
                                 vocabulary=None)),
                ('clf',
                 DecisionTreeClassifier(ccp_alpha=0.0, class_weight=None,
                                        criterion='gini', max_depth=49,
                                        max_features=0.56, max_leaf_nodes=316,
                                        min_impurity_decrease=1e-07,
                                        min_impurity_split=None,
                                        min_samples_leaf=4,
                                        min_samples_split=0.015198715678453873,
                                        min_weight_fraction_leaf=0.0,
                                        presort='deprecated', random_state=0,
                                        splitter='random'))],
         verbose=False),
                   None,
                   Pipeline(memory=None,
         steps=[('vect',
                 TfidfVectorizer(analyzer='word', binary=False,
                                 decode_error='strict',
                                 dtype=<class 'numpy.float64'>,
                                 encoding='utf-8', input='content',
                                 lowercase=True, max_df=0.7222222222222222,
                                 max_features=None,
                                 min_df=0.00014705642569160768,
                                 ngram_range=(1, 1), norm='l2',
                                 preprocessor=None, smooth_idf=True,
                                 stop_words=None, strip_accents=None...
                                        max_features=0.01, max_leaf_nodes=None,
                                        max_samples=0.007808728196651904,
                                        min_impurity_decrease=3.1622776601683795e-10,
                                        min_impurity_split=None,
                                        min_samples_leaf=62,
                                        min_samples_split=0.0008966659727253451,
                                        min_weight_fraction_leaf=0.0007448459487372297,
                                        n_estimators=251, n_jobs=None,
                                        oob_score=False, random_state=0,
                                        verbose=True, warm_start=True))],
         verbose=False),
                   Pipeline(memory=None,
         steps=[('vect',
                 TfidfVectorizer(analyzer='word', binary=False,
                                 decode_error='strict',
                                 dtype=<class 'numpy.float64'>,
                                 encoding='utf-8', input='content',
                                 lowercase=True, max_df=0.7666666666666666,
                                 max_features=None,
                                 min_df=0.00010572508827796376,
                                 ngram_range=(1, 2), norm='l2',
                                 preprocessor=None, smooth_idf=True,
                                 stop_words='english', strip_accents=None,
                                 sublinear_tf=False, token_pattern='\\w{1,}',
                                 tokenizer=None, use_idf=True,
                                 vocabulary=None)),
                ('clf',
                 LinearSVC(C=99.37702864845689, class_weight=None, dual=False,
                           fit_intercept=False, intercept_scaling=1,
                           loss='squared_hinge', max_iter=1000,
                           multi_class='ovr', penalty='l2', random_state=0,
                           tol=3.8359242531998225e-06, verbose=0))],
         verbose=False),
                   Pipeline(memory=None,
         steps=[('vect',
                 TfidfVectorizer(analyzer='word', binary=False,
                                 decode_error='strict',
                                 dtype=<class 'numpy.float64'>,
                                 encoding='utf-8', input='content',
                                 lowercase=True, max_df=0.7222222222222222,
                                 max_features=None,
                                 min_df=0.00014705642569160768,
                                 ngram_range=(1, 1), norm='l2',
                                 preprocessor=None, smooth_idf=True,
                                 stop_words=None, strip_accents=None...
                                        max_features=0.01, max_leaf_nodes=None,
                                        max_samples=0.007808728196651904,
                                        min_impurity_decrease=3.1622776601683795e-10,
                                        min_impurity_split=None,
                                        min_samples_leaf=62,
                                        min_samples_split=0.0008966659727253451,
                                        min_weight_fraction_leaf=0.0007448459487372297,
                                        n_estimators=251, n_jobs=None,
                                        oob_score=False, random_state=0,
                                        verbose=True, warm_start=True))],
         verbose=False),
                   Pipeline(memory=None,
         steps=[('vect',
                 TfidfVectorizer(analyzer='word', binary=False,
                                 decode_error='strict',
                                 dtype=<class 'numpy.float64'>,
                                 encoding='utf-8', input='content',
                                 lowercase=True, max_df=0.7666666666666666,
                                 max_features=None,
                                 min_df=0.00010572508827796376,
                                 ngram_range=(1, 2), norm='l2',
                                 preprocessor=None, smooth_idf=True,
                                 stop_words='english', strip_accents=None,
                                 sublinear_tf=False, token_pattern='\\w{1,}',
                                 tokenizer=None, use_idf=True,
                                 vocabulary=None)),
                ('clf',
                 LinearSVC(C=99.37702864845689, class_weight=None, dual=False,
                           fit_intercept=False, intercept_scaling=1,
                           loss='squared_hinge', max_iter=1000,
                           multi_class='ovr', penalty='l2', random_state=0,
                           tol=3.8359242531998225e-06, verbose=0))],
         verbose=False),
                   MultinomialNB(alpha=1.0, class_prior=None, fit_prior=True),
                   Pipeline(memory=None,
         steps=[('vect',
                 TfidfVectorizer(analyzer='word', binary=False,
                                 decode_error='strict',
                                 dtype=<class 'numpy.float64'>,
                                 encoding='utf-8', input='content',
                                 lowercase=True, max_df=0.8555555555555556,
                                 max_features=None,
                                 min_df=0.00016483617040605902,
                                 ngram_range=(1, 2), norm='l2',
                                 preprocessor=None, smooth_idf=True,
                                 stop_words='english', strip_accents...
                                 tokenizer=None, use_idf=True,
                                 vocabulary=None)),
                ('clf',
                 LogisticRegression(C=0.47946121772409134, class_weight=None,
                                    dual=False, fit_intercept=True,
                                    intercept_scaling=1,
                                    l1_ratio=0.9168495407673116, max_iter=75,
                                    multi_class='multinomial', n_jobs=None,
                                    penalty='none', random_state=0,
                                    solver='sag', tol=6.514227791666363e-07,
                                    verbose=0, warm_start=False))],
         verbose=False)],
             mask=[False, False, False, False, False, False, False, False,
                   False, False, False, False, False, False, False, False,
                   False, False, False, False, False, False, False, False,
                   False, False, False, False, False, False, False, False,
                   False, False, False, False, False, False, False, False,
                   False, False, False, False, False, False, False, False,
                   False, False, False, False, False, False, False, False,
                   False, False, False, False, False, False, False, False,
                   False, False, False, False, False, False, False, False,
                   False, False, False, False, False, False, False, False,
                   False, False, False, False, False, False, False, False,
                   False, False, False, False, False, False, False, False,
                   False, False, False, False, False, False, False, False,
                   False, False, False, False, False, False, False, False,
                   False, False, False, False, False, False, False, False,
                   False, False, False, False, False, False, False, False,
                   False, False, False, False, False, False, False, False,
                   False, False, False, False, False, False, False, False,
                   False, False, False, False, False, False, False, False,
                   False, False, False, False, False, False, False, False,
                   False, False, False, False, False, False, False, False,
                   False, False, False, False, False, False, False, False,
                   False, False, False, False, False, False, False, False,
                   False, False, False, False, False, False, False, False,
                   False, False, False, False, False, False, False, False],
       fill_value='?',
            dtype=object), 'param_clf__bootstrap': masked_array(data=[False, True, True, True, False, False, False, False,
                   False, False, True, False, True, False, True, False,
                   True, False, True, True, True, True, True, True, True,
                   True, False, True, True, True, True, False, False,
                   True, True, True, True, True, False, False, True, True,
                   True, True, False, False, True, True, False, True,
                   True, True, False, True, True, False, False, True,
                   True, True, False, True, False, False, False, False,
                   True, False, True, True, False, False, False, True,
                   False, False, True, False, True, False, False, False,
                   True, True, False, False, True, True, True, False,
                   False, False, False, False, True, True, True, True,
                   False, True, True, True, False, False, True, False,
                   False, False, False, False, True, False, False, False,
                   False, True, True, False, False, True, True, False,
                   True, False, False, False, True, False, False, True,
                   False, True, False, True, True, True, True, True, True,
                   False, False, False, False, False, False, True, False,
                   False, False, True, True, False, False, True, True,
                   True, False, False, True, False, False, False, True,
                   False, True, True, False, True, True, False, False,
                   True, False, False, False, True, True, True, True,
                   False, False, True, False, True, False, False, True,
                   False, True, False, False, False, True, False, False,
                   True, False, False, True, True],
             mask=[False, False, False, False, False, False, False, False,
                   False, False, False, False, False, False, False, False,
                   False, False, False, False, False, False, False, False,
                   False, False, False, False, False, False, False, False,
                   False, False, False, False, False, False, False, False,
                   False, False, False, False, False, False, False, False,
                   False, False, False, False, False, False, False, False,
                   False, False, False, False, False, False, False, False,
                   False, False, False, False, False, False, False, False,
                   False, False, False, False, False, False, False, False,
                   False, False, False, False, False, False, False, False,
                   False, False, False, False, False, False, False, False,
                   False, False, False, False, False, False, False, False,
                   False, False, False, False, False, False, False, False,
                   False, False, False, False, False, False, False, False,
                   False, False, False, False, False, False, False, False,
                   False, False, False, False, False, False, False, False,
                   False, False, False, False, False, False, False, False,
                   False, False, False, False, False, False, False, False,
                   False, False, False, False, False, False, False, False,
                   False, False, False, False, False, False, False, False,
                   False, False, False, False, False, False, False, False,
                   False, False, False, False, False, False, False, False,
                   False, False, False, False, False, False, False, False,
                   False, False, False, False, False, False, False, False],
       fill_value='?',
            dtype=object), 'param_clf__bootstrap_features': masked_array(data=[False, True, True, False, True, True, False, True,
                   False, True, False, False, True, True, True, True,
                   True, True, True, True, True, True, False, True, False,
                   True, True, True, False, False, True, False, False,
                   True, False, False, True, False, False, False, False,
                   True, True, False, False, True, False, False, False,
                   False, True, True, True, True, True, False, True, True,
                   True, False, True, False, False, False, True, False,
                   True, False, False, False, True, True, True, True,
                   True, True, False, True, False, False, False, False,
                   True, False, True, False, False, True, True, False,
                   True, False, False, True, True, False, False, False,
                   True, False, True, False, False, True, False, False,
                   False, False, False, False, False, False, True, False,
                   False, True, True, False, False, True, False, True,
                   True, False, True, True, False, True, False, True,
                   False, True, False, True, True, False, False, True,
                   True, True, False, True, True, True, True, True, False,
                   True, False, True, False, True, True, True, False,
                   False, False, True, False, True, True, False, False,
                   False, True, True, True, True, False, False, True,
                   False, True, True, False, True, True, False, False,
                   False, False, True, False, True, False, True, False,
                   True, True, False, False, True, True, False, False,
                   False, True, True, False, True],
             mask=[False, False, False, False, False, False, False, False,
                   False, False, False, False, False, False, False, False,
                   False, False, False, False, False, False, False, False,
                   False, False, False, False, False, False, False, False,
                   False, False, False, False, False, False, False, False,
                   False, False, False, False, False, False, False, False,
                   False, False, False, False, False, False, False, False,
                   False, False, False, False, False, False, False, False,
                   False, False, False, False, False, False, False, False,
                   False, False, False, False, False, False, False, False,
                   False, False, False, False, False, False, False, False,
                   False, False, False, False, False, False, False, False,
                   False, False, False, False, False, False, False, False,
                   False, False, False, False, False, False, False, False,
                   False, False, False, False, False, False, False, False,
                   False, False, False, False, False, False, False, False,
                   False, False, False, False, False, False, False, False,
                   False, False, False, False, False, False, False, False,
                   False, False, False, False, False, False, False, False,
                   False, False, False, False, False, False, False, False,
                   False, False, False, False, False, False, False, False,
                   False, False, False, False, False, False, False, False,
                   False, False, False, False, False, False, False, False,
                   False, False, False, False, False, False, False, False,
                   False, False, False, False, False, False, False, False],
       fill_value='?',
            dtype=object), 'param_clf__max_features': masked_array(data=[0.8041078097617904, 0.7287697098281616,
                   0.4322058771399403, 0.383761926579255,
                   0.9149310235360102, 0.5748157710807602,
                   0.7625926565087582, 0.04542848786576137,
                   0.5329616703511596, 0.23974958924240186,
                   0.9706428457363693, 0.9912170847466315,
                   0.9702722032280183, 0.7327551487813436,
                   0.68824741621278, 0.4436750274867354,
                   0.727184705159369, 0.2601507017519247,
                   0.30847453480970344, 0.5728451395817441,
                   0.4008274816832872, 0.49538255661070274,
                   0.3528695517314673, 0.9389390531664465,
                   0.7479062299476722, 0.638495410494338,
                   0.6432583119624558, 0.7893894517715161,
                   0.5719363377342171, 0.25478836014821593,
                   0.47727954840637454, 0.2976403548989238,
                   0.7595081502813072, 0.6435766743368304,
                   0.008372228833342876, 0.8444328241010104,
                   0.4171102830987832, 0.37137232006455667,
                   0.9120442729722144, 0.007139068602398213,
                   0.09277274417469461, 0.4250279746264455,
                   0.5293424173908581, 0.10771967723621545,
                   0.45507498143373115, 0.03511115590254876,
                   0.17421674979066692, 0.47871284706891626,
                   0.26869196415566154, 0.8377209430590781,
                   0.778318222225744, 0.9072196124544909,
                   0.544834583549896, 0.7317945393233404,
                   0.6188687452697725, 0.447107103317256,
                   0.9723809986161742, 0.049286860290485235,
                   0.18810275688821476, 0.08096147040951684,
                   0.8701302701171609, 0.560947528673034,
                   0.940100635503031, 0.8355917434025741,
                   0.7116185081049804, 0.5076258896806638,
                   0.25496068933087734, 0.9293175369230119,
                   0.47414643880097385, 0.8818974495685915,
                   0.3424883506569144, 0.5194764278959346,
                   0.11499687169032813, 0.9782128223612612,
                   0.5654622682897744, 0.054473449691950004,
                   0.5218093408741838, 0.2234393375689152,
                   0.49753536407653187, 0.40839279404424256,
                   0.7780680470435732, 0.6064518812101772,
                   0.3429916807127845, 0.31232291817990376,
                   0.10666852510585423, 0.053395335722344894,
                   0.7740715959711809, 0.6957299722211582,
                   0.12315876133004233, 0.6071894635555929,
                   0.851390828998649, 0.5455645453535705,
                   0.5724143997436254, 0.9419918734182068,
                   0.5569099301284081, 0.5271951341226369,
                   0.7085529287479299, 0.34708335091212694,
                   0.924683570865367, 0.9538123156103031,
                   0.9786387618449437, 0.39954510752324923,
                   0.9654803645813562, 0.2957195072027342,
                   0.17555351180611867, 0.704675151726222,
                   0.7384145269866882, 0.13755702703560413,
                   0.9092099774779366, 0.8042730110735646,
                   0.9067509207291913, 0.012365978156541946,
                   0.7906509778233803, 0.7918667656201579,
                   0.7999896543473607, 0.25162272665336205,
                   0.4298021923059766, 0.27931808606511144,
                   0.5048419906666137, 0.9020546184530749,
                   0.8163514228866441, 0.864386195652989,
                   0.12807400406726588, 0.21087698441783165,
                   0.9546438994928883, 0.47618990370366787,
                   0.5817778153348276, 0.08686111066153979,
                   0.6599270267321241, 0.054843499661434536,
                   0.7994901838837157, 0.5336550066058341,
                   0.36784273545426227, 0.7148884596336798,
                   0.9867872743739655, 0.7102830777566337,
                   0.6766866162111446, 0.3832492030551987,
                   0.3208357572921381, 0.25629116388267914,
                   0.5121798562061132, 0.2730893536359167,
                   0.4455065984560159, 0.7366223518910722,
                   0.151452389878599, 0.027759669235459472,
                   0.6908098720302277, 0.8005217670598533,
                   0.587391438566732, 0.9635043441286752,
                   0.10559982955473901, 0.7661370927239434,
                   0.44835000500279676, 0.45847923467041496,
                   0.975599621470175, 0.978099744278984,
                   0.5927072780383205, 0.9997654954939011,
                   0.43779661375638634, 0.9216901742057149,
                   0.18049796499991688, 0.03246607871210283,
                   0.5178669638726272, 0.04224561463328125,
                   0.4989348037760941, 0.14108123148074647,
                   0.11885017508589724, 0.472945975317727,
                   0.026169964054654993, 0.24416450273006962,
                   0.8492396289409444, 0.8508259620451911,
                   0.48551459314237844, 0.39794236451718035,
                   0.15526405782347297, 0.24054441575724628,
                   0.030038975497580833, 0.9149855489411466,
                   0.46207474901789525, 0.12805494124630634,
                   0.7416617230448526, 0.028609864096968196,
                   0.21239798536546894, 0.9013347673197406,
                   0.3779847902337966, 0.39009505777512277,
                   0.18322640896232567, 0.9312787930942861,
                   0.08811614901174147, 0.5530024589549252,
                   0.9665716350714949, 0.7992400271516518,
                   0.9802402877274728, 0.17454393949296276,
                   0.4172577149382912, 0.12204752707809907,
                   0.5057664385563188, 0.5914234847917038,
                   0.30068257675087484, 0.4033915522597019],
             mask=[False, False, False, False, False, False, False, False,
                   False, False, False, False, False, False, False, False,
                   False, False, False, False, False, False, False, False,
                   False, False, False, False, False, False, False, False,
                   False, False, False, False, False, False, False, False,
                   False, False, False, False, False, False, False, False,
                   False, False, False, False, False, False, False, False,
                   False, False, False, False, False, False, False, False,
                   False, False, False, False, False, False, False, False,
                   False, False, False, False, False, False, False, False,
                   False, False, False, False, False, False, False, False,
                   False, False, False, False, False, False, False, False,
                   False, False, False, False, False, False, False, False,
                   False, False, False, False, False, False, False, False,
                   False, False, False, False, False, False, False, False,
                   False, False, False, False, False, False, False, False,
                   False, False, False, False, False, False, False, False,
                   False, False, False, False, False, False, False, False,
                   False, False, False, False, False, False, False, False,
                   False, False, False, False, False, False, False, False,
                   False, False, False, False, False, False, False, False,
                   False, False, False, False, False, False, False, False,
                   False, False, False, False, False, False, False, False,
                   False, False, False, False, False, False, False, False,
                   False, False, False, False, False, False, False, False],
       fill_value='?',
            dtype=object), 'param_clf__max_samples': masked_array(data=[0.2735320195908342, 0.7879395319875209,
                   0.8230564387605322, 0.2007225235805875,
                   0.09590290202661356, 0.3351609191089997,
                   0.6666458033201907, 0.3217842718304378,
                   0.41840955991735407, 0.5302926477800505,
                   0.2026926185439456, 0.4714676509724546,
                   0.16872048823170893, 0.7928231403207014,
                   0.17644668628513682, 0.413312445615292,
                   0.22250765850153997, 0.007729921432522158,
                   0.47069058370522077, 0.5039549386743591,
                   0.09869685745184587, 0.04808078048923714,
                   0.376727221218914, 0.9554426141881001,
                   0.29725303018461946, 0.7548770895467611,
                   0.5644735982311048, 0.8231380350068013,
                   0.5265974126524174, 0.8922210588536693,
                   0.5825156734183312, 0.09151833628746264,
                   0.903855776861897, 0.385892732539557,
                   0.415391809109534, 0.5985287428914258,
                   0.18390903840461814, 0.023256728536597104,
                   0.7892022585496922, 0.13171362073366055,
                   0.6258048798011573, 0.751836587228482,
                   0.9110648211081864, 0.2572248357701825,
                   0.8548262172292455, 0.3552915209323252,
                   0.8113971982389097, 0.8133637224031106,
                   0.6456113691567699, 0.07558349525571029,
                   0.40002243880555655, 0.41884548470668737,
                   0.5409697872786783, 0.8536505542381208,
                   0.3040236097275262, 0.779694010923419,
                   0.2891853260660555, 0.9289830976379987,
                   0.5160259286386163, 0.5475849271889746,
                   0.9608173209766312, 0.9009107711343867,
                   0.5325217897887943, 0.7994091389532858,
                   0.36896434791804444, 0.8166343061196542,
                   0.9577830686880459, 0.6850037078777709,
                   0.9238676540591991, 0.634404928538508,
                   0.4639065009028883, 0.9564303427545765,
                   0.7180155333841137, 0.9577613868555912,
                   0.1230210707248105, 0.7501543774411317,
                   0.4031891535290265, 0.23411411928859738,
                   0.35598884371772344, 0.0787144659548229,
                   0.37774934648073233, 0.43757160838575504,
                   0.08253199552734625, 0.43190422151338614,
                   0.27495234241576205, 0.6690228827883734,
                   0.3002715380483616, 0.32029010770056976,
                   0.7420284747619605, 0.632082096196872,
                   0.3054621284374154, 0.9266099422801278,
                   0.32967129045097576, 0.4048020821937648,
                   0.572056115291836, 0.021974094763683283,
                   0.4322614741936128, 0.4306742289733726,
                   0.18329344051423435, 0.24830564326047055,
                   0.07823710419563101, 0.6663561509390348,
                   0.2697257518287005, 0.7275811653186909,
                   0.10951785116950674, 0.2418837340599238,
                   0.16924641650506766, 0.2345568286488937,
                   0.5099495424132175, 0.0316723526150523,
                   0.07565498164459505, 0.09566562095242104,
                   0.16631826740608846, 0.9811373833490997,
                   0.542419495758003, 0.7455963612487158,
                   0.9716220566919046, 0.16184021655008674,
                   0.775440371777647, 0.5220241186319174,
                   0.4981032487442846, 0.1530630987579954,
                   0.3387168797872051, 0.02044580954985431,
                   0.6696029915573883, 0.8686838168058331,
                   0.23230537090633374, 0.03300628965408958,
                   0.5792412774287145, 0.35605675920268476,
                   0.9243549643085232, 0.08530926639771286,
                   0.4769452750834302, 0.41352797682613096,
                   0.7760959526282329, 0.6298283816488258,
                   0.504557234462983, 0.17044675051988067,
                   0.2271161672886154, 0.5811305728842577,
                   0.6038420250659001, 0.5398046144481868,
                   0.22089896507423956, 0.3051935497101078,
                   0.40181354789084767, 0.96603655740548,
                   0.8785934455288432, 0.8184205630546396,
                   0.49608767756915506, 0.4115411324141588,
                   0.07520421819798329, 0.6681498229166721,
                   0.6646105446978837, 0.500718691417731,
                   0.4810484923142846, 0.27356893723065945,
                   0.5701921655306, 0.13206277631704466,
                   0.010096817993019314, 0.4487298713740224,
                   0.11256828259155949, 0.11955216962131432,
                   0.06882657482202359, 0.2741235511865233,
                   0.33259806240232637, 0.749260650702883,
                   0.8538423290162976, 0.31760660238217986,
                   0.5616100320361351, 0.9314701714465692,
                   0.9908035841022258, 0.9423628271767283,
                   0.2342201903308665, 0.4422286088110027,
                   0.8708032266022915, 0.7641143165482993,
                   0.8816462999279175, 0.20561886827850373,
                   0.005872825254672476, 0.7387372220155091,
                   0.5724320473455495, 0.5436549244718809,
                   0.9907500064232448, 0.5795762074280727,
                   0.26358137053301645, 0.9477764248603917,
                   0.4847466187924927, 0.44603033432367944,
                   0.42438906284110456, 0.532668748242248,
                   0.47672671190122273, 0.5164785347710172,
                   0.0724394139679907, 0.9488968049234774,
                   0.6819205156481029, 0.39546784875406715,
                   0.5145267519376905, 0.6032121819431405,
                   0.75956566277332, 0.10902863351855907],
             mask=[False, False, False, False, False, False, False, False,
                   False, False, False, False, False, False, False, False,
                   False, False, False, False, False, False, False, False,
                   False, False, False, False, False, False, False, False,
                   False, False, False, False, False, False, False, False,
                   False, False, False, False, False, False, False, False,
                   False, False, False, False, False, False, False, False,
                   False, False, False, False, False, False, False, False,
                   False, False, False, False, False, False, False, False,
                   False, False, False, False, False, False, False, False,
                   False, False, False, False, False, False, False, False,
                   False, False, False, False, False, False, False, False,
                   False, False, False, False, False, False, False, False,
                   False, False, False, False, False, False, False, False,
                   False, False, False, False, False, False, False, False,
                   False, False, False, False, False, False, False, False,
                   False, False, False, False, False, False, False, False,
                   False, False, False, False, False, False, False, False,
                   False, False, False, False, False, False, False, False,
                   False, False, False, False, False, False, False, False,
                   False, False, False, False, False, False, False, False,
                   False, False, False, False, False, False, False, False,
                   False, False, False, False, False, False, False, False,
                   False, False, False, False, False, False, False, False,
                   False, False, False, False, False, False, False, False],
       fill_value='?',
            dtype=object), 'param_clf__n_estimators': masked_array(data=[258, 287, 21, 357, 14, 223, 255, 90, 192, 336, 280,
                   321, 319, 216, 297, 378, 148, 56, 193, 324, 328, 305,
                   303, 290, 351, 46, 297, 58, 193, 133, 32, 165, 54, 154,
                   66, 82, 99, 57, 209, 389, 185, 29, 164, 299, 300, 129,
                   263, 25, 219, 154, 114, 398, 112, 326, 350, 240, 264,
                   15, 38, 43, 372, 306, 344, 42, 203, 121, 232, 132, 342,
                   327, 307, 325, 52, 369, 164, 177, 314, 22, 396, 114,
                   51, 183, 336, 356, 58, 164, 55, 135, 399, 388, 31, 368,
                   223, 348, 296, 15, 253, 55, 152, 273, 212, 225, 379,
                   137, 63, 283, 118, 177, 333, 193, 162, 56, 361, 82,
                   116, 225, 28, 389, 44, 304, 212, 296, 289, 74, 245,
                   171, 259, 126, 33, 60, 103, 129, 292, 133, 224, 62, 22,
                   279, 169, 322, 331, 109, 108, 120, 156, 95, 181, 346,
                   327, 259, 31, 205, 347, 131, 202, 118, 357, 359, 233,
                   80, 44, 86, 31, 134, 203, 213, 307, 391, 397, 84, 355,
                   48, 329, 141, 152, 323, 117, 31, 96, 399, 75, 196, 230,
                   386, 338, 350, 266, 383, 388, 214, 178, 127, 32, 38,
                   107, 277, 322, 246, 75, 364],
             mask=[False, False, False, False, False, False, False, False,
                   False, False, False, False, False, False, False, False,
                   False, False, False, False, False, False, False, False,
                   False, False, False, False, False, False, False, False,
                   False, False, False, False, False, False, False, False,
                   False, False, False, False, False, False, False, False,
                   False, False, False, False, False, False, False, False,
                   False, False, False, False, False, False, False, False,
                   False, False, False, False, False, False, False, False,
                   False, False, False, False, False, False, False, False,
                   False, False, False, False, False, False, False, False,
                   False, False, False, False, False, False, False, False,
                   False, False, False, False, False, False, False, False,
                   False, False, False, False, False, False, False, False,
                   False, False, False, False, False, False, False, False,
                   False, False, False, False, False, False, False, False,
                   False, False, False, False, False, False, False, False,
                   False, False, False, False, False, False, False, False,
                   False, False, False, False, False, False, False, False,
                   False, False, False, False, False, False, False, False,
                   False, False, False, False, False, False, False, False,
                   False, False, False, False, False, False, False, False,
                   False, False, False, False, False, False, False, False,
                   False, False, False, False, False, False, False, False,
                   False, False, False, False, False, False, False, False],
       fill_value='?',
            dtype=object), 'param_clf__oob_score': masked_array(data=[False, True, True, False, True, False, False, False,
                   False, False, True, True, True, True, False, True,
                   False, True, True, True, False, True, True, False,
                   False, True, True, True, True, True, False, False,
                   False, True, False, True, True, True, False, True,
                   False, True, False, True, False, True, True, False,
                   True, True, False, False, True, False, True, False,
                   False, True, True, True, True, False, False, False,
                   True, True, False, True, True, True, False, True,
                   False, True, True, True, False, False, True, True,
                   False, False, True, False, True, False, False, True,
                   True, False, True, True, False, False, False, True,
                   True, True, False, False, True, False, False, True,
                   True, False, False, True, False, False, True, True,
                   False, True, False, False, True, False, True, True,
                   True, True, False, False, False, True, False, False,
                   True, True, True, True, True, False, True, True, False,
                   True, True, False, True, True, True, True, True, False,
                   False, False, False, True, False, False, False, True,
                   False, False, True, True, True, True, False, True,
                   False, True, True, True, False, False, True, False,
                   False, False, False, True, True, True, False, True,
                   True, True, False, True, True, True, True, True, True,
                   False, False, False, False, True, True, False, False,
                   True, False, False, False, False],
             mask=[False, False, False, False, False, False, False, False,
                   False, False, False, False, False, False, False, False,
                   False, False, False, False, False, False, False, False,
                   False, False, False, False, False, False, False, False,
                   False, False, False, False, False, False, False, False,
                   False, False, False, False, False, False, False, False,
                   False, False, False, False, False, False, False, False,
                   False, False, False, False, False, False, False, False,
                   False, False, False, False, False, False, False, False,
                   False, False, False, False, False, False, False, False,
                   False, False, False, False, False, False, False, False,
                   False, False, False, False, False, False, False, False,
                   False, False, False, False, False, False, False, False,
                   False, False, False, False, False, False, False, False,
                   False, False, False, False, False, False, False, False,
                   False, False, False, False, False, False, False, False,
                   False, False, False, False, False, False, False, False,
                   False, False, False, False, False, False, False, False,
                   False, False, False, False, False, False, False, False,
                   False, False, False, False, False, False, False, False,
                   False, False, False, False, False, False, False, False,
                   False, False, False, False, False, False, False, False,
                   False, False, False, False, False, False, False, False,
                   False, False, False, False, False, False, False, False,
                   False, False, False, False, False, False, False, False],
       fill_value='?',
            dtype=object), 'param_clf__warm_start': masked_array(data=[False, False, False, True, True, False, True, True,
                   True, False, False, False, False, False, False, False,
                   True, True, True, True, False, False, True, False,
                   True, False, False, True, False, True, False, True,
                   True, True, False, False, False, True, False, True,
                   True, False, False, False, False, True, True, True,
                   True, True, False, False, True, False, False, True,
                   True, True, True, False, False, False, False, False,
                   False, False, True, False, True, False, True, True,
                   False, False, True, True, False, False, False, False,
                   True, True, False, False, False, True, True, True,
                   True, True, False, False, False, True, False, True,
                   False, False, False, False, False, False, False, False,
                   True, True, False, True, True, True, False, False,
                   True, False, True, True, False, True, True, True, True,
                   False, True, True, True, True, True, False, False,
                   True, True, True, True, True, True, True, False, True,
                   True, True, True, True, False, True, False, False,
                   True, False, True, False, False, False, True, True,
                   True, False, True, True, True, False, False, False,
                   True, False, True, False, False, False, True, False,
                   False, True, False, True, True, True, False, True,
                   True, False, True, True, True, True, False, True, True,
                   False, True, True, False, False, True, False, True,
                   True, True, False, False, True],
             mask=[False, False, False, False, False, False, False, False,
                   False, False, False, False, False, False, False, False,
                   False, False, False, False, False, False, False, False,
                   False, False, False, False, False, False, False, False,
                   False, False, False, False, False, False, False, False,
                   False, False, False, False, False, False, False, False,
                   False, False, False, False, False, False, False, False,
                   False, False, False, False, False, False, False, False,
                   False, False, False, False, False, False, False, False,
                   False, False, False, False, False, False, False, False,
                   False, False, False, False, False, False, False, False,
                   False, False, False, False, False, False, False, False,
                   False, False, False, False, False, False, False, False,
                   False, False, False, False, False, False, False, False,
                   False, False, False, False, False, False, False, False,
                   False, False, False, False, False, False, False, False,
                   False, False, False, False, False, False, False, False,
                   False, False, False, False, False, False, False, False,
                   False, False, False, False, False, False, False, False,
                   False, False, False, False, False, False, False, False,
                   False, False, False, False, False, False, False, False,
                   False, False, False, False, False, False, False, False,
                   False, False, False, False, False, False, False, False,
                   False, False, False, False, False, False, False, False,
                   False, False, False, False, False, False, False, False],
       fill_value='?',
            dtype=object), 'param_vect__max_df': masked_array(data=[0.5444444444444444, 0.5, 0.6777777777777778,
                   0.6333333333333333, 0.9, 0.7222222222222222, 0.5,
                   0.5888888888888889, 0.9, 0.8555555555555556, 0.5,
                   0.6333333333333333, 0.6333333333333333, 0.9,
                   0.7222222222222222, 0.6777777777777778, 0.5,
                   0.8555555555555556, 0.5888888888888889,
                   0.5444444444444444, 0.7222222222222222, 0.5,
                   0.8555555555555556, 0.5444444444444444,
                   0.5444444444444444, 0.7222222222222222,
                   0.5444444444444444, 0.6777777777777778,
                   0.6777777777777778, 0.8555555555555556,
                   0.6333333333333333, 0.8111111111111111,
                   0.5888888888888889, 0.5888888888888889,
                   0.7222222222222222, 0.8111111111111111, 0.9,
                   0.7222222222222222, 0.6333333333333333, 0.5,
                   0.5888888888888889, 0.7222222222222222, 0.9,
                   0.8111111111111111, 0.8111111111111111,
                   0.6333333333333333, 0.6333333333333333, 0.5, 0.9,
                   0.6777777777777778, 0.9, 0.7222222222222222,
                   0.7666666666666666, 0.7222222222222222, 0.9,
                   0.5888888888888889, 0.6777777777777778,
                   0.5888888888888889, 0.5, 0.5, 0.7222222222222222,
                   0.5888888888888889, 0.8555555555555556, 0.9,
                   0.6333333333333333, 0.7222222222222222,
                   0.5444444444444444, 0.5, 0.7222222222222222,
                   0.6777777777777778, 0.9, 0.5, 0.8555555555555556,
                   0.7666666666666666, 0.6777777777777778, 0.9,
                   0.5444444444444444, 0.5888888888888889,
                   0.8555555555555556, 0.5444444444444444,
                   0.6777777777777778, 0.5, 0.8111111111111111,
                   0.7666666666666666, 0.9, 0.8111111111111111,
                   0.5888888888888889, 0.5, 0.6333333333333333,
                   0.7222222222222222, 0.8555555555555556,
                   0.7222222222222222, 0.8555555555555556,
                   0.5888888888888889, 0.9, 0.5, 0.7222222222222222,
                   0.6333333333333333, 0.9, 0.6333333333333333,
                   0.7222222222222222, 0.7666666666666666,
                   0.8111111111111111, 0.5888888888888889,
                   0.6777777777777778, 0.7222222222222222,
                   0.6333333333333333, 0.6333333333333333,
                   0.7666666666666666, 0.6333333333333333,
                   0.6777777777777778, 0.7222222222222222,
                   0.7666666666666666, 0.9, 0.6333333333333333,
                   0.8111111111111111, 0.7222222222222222,
                   0.6333333333333333, 0.5444444444444444,
                   0.7222222222222222, 0.5, 0.7666666666666666,
                   0.5444444444444444, 0.8555555555555556,
                   0.6777777777777778, 0.6333333333333333,
                   0.5444444444444444, 0.6777777777777778,
                   0.6777777777777778, 0.7222222222222222,
                   0.7222222222222222, 0.8555555555555556, 0.9,
                   0.6777777777777778, 0.5888888888888889,
                   0.6777777777777778, 0.7222222222222222,
                   0.5888888888888889, 0.8111111111111111,
                   0.7666666666666666, 0.5888888888888889,
                   0.5444444444444444, 0.9, 0.5, 0.7222222222222222,
                   0.7666666666666666, 0.6777777777777778,
                   0.8111111111111111, 0.5, 0.9, 0.9, 0.8555555555555556,
                   0.9, 0.7666666666666666, 0.5888888888888889,
                   0.5888888888888889, 0.5888888888888889, 0.5,
                   0.5444444444444444, 0.6777777777777778,
                   0.7666666666666666, 0.8111111111111111,
                   0.5888888888888889, 0.5444444444444444,
                   0.5444444444444444, 0.5, 0.5444444444444444,
                   0.8555555555555556, 0.5444444444444444,
                   0.5444444444444444, 0.7222222222222222,
                   0.6333333333333333, 0.9, 0.5444444444444444,
                   0.7666666666666666, 0.5888888888888889,
                   0.7222222222222222, 0.6777777777777778,
                   0.8555555555555556, 0.7222222222222222,
                   0.5444444444444444, 0.5444444444444444,
                   0.7222222222222222, 0.6777777777777778, 0.5,
                   0.6333333333333333, 0.7222222222222222,
                   0.5444444444444444, 0.5888888888888889,
                   0.8555555555555556, 0.5444444444444444,
                   0.8111111111111111, 0.6333333333333333,
                   0.6333333333333333, 0.5, 0.5888888888888889, 0.5, 0.5,
                   0.9, 0.6777777777777778],
             mask=[False, False, False, False, False, False, False, False,
                   False, False, False, False, False, False, False, False,
                   False, False, False, False, False, False, False, False,
                   False, False, False, False, False, False, False, False,
                   False, False, False, False, False, False, False, False,
                   False, False, False, False, False, False, False, False,
                   False, False, False, False, False, False, False, False,
                   False, False, False, False, False, False, False, False,
                   False, False, False, False, False, False, False, False,
                   False, False, False, False, False, False, False, False,
                   False, False, False, False, False, False, False, False,
                   False, False, False, False, False, False, False, False,
                   False, False, False, False, False, False, False, False,
                   False, False, False, False, False, False, False, False,
                   False, False, False, False, False, False, False, False,
                   False, False, False, False, False, False, False, False,
                   False, False, False, False, False, False, False, False,
                   False, False, False, False, False, False, False, False,
                   False, False, False, False, False, False, False, False,
                   False, False, False, False, False, False, False, False,
                   False, False, False, False, False, False, False, False,
                   False, False, False, False, False, False, False, False,
                   False, False, False, False, False, False, False, False,
                   False, False, False, False, False, False, False, False,
                   False, False, False, False, False, False, False, False],
       fill_value='?',
            dtype=object), 'param_vect__min_df': masked_array(data=[0.0021190046894787417, 0.0002643818901889405,
                   0.00631974845099825, 0.0018079827514345475,
                   0.00653053422447245, 0.00014555037031477257,
                   0.000320313961152863, 0.0007741593697836997,
                   0.0008853809993531954, 0.0005220522097213829,
                   0.0006799552746946891, 0.00018880401416137508,
                   0.000913209234847792, 0.0004452242349239306,
                   0.00033917672197626665, 0.005281483987263175,
                   0.00013721198472947332, 0.0009961732931659346,
                   0.003937448231841961, 0.00014633039720326354,
                   0.00012231017198945855, 0.0005612551158176723,
                   0.0002889041142593464, 0.00045934343801172197,
                   0.00022473608112151246, 0.00022363202156393489,
                   0.0026849386756358827, 0.005899296406890342,
                   0.00022641386006295574, 0.0001678236169140789,
                   0.0003294164160658745, 0.006787725076990622,
                   0.0028374042784047823, 0.00010110534491283214,
                   0.00011433204615976263, 0.006039793362876566,
                   0.006245718070590018, 0.0011176748476231888,
                   0.00011618768297197917, 0.00014619490533752404,
                   0.008728335560411056, 0.0010956337685385177,
                   0.00043848197430785386, 0.0005036350174824408,
                   0.000992503800405849, 0.0002743292348975704,
                   0.004144680082476916, 0.0038753426137013193,
                   0.007863381012280839, 0.005846593202055442,
                   0.00012082491075995292, 0.00011345558386229489,
                   0.00037494029853853107, 0.0026528583442507123,
                   0.0029827045998427337, 0.005738348214456635,
                   0.0010709985959376131, 0.006147816198218495,
                   0.0006848767115033545, 0.00026567793682637046,
                   0.0016465959583930765, 0.0001291782627833387,
                   0.00989684672758612, 0.00028988551370307105,
                   0.00011126768996562946, 0.00042943981412846646,
                   0.007683120333114825, 0.0076293976855326025,
                   0.0002753203215764797, 0.0031498664850481417,
                   0.001114695738465579, 0.008083209625476819,
                   0.0016247695336201564, 0.0043253901386184485,
                   0.0002207695978457939, 0.0001826073778712324,
                   0.0006683588665809419, 0.0001801829439669374,
                   0.005607715332818271, 0.0010298144292754932,
                   0.002278032055081042, 0.00042708290951816094,
                   0.0016037351754642133, 0.0014637209476171746,
                   0.0013054375511533823, 0.0003408411815757151,
                   0.00038759450399179264, 0.0003973804491633026,
                   0.00018024024198573567, 0.0004678219292177023,
                   0.0008076504222293942, 0.00627687027381037,
                   0.0009098737101808315, 0.00010058370526151803,
                   0.00275719602829668, 0.00036729757885476164,
                   0.0002565244324090726, 0.002218649004451763,
                   0.007541691599883028, 0.00016147777227596658,
                   0.0005357260075745206, 0.0017424910007500895,
                   0.00021567281452032302, 0.004324650568284658,
                   0.00014154668451425088, 0.004809084251460617,
                   0.0001135674029086469, 0.0015532522182295793,
                   0.00031451792822378554, 0.00268465581437476,
                   0.0033308596826294792, 0.008383686129095453,
                   0.009913783614138921, 0.002180891700964028,
                   0.001731777435889714, 0.00048804517689075166,
                   0.0002874151321146624, 0.004732211910256779,
                   0.0008879383365991017, 0.00024193889788516666,
                   0.003323986531247918, 0.003383252045944012,
                   0.001004915748974247, 0.0034121217143816373,
                   0.008133989202752023, 0.0001824953888680803,
                   0.009429671739097084, 0.0004638053111666062,
                   0.0018334549837277117, 0.00856091324002353,
                   0.0006614152214088263, 0.0008982512872686716,
                   0.00027265839554225287, 0.00040628383281475673,
                   0.0004155394657802238, 0.0004465185289583982,
                   0.00022796749450944482, 0.0002179771330414835,
                   0.00223152980203543, 0.0029140391892371613,
                   0.0034539434995552493, 0.0002935459794421058,
                   0.002190870002485152, 0.0003291500480486555,
                   0.0001624870226360064, 0.000501393193226579,
                   0.005727546994389818, 0.004897717319979126,
                   0.00028564644614490625, 0.0025895745931076627,
                   0.00039207508208072734, 0.0013472353657205279,
                   0.0002250241561866282, 0.004214472271523116,
                   0.00038803308003453486, 0.0008287171043350852,
                   0.0005530373739804331, 0.0001513802744815044,
                   0.005598766364727352, 0.004419929762285939,
                   0.0012245787062838288, 0.002713421888996022,
                   0.006880988055417675, 0.00044294358808228686,
                   0.00020962090241963318, 0.000593739604302443,
                   0.00020380309208005223, 0.009502988782348575,
                   0.007205596954635388, 0.00047889587110035495,
                   0.002637023046189934, 0.0008661652043599906,
                   0.005924074547707516, 0.009981339169120484,
                   0.001147372405154575, 0.0004446598761497269,
                   0.0034810124102999155, 0.00961301992181817,
                   0.0014749151153054764, 0.0002304567810434819,
                   0.008500887639270652, 0.0001880763629327088,
                   0.0010647922993256248, 0.0017052197067315843,
                   0.0032975676312138654, 0.00032451846073530645,
                   0.00011978539103282762, 0.0013280530323949023,
                   0.0005981036708715185, 0.0028354733636035655,
                   0.001261008822907652, 0.0027142272290598896,
                   0.00745914718205243, 0.0012180400671630584,
                   0.004558549171871819, 0.003925143857322602,
                   0.0006241953388285983, 0.004200077108533546,
                   0.00015088836173661365, 0.00010056292290556832],
             mask=[False, False, False, False, False, False, False, False,
                   False, False, False, False, False, False, False, False,
                   False, False, False, False, False, False, False, False,
                   False, False, False, False, False, False, False, False,
                   False, False, False, False, False, False, False, False,
                   False, False, False, False, False, False, False, False,
                   False, False, False, False, False, False, False, False,
                   False, False, False, False, False, False, False, False,
                   False, False, False, False, False, False, False, False,
                   False, False, False, False, False, False, False, False,
                   False, False, False, False, False, False, False, False,
                   False, False, False, False, False, False, False, False,
                   False, False, False, False, False, False, False, False,
                   False, False, False, False, False, False, False, False,
                   False, False, False, False, False, False, False, False,
                   False, False, False, False, False, False, False, False,
                   False, False, False, False, False, False, False, False,
                   False, False, False, False, False, False, False, False,
                   False, False, False, False, False, False, False, False,
                   False, False, False, False, False, False, False, False,
                   False, False, False, False, False, False, False, False,
                   False, False, False, False, False, False, False, False,
                   False, False, False, False, False, False, False, False,
                   False, False, False, False, False, False, False, False,
                   False, False, False, False, False, False, False, False],
       fill_value='?',
            dtype=object), 'param_vect__ngram_range': masked_array(data=[(1, 1), (1, 1), (1, 1), (1, 1), (1, 2), (1, 1), (1, 2),
                   (1, 2), (1, 2), (1, 2), (1, 2), (1, 2), (1, 2), (1, 1),
                   (1, 1), (1, 2), (1, 2), (1, 2), (1, 2), (1, 1), (1, 2),
                   (1, 1), (1, 2), (1, 2), (1, 1), (1, 2), (1, 2), (1, 2),
                   (1, 2), (1, 2), (1, 1), (1, 1), (1, 1), (1, 1), (1, 1),
                   (1, 2), (1, 1), (1, 1), (1, 2), (1, 2), (1, 1), (1, 2),
                   (1, 1), (1, 2), (1, 2), (1, 1), (1, 2), (1, 1), (1, 1),
                   (1, 2), (1, 2), (1, 2), (1, 2), (1, 1), (1, 1), (1, 2),
                   (1, 1), (1, 1), (1, 1), (1, 1), (1, 1), (1, 2), (1, 2),
                   (1, 1), (1, 1), (1, 2), (1, 2), (1, 1), (1, 2), (1, 1),
                   (1, 1), (1, 1), (1, 2), (1, 2), (1, 1), (1, 2), (1, 2),
                   (1, 1), (1, 2), (1, 1), (1, 1), (1, 2), (1, 2), (1, 2),
                   (1, 1), (1, 2), (1, 2), (1, 1), (1, 2), (1, 2), (1, 2),
                   (1, 2), (1, 2), (1, 2), (1, 2), (1, 1), (1, 2), (1, 1),
                   (1, 2), (1, 1), (1, 2), (1, 2), (1, 2), (1, 1), (1, 1),
                   (1, 2), (1, 2), (1, 1), (1, 1), (1, 1), (1, 1), (1, 1),
                   (1, 1), (1, 2), (1, 1), (1, 1), (1, 2), (1, 1), (1, 1),
                   (1, 2), (1, 1), (1, 2), (1, 1), (1, 2), (1, 2), (1, 2),
                   (1, 2), (1, 2), (1, 1), (1, 1), (1, 1), (1, 2), (1, 1),
                   (1, 1), (1, 1), (1, 2), (1, 2), (1, 1), (1, 1), (1, 1),
                   (1, 1), (1, 1), (1, 2), (1, 2), (1, 2), (1, 2), (1, 1),
                   (1, 1), (1, 2), (1, 2), (1, 2), (1, 1), (1, 1), (1, 2),
                   (1, 2), (1, 1), (1, 2), (1, 1), (1, 1), (1, 2), (1, 1),
                   (1, 2), (1, 2), (1, 2), (1, 1), (1, 1), (1, 2), (1, 1),
                   (1, 1), (1, 2), (1, 1), (1, 2), (1, 1), (1, 1), (1, 2),
                   (1, 2), (1, 2), (1, 2), (1, 1), (1, 1), (1, 1), (1, 1),
                   (1, 2), (1, 2), (1, 2), (1, 2), (1, 1), (1, 2), (1, 2),
                   (1, 2), (1, 1), (1, 1), (1, 2), (1, 1), (1, 1), (1, 1),
                   (1, 1), (1, 2), (1, 2), (1, 2)],
             mask=[False, False, False, False, False, False, False, False,
                   False, False, False, False, False, False, False, False,
                   False, False, False, False, False, False, False, False,
                   False, False, False, False, False, False, False, False,
                   False, False, False, False, False, False, False, False,
                   False, False, False, False, False, False, False, False,
                   False, False, False, False, False, False, False, False,
                   False, False, False, False, False, False, False, False,
                   False, False, False, False, False, False, False, False,
                   False, False, False, False, False, False, False, False,
                   False, False, False, False, False, False, False, False,
                   False, False, False, False, False, False, False, False,
                   False, False, False, False, False, False, False, False,
                   False, False, False, False, False, False, False, False,
                   False, False, False, False, False, False, False, False,
                   False, False, False, False, False, False, False, False,
                   False, False, False, False, False, False, False, False,
                   False, False, False, False, False, False, False, False,
                   False, False, False, False, False, False, False, False,
                   False, False, False, False, False, False, False, False,
                   False, False, False, False, False, False, False, False,
                   False, False, False, False, False, False, False, False,
                   False, False, False, False, False, False, False, False,
                   False, False, False, False, False, False, False, False,
                   False, False, False, False, False, False, False, False],
       fill_value='?',
            dtype=object), 'param_vect__stop_words': masked_array(data=[None, 'english', None, None, 'english', 'english',
                   None, None, None, 'english', None, 'english',
                   'english', None, None, None, None, 'english',
                   'english', 'english', None, 'english', None, None,
                   'english', None, 'english', 'english', None, None,
                   'english', None, None, 'english', 'english', 'english',
                   'english', None, 'english', 'english', 'english', None,
                   'english', 'english', None, 'english', 'english',
                   'english', 'english', 'english', None, 'english', None,
                   None, 'english', 'english', None, 'english', 'english',
                   'english', None, None, None, 'english', 'english',
                   None, 'english', 'english', 'english', 'english', None,
                   None, 'english', None, 'english', 'english', 'english',
                   'english', 'english', None, 'english', 'english', None,
                   'english', 'english', 'english', 'english', None, None,
                   'english', 'english', None, 'english', 'english', None,
                   'english', 'english', 'english', None, None, 'english',
                   None, 'english', None, None, None, 'english',
                   'english', 'english', 'english', None, None, None,
                   None, 'english', 'english', 'english', 'english', None,
                   'english', 'english', 'english', None, None, None,
                   None, 'english', None, 'english', 'english', None,
                   None, None, 'english', 'english', None, 'english',
                   None, 'english', None, 'english', None, 'english',
                   'english', 'english', None, None, None, 'english',
                   None, 'english', None, 'english', 'english', 'english',
                   None, None, None, 'english', None, 'english', None,
                   'english', None, None, 'english', None, 'english',
                   'english', 'english', 'english', 'english', None, None,
                   None, 'english', None, None, None, 'english', None,
                   None, None, 'english', None, 'english', None, None,
                   None, None, 'english', None, 'english', None,
                   'english', None, 'english', 'english', 'english', None],
             mask=[False, False, False, False, False, False, False, False,
                   False, False, False, False, False, False, False, False,
                   False, False, False, False, False, False, False, False,
                   False, False, False, False, False, False, False, False,
                   False, False, False, False, False, False, False, False,
                   False, False, False, False, False, False, False, False,
                   False, False, False, False, False, False, False, False,
                   False, False, False, False, False, False, False, False,
                   False, False, False, False, False, False, False, False,
                   False, False, False, False, False, False, False, False,
                   False, False, False, False, False, False, False, False,
                   False, False, False, False, False, False, False, False,
                   False, False, False, False, False, False, False, False,
                   False, False, False, False, False, False, False, False,
                   False, False, False, False, False, False, False, False,
                   False, False, False, False, False, False, False, False,
                   False, False, False, False, False, False, False, False,
                   False, False, False, False, False, False, False, False,
                   False, False, False, False, False, False, False, False,
                   False, False, False, False, False, False, False, False,
                   False, False, False, False, False, False, False, False,
                   False, False, False, False, False, False, False, False,
                   False, False, False, False, False, False, False, False,
                   False, False, False, False, False, False, False, False,
                   False, False, False, False, False, False, False, False],
       fill_value='?',
            dtype=object), 'param_vect__token_pattern': masked_array(data=['\\w{1,}', '\\w{1,}', '\\w{1,}', '\\w{2,}', '\\w{2,}',
                   '\\w{2,}', '\\w{1,}', '\\w{1,}', '\\w{2,}', '\\w{1,}',
                   '\\w{2,}', '\\w{1,}', '\\w{1,}', '\\w{1,}', '\\w{2,}',
                   '\\w{2,}', '\\w{2,}', '\\w{2,}', '\\w{2,}', '\\w{1,}',
                   '\\w{1,}', '\\w{1,}', '\\w{2,}', '\\w{2,}', '\\w{2,}',
                   '\\w{1,}', '\\w{1,}', '\\w{2,}', '\\w{1,}', '\\w{2,}',
                   '\\w{2,}', '\\w{1,}', '\\w{1,}', '\\w{2,}', '\\w{2,}',
                   '\\w{1,}', '\\w{1,}', '\\w{2,}', '\\w{2,}', '\\w{1,}',
                   '\\w{1,}', '\\w{2,}', '\\w{1,}', '\\w{2,}', '\\w{1,}',
                   '\\w{1,}', '\\w{1,}', '\\w{2,}', '\\w{1,}', '\\w{2,}',
                   '\\w{1,}', '\\w{2,}', '\\w{1,}', '\\w{1,}', '\\w{2,}',
                   '\\w{2,}', '\\w{2,}', '\\w{2,}', '\\w{2,}', '\\w{2,}',
                   '\\w{2,}', '\\w{1,}', '\\w{1,}', '\\w{1,}', '\\w{2,}',
                   '\\w{2,}', '\\w{1,}', '\\w{1,}', '\\w{1,}', '\\w{1,}',
                   '\\w{1,}', '\\w{2,}', '\\w{1,}', '\\w{2,}', '\\w{1,}',
                   '\\w{1,}', '\\w{2,}', '\\w{1,}', '\\w{2,}', '\\w{2,}',
                   '\\w{1,}', '\\w{1,}', '\\w{1,}', '\\w{1,}', '\\w{2,}',
                   '\\w{2,}', '\\w{2,}', '\\w{2,}', '\\w{1,}', '\\w{2,}',
                   '\\w{1,}', '\\w{2,}', '\\w{2,}', '\\w{1,}', '\\w{2,}',
                   '\\w{2,}', '\\w{2,}', '\\w{1,}', '\\w{2,}', '\\w{2,}',
                   '\\w{2,}', '\\w{2,}', '\\w{2,}', '\\w{1,}', '\\w{2,}',
                   '\\w{1,}', '\\w{1,}', '\\w{1,}', '\\w{2,}', '\\w{2,}',
                   '\\w{1,}', '\\w{2,}', '\\w{1,}', '\\w{2,}', '\\w{2,}',
                   '\\w{2,}', '\\w{2,}', '\\w{1,}', '\\w{1,}', '\\w{1,}',
                   '\\w{2,}', '\\w{2,}', '\\w{2,}', '\\w{1,}', '\\w{2,}',
                   '\\w{1,}', '\\w{1,}', '\\w{1,}', '\\w{2,}', '\\w{1,}',
                   '\\w{2,}', '\\w{1,}', '\\w{2,}', '\\w{1,}', '\\w{2,}',
                   '\\w{1,}', '\\w{1,}', '\\w{2,}', '\\w{1,}', '\\w{2,}',
                   '\\w{2,}', '\\w{1,}', '\\w{2,}', '\\w{1,}', '\\w{1,}',
                   '\\w{1,}', '\\w{1,}', '\\w{1,}', '\\w{1,}', '\\w{2,}',
                   '\\w{1,}', '\\w{1,}', '\\w{2,}', '\\w{2,}', '\\w{2,}',
                   '\\w{1,}', '\\w{2,}', '\\w{1,}', '\\w{2,}', '\\w{2,}',
                   '\\w{2,}', '\\w{2,}', '\\w{2,}', '\\w{2,}', '\\w{2,}',
                   '\\w{1,}', '\\w{2,}', '\\w{1,}', '\\w{1,}', '\\w{1,}',
                   '\\w{2,}', '\\w{1,}', '\\w{1,}', '\\w{1,}', '\\w{2,}',
                   '\\w{1,}', '\\w{1,}', '\\w{2,}', '\\w{2,}', '\\w{1,}',
                   '\\w{2,}', '\\w{1,}', '\\w{2,}', '\\w{1,}', '\\w{1,}',
                   '\\w{1,}', '\\w{1,}', '\\w{2,}', '\\w{1,}', '\\w{2,}',
                   '\\w{1,}', '\\w{2,}', '\\w{2,}', '\\w{2,}', '\\w{2,}',
                   '\\w{1,}', '\\w{1,}', '\\w{1,}', '\\w{2,}', '\\w{2,}'],
             mask=[False, False, False, False, False, False, False, False,
                   False, False, False, False, False, False, False, False,
                   False, False, False, False, False, False, False, False,
                   False, False, False, False, False, False, False, False,
                   False, False, False, False, False, False, False, False,
                   False, False, False, False, False, False, False, False,
                   False, False, False, False, False, False, False, False,
                   False, False, False, False, False, False, False, False,
                   False, False, False, False, False, False, False, False,
                   False, False, False, False, False, False, False, False,
                   False, False, False, False, False, False, False, False,
                   False, False, False, False, False, False, False, False,
                   False, False, False, False, False, False, False, False,
                   False, False, False, False, False, False, False, False,
                   False, False, False, False, False, False, False, False,
                   False, False, False, False, False, False, False, False,
                   False, False, False, False, False, False, False, False,
                   False, False, False, False, False, False, False, False,
                   False, False, False, False, False, False, False, False,
                   False, False, False, False, False, False, False, False,
                   False, False, False, False, False, False, False, False,
                   False, False, False, False, False, False, False, False,
                   False, False, False, False, False, False, False, False,
                   False, False, False, False, False, False, False, False,
                   False, False, False, False, False, False, False, False],
       fill_value='?',
            dtype=object), 'params': [{'clf__base_estimator': Pipeline(memory=None,
         steps=[('vect',
                 TfidfVectorizer(analyzer='word', binary=False,
                                 decode_error='strict',
                                 dtype=<class 'numpy.float64'>,
                                 encoding='utf-8', input='content',
                                 lowercase=True, max_df=0.6777777777777778,
                                 max_features=None,
                                 min_df=0.0012707570261625706,
                                 ngram_range=(1, 2), norm='l2',
                                 preprocessor=None, smooth_idf=True,
                                 stop_words=None, strip_accents=None,...
                                 vocabulary=None)),
                ('clf',
                 DecisionTreeClassifier(ccp_alpha=0.0, class_weight=None,
                                        criterion='gini', max_depth=49,
                                        max_features=0.56, max_leaf_nodes=316,
                                        min_impurity_decrease=1e-07,
                                        min_impurity_split=None,
                                        min_samples_leaf=4,
                                        min_samples_split=0.015198715678453873,
                                        min_weight_fraction_leaf=0.0,
                                        presort='deprecated', random_state=0,
                                        splitter='random'))],
         verbose=False), 'clf__bootstrap': False, 'clf__bootstrap_features': False, 'clf__max_features': 0.8041078097617904, 'clf__max_samples': 0.2735320195908342, 'clf__n_estimators': 258, 'clf__oob_score': False, 'clf__warm_start': False, 'vect__max_df': 0.5444444444444444, 'vect__min_df': 0.0021190046894787417, 'vect__ngram_range': (1, 1), 'vect__stop_words': None, 'vect__token_pattern': '\\w{1,}'}, {'clf__base_estimator': Pipeline(memory=None,
         steps=[('vect',
                 TfidfVectorizer(analyzer='word', binary=False,
                                 decode_error='strict',
                                 dtype=<class 'numpy.float64'>,
                                 encoding='utf-8', input='content',
                                 lowercase=True, max_df=0.6333333333333333,
                                 max_features=None,
                                 min_df=0.0010083260374073057,
                                 ngram_range=(1, 1), norm='l2',
                                 preprocessor=None, smooth_idf=True,
                                 stop_words='english', strip_accents=...
                                 tokenizer=None, use_idf=True,
                                 vocabulary=None)),
                ('clf',
                 AdaBoostClassifier(algorithm='SAMME',
                                    base_estimator=LinearSVC(C=1.0,
                                                             class_weight=None,
                                                             dual=True,
                                                             fit_intercept=True,
                                                             intercept_scaling=1,
                                                             loss='squared_hinge',
                                                             max_iter=1000,
                                                             multi_class='ovr',
                                                             penalty='l2',
                                                             random_state=None,
                                                             tol=0.0001,
                                                             verbose=0),
                                    learning_rate=0.40053644159285917,
                                    n_estimators=190, random_state=0))],
         verbose=False), 'clf__bootstrap': True, 'clf__bootstrap_features': True, 'clf__max_features': 0.7287697098281616, 'clf__max_samples': 0.7879395319875209, 'clf__n_estimators': 287, 'clf__oob_score': True, 'clf__warm_start': False, 'vect__max_df': 0.5, 'vect__min_df': 0.0002643818901889405, 'vect__ngram_range': (1, 1), 'vect__stop_words': 'english', 'vect__token_pattern': '\\w{1,}'}, {'clf__base_estimator': None, 'clf__bootstrap': True, 'clf__bootstrap_features': True, 'clf__max_features': 0.4322058771399403, 'clf__max_samples': 0.8230564387605322, 'clf__n_estimators': 21, 'clf__oob_score': True, 'clf__warm_start': False, 'vect__max_df': 0.6777777777777778, 'vect__min_df': 0.00631974845099825, 'vect__ngram_range': (1, 1), 'vect__stop_words': None, 'vect__token_pattern': '\\w{1,}'}, {'clf__base_estimator': Pipeline(memory=None,
         steps=[('vect',
                 TfidfVectorizer(analyzer='word', binary=False,
                                 decode_error='strict',
                                 dtype=<class 'numpy.float64'>,
                                 encoding='utf-8', input='content',
                                 lowercase=True, max_df=0.7666666666666666,
                                 max_features=None,
                                 min_df=0.00010572508827796376,
                                 ngram_range=(1, 2), norm='l2',
                                 preprocessor=None, smooth_idf=True,
                                 stop_words='english', strip_accents=None,
                                 sublinear_tf=False, token_pattern='\\w{1,}',
                                 tokenizer=None, use_idf=True,
                                 vocabulary=None)),
                ('clf',
                 LinearSVC(C=99.37702864845689, class_weight=None, dual=False,
                           fit_intercept=False, intercept_scaling=1,
                           loss='squared_hinge', max_iter=1000,
                           multi_class='ovr', penalty='l2', random_state=0,
                           tol=3.8359242531998225e-06, verbose=0))],
         verbose=False), 'clf__bootstrap': True, 'clf__bootstrap_features': False, 'clf__max_features': 0.383761926579255, 'clf__max_samples': 0.2007225235805875, 'clf__n_estimators': 357, 'clf__oob_score': False, 'clf__warm_start': True, 'vect__max_df': 0.6333333333333333, 'vect__min_df': 0.0018079827514345475, 'vect__ngram_range': (1, 1), 'vect__stop_words': None, 'vect__token_pattern': '\\w{2,}'}, {'clf__base_estimator': MultinomialNB(alpha=1.0, class_prior=None, fit_prior=True), 'clf__bootstrap': False, 'clf__bootstrap_features': True, 'clf__max_features': 0.9149310235360102, 'clf__max_samples': 0.09590290202661356, 'clf__n_estimators': 14, 'clf__oob_score': True, 'clf__warm_start': True, 'vect__max_df': 0.9, 'vect__min_df': 0.00653053422447245, 'vect__ngram_range': (1, 2), 'vect__stop_words': 'english', 'vect__token_pattern': '\\w{2,}'}, {'clf__base_estimator': None, 'clf__bootstrap': False, 'clf__bootstrap_features': True, 'clf__max_features': 0.5748157710807602, 'clf__max_samples': 0.3351609191089997, 'clf__n_estimators': 223, 'clf__oob_score': False, 'clf__warm_start': False, 'vect__max_df': 0.7222222222222222, 'vect__min_df': 0.00014555037031477257, 'vect__ngram_range': (1, 1), 'vect__stop_words': 'english', 'vect__token_pattern': '\\w{2,}'}, {'clf__base_estimator': MultinomialNB(alpha=1.0, class_prior=None, fit_prior=True), 'clf__bootstrap': False, 'clf__bootstrap_features': False, 'clf__max_features': 0.7625926565087582, 'clf__max_samples': 0.6666458033201907, 'clf__n_estimators': 255, 'clf__oob_score': False, 'clf__warm_start': True, 'vect__max_df': 0.5, 'vect__min_df': 0.000320313961152863, 'vect__ngram_range': (1, 2), 'vect__stop_words': None, 'vect__token_pattern': '\\w{1,}'}, {'clf__base_estimator': Pipeline(memory=None,
         steps=[('vect',
                 TfidfVectorizer(analyzer='word', binary=False,
                                 decode_error='strict',
                                 dtype=<class 'numpy.float64'>,
                                 encoding='utf-8', input='content',
                                 lowercase=True, max_df=0.6777777777777778,
                                 max_features=None,
                                 min_df=0.0012707570261625706,
                                 ngram_range=(1, 2), norm='l2',
                                 preprocessor=None, smooth_idf=True,
                                 stop_words=None, strip_accents=None,...
                                 vocabulary=None)),
                ('clf',
                 DecisionTreeClassifier(ccp_alpha=0.0, class_weight=None,
                                        criterion='gini', max_depth=49,
                                        max_features=0.56, max_leaf_nodes=316,
                                        min_impurity_decrease=1e-07,
                                        min_impurity_split=None,
                                        min_samples_leaf=4,
                                        min_samples_split=0.015198715678453873,
                                        min_weight_fraction_leaf=0.0,
                                        presort='deprecated', random_state=0,
                                        splitter='random'))],
         verbose=False), 'clf__bootstrap': False, 'clf__bootstrap_features': True, 'clf__max_features': 0.04542848786576137, 'clf__max_samples': 0.3217842718304378, 'clf__n_estimators': 90, 'clf__oob_score': False, 'clf__warm_start': True, 'vect__max_df': 0.5888888888888889, 'vect__min_df': 0.0007741593697836997, 'vect__ngram_range': (1, 2), 'vect__stop_words': None, 'vect__token_pattern': '\\w{1,}'}, {'clf__base_estimator': Pipeline(memory=None,
         steps=[('vect',
                 TfidfVectorizer(analyzer='word', binary=False,
                                 decode_error='strict',
                                 dtype=<class 'numpy.float64'>,
                                 encoding='utf-8', input='content',
                                 lowercase=True, max_df=0.7222222222222222,
                                 max_features=None,
                                 min_df=0.00014705642569160768,
                                 ngram_range=(1, 1), norm='l2',
                                 preprocessor=None, smooth_idf=True,
                                 stop_words=None, strip_accents=None...
                                        max_features=0.01, max_leaf_nodes=None,
                                        max_samples=0.007808728196651904,
                                        min_impurity_decrease=3.1622776601683795e-10,
                                        min_impurity_split=None,
                                        min_samples_leaf=62,
                                        min_samples_split=0.0008966659727253451,
                                        min_weight_fraction_leaf=0.0007448459487372297,
                                        n_estimators=251, n_jobs=None,
                                        oob_score=False, random_state=0,
                                        verbose=True, warm_start=True))],
         verbose=False), 'clf__bootstrap': False, 'clf__bootstrap_features': False, 'clf__max_features': 0.5329616703511596, 'clf__max_samples': 0.41840955991735407, 'clf__n_estimators': 192, 'clf__oob_score': False, 'clf__warm_start': True, 'vect__max_df': 0.9, 'vect__min_df': 0.0008853809993531954, 'vect__ngram_range': (1, 2), 'vect__stop_words': None, 'vect__token_pattern': '\\w{2,}'}, {'clf__base_estimator': Pipeline(memory=None,
         steps=[('vect',
                 TfidfVectorizer(analyzer='word', binary=False,
                                 decode_error='strict',
                                 dtype=<class 'numpy.float64'>,
                                 encoding='utf-8', input='content',
                                 lowercase=True, max_df=0.7222222222222222,
                                 max_features=None,
                                 min_df=0.00014705642569160768,
                                 ngram_range=(1, 1), norm='l2',
                                 preprocessor=None, smooth_idf=True,
                                 stop_words=None, strip_accents=None...
                                        max_features=0.01, max_leaf_nodes=None,
                                        max_samples=0.007808728196651904,
                                        min_impurity_decrease=3.1622776601683795e-10,
                                        min_impurity_split=None,
                                        min_samples_leaf=62,
                                        min_samples_split=0.0008966659727253451,
                                        min_weight_fraction_leaf=0.0007448459487372297,
                                        n_estimators=251, n_jobs=None,
                                        oob_score=False, random_state=0,
                                        verbose=True, warm_start=True))],
         verbose=False), 'clf__bootstrap': False, 'clf__bootstrap_features': True, 'clf__max_features': 0.23974958924240186, 'clf__max_samples': 0.5302926477800505, 'clf__n_estimators': 336, 'clf__oob_score': False, 'clf__warm_start': False, 'vect__max_df': 0.8555555555555556, 'vect__min_df': 0.0005220522097213829, 'vect__ngram_range': (1, 2), 'vect__stop_words': 'english', 'vect__token_pattern': '\\w{1,}'}, {'clf__base_estimator': Pipeline(memory=None,
         steps=[('vect',
                 TfidfVectorizer(analyzer='word', binary=False,
                                 decode_error='strict',
                                 dtype=<class 'numpy.float64'>,
                                 encoding='utf-8', input='content',
                                 lowercase=True, max_df=0.8555555555555556,
                                 max_features=None,
                                 min_df=0.00016483617040605902,
                                 ngram_range=(1, 2), norm='l2',
                                 preprocessor=None, smooth_idf=True,
                                 stop_words='english', strip_accents...
                                 tokenizer=None, use_idf=True,
                                 vocabulary=None)),
                ('clf',
                 LogisticRegression(C=0.47946121772409134, class_weight=None,
                                    dual=False, fit_intercept=True,
                                    intercept_scaling=1,
                                    l1_ratio=0.9168495407673116, max_iter=75,
                                    multi_class='multinomial', n_jobs=None,
                                    penalty='none', random_state=0,
                                    solver='sag', tol=6.514227791666363e-07,
                                    verbose=0, warm_start=False))],
         verbose=False), 'clf__bootstrap': True, 'clf__bootstrap_features': False, 'clf__max_features': 0.9706428457363693, 'clf__max_samples': 0.2026926185439456, 'clf__n_estimators': 280, 'clf__oob_score': True, 'clf__warm_start': False, 'vect__max_df': 0.5, 'vect__min_df': 0.0006799552746946891, 'vect__ngram_range': (1, 2), 'vect__stop_words': None, 'vect__token_pattern': '\\w{2,}'}, {'clf__base_estimator': MultinomialNB(alpha=1.0, class_prior=None, fit_prior=True), 'clf__bootstrap': False, 'clf__bootstrap_features': False, 'clf__max_features': 0.9912170847466315, 'clf__max_samples': 0.4714676509724546, 'clf__n_estimators': 321, 'clf__oob_score': True, 'clf__warm_start': False, 'vect__max_df': 0.6333333333333333, 'vect__min_df': 0.00018880401416137508, 'vect__ngram_range': (1, 2), 'vect__stop_words': 'english', 'vect__token_pattern': '\\w{1,}'}, {'clf__base_estimator': None, 'clf__bootstrap': True, 'clf__bootstrap_features': True, 'clf__max_features': 0.9702722032280183, 'clf__max_samples': 0.16872048823170893, 'clf__n_estimators': 319, 'clf__oob_score': True, 'clf__warm_start': False, 'vect__max_df': 0.6333333333333333, 'vect__min_df': 0.000913209234847792, 'vect__ngram_range': (1, 2), 'vect__stop_words': 'english', 'vect__token_pattern': '\\w{1,}'}, {'clf__base_estimator': MultinomialNB(alpha=1.0, class_prior=None, fit_prior=True), 'clf__bootstrap': False, 'clf__bootstrap_features': True, 'clf__max_features': 0.7327551487813436, 'clf__max_samples': 0.7928231403207014, 'clf__n_estimators': 216, 'clf__oob_score': True, 'clf__warm_start': False, 'vect__max_df': 0.9, 'vect__min_df': 0.0004452242349239306, 'vect__ngram_range': (1, 1), 'vect__stop_words': None, 'vect__token_pattern': '\\w{1,}'}, {'clf__base_estimator': Pipeline(memory=None,
         steps=[('vect',
                 TfidfVectorizer(analyzer='word', binary=False,
                                 decode_error='strict',
                                 dtype=<class 'numpy.float64'>,
                                 encoding='utf-8', input='content',
                                 lowercase=True, max_df=0.6777777777777778,
                                 max_features=None,
                                 min_df=0.0012707570261625706,
                                 ngram_range=(1, 2), norm='l2',
                                 preprocessor=None, smooth_idf=True,
                                 stop_words=None, strip_accents=None,...
                                 vocabulary=None)),
                ('clf',
                 DecisionTreeClassifier(ccp_alpha=0.0, class_weight=None,
                                        criterion='gini', max_depth=49,
                                        max_features=0.56, max_leaf_nodes=316,
                                        min_impurity_decrease=1e-07,
                                        min_impurity_split=None,
                                        min_samples_leaf=4,
                                        min_samples_split=0.015198715678453873,
                                        min_weight_fraction_leaf=0.0,
                                        presort='deprecated', random_state=0,
                                        splitter='random'))],
         verbose=False), 'clf__bootstrap': True, 'clf__bootstrap_features': True, 'clf__max_features': 0.68824741621278, 'clf__max_samples': 0.17644668628513682, 'clf__n_estimators': 297, 'clf__oob_score': False, 'clf__warm_start': False, 'vect__max_df': 0.7222222222222222, 'vect__min_df': 0.00033917672197626665, 'vect__ngram_range': (1, 1), 'vect__stop_words': None, 'vect__token_pattern': '\\w{2,}'}, {'clf__base_estimator': Pipeline(memory=None,
         steps=[('vect',
                 TfidfVectorizer(analyzer='word', binary=False,
                                 decode_error='strict',
                                 dtype=<class 'numpy.float64'>,
                                 encoding='utf-8', input='content',
                                 lowercase=True, max_df=0.8555555555555556,
                                 max_features=None,
                                 min_df=0.00016483617040605902,
                                 ngram_range=(1, 2), norm='l2',
                                 preprocessor=None, smooth_idf=True,
                                 stop_words='english', strip_accents...
                                 tokenizer=None, use_idf=True,
                                 vocabulary=None)),
                ('clf',
                 LogisticRegression(C=0.47946121772409134, class_weight=None,
                                    dual=False, fit_intercept=True,
                                    intercept_scaling=1,
                                    l1_ratio=0.9168495407673116, max_iter=75,
                                    multi_class='multinomial', n_jobs=None,
                                    penalty='none', random_state=0,
                                    solver='sag', tol=6.514227791666363e-07,
                                    verbose=0, warm_start=False))],
         verbose=False), 'clf__bootstrap': False, 'clf__bootstrap_features': True, 'clf__max_features': 0.4436750274867354, 'clf__max_samples': 0.413312445615292, 'clf__n_estimators': 378, 'clf__oob_score': True, 'clf__warm_start': False, 'vect__max_df': 0.6777777777777778, 'vect__min_df': 0.005281483987263175, 'vect__ngram_range': (1, 2), 'vect__stop_words': None, 'vect__token_pattern': '\\w{2,}'}, {'clf__base_estimator': None, 'clf__bootstrap': True, 'clf__bootstrap_features': True, 'clf__max_features': 0.727184705159369, 'clf__max_samples': 0.22250765850153997, 'clf__n_estimators': 148, 'clf__oob_score': False, 'clf__warm_start': True, 'vect__max_df': 0.5, 'vect__min_df': 0.00013721198472947332, 'vect__ngram_range': (1, 2), 'vect__stop_words': None, 'vect__token_pattern': '\\w{2,}'}, {'clf__base_estimator': None, 'clf__bootstrap': False, 'clf__bootstrap_features': True, 'clf__max_features': 0.2601507017519247, 'clf__max_samples': 0.007729921432522158, 'clf__n_estimators': 56, 'clf__oob_score': True, 'clf__warm_start': True, 'vect__max_df': 0.8555555555555556, 'vect__min_df': 0.0009961732931659346, 'vect__ngram_range': (1, 2), 'vect__stop_words': 'english', 'vect__token_pattern': '\\w{2,}'}, {'clf__base_estimator': MultinomialNB(alpha=1.0, class_prior=None, fit_prior=True), 'clf__bootstrap': True, 'clf__bootstrap_features': True, 'clf__max_features': 0.30847453480970344, 'clf__max_samples': 0.47069058370522077, 'clf__n_estimators': 193, 'clf__oob_score': True, 'clf__warm_start': True, 'vect__max_df': 0.5888888888888889, 'vect__min_df': 0.003937448231841961, 'vect__ngram_range': (1, 2), 'vect__stop_words': 'english', 'vect__token_pattern': '\\w{2,}'}, {'clf__base_estimator': Pipeline(memory=None,
         steps=[('vect',
                 TfidfVectorizer(analyzer='word', binary=False,
                                 decode_error='strict',
                                 dtype=<class 'numpy.float64'>,
                                 encoding='utf-8', input='content',
                                 lowercase=True, max_df=0.7666666666666666,
                                 max_features=None,
                                 min_df=0.00010572508827796376,
                                 ngram_range=(1, 2), norm='l2',
                                 preprocessor=None, smooth_idf=True,
                                 stop_words='english', strip_accents=None,
                                 sublinear_tf=False, token_pattern='\\w{1,}',
                                 tokenizer=None, use_idf=True,
                                 vocabulary=None)),
                ('clf',
                 LinearSVC(C=99.37702864845689, class_weight=None, dual=False,
                           fit_intercept=False, intercept_scaling=1,
                           loss='squared_hinge', max_iter=1000,
                           multi_class='ovr', penalty='l2', random_state=0,
                           tol=3.8359242531998225e-06, verbose=0))],
         verbose=False), 'clf__bootstrap': True, 'clf__bootstrap_features': True, 'clf__max_features': 0.5728451395817441, 'clf__max_samples': 0.5039549386743591, 'clf__n_estimators': 324, 'clf__oob_score': True, 'clf__warm_start': True, 'vect__max_df': 0.5444444444444444, 'vect__min_df': 0.00014633039720326354, 'vect__ngram_range': (1, 1), 'vect__stop_words': 'english', 'vect__token_pattern': '\\w{1,}'}, {'clf__base_estimator': Pipeline(memory=None,
         steps=[('vect',
                 TfidfVectorizer(analyzer='word', binary=False,
                                 decode_error='strict',
                                 dtype=<class 'numpy.float64'>,
                                 encoding='utf-8', input='content',
                                 lowercase=True, max_df=0.6777777777777778,
                                 max_features=None,
                                 min_df=0.0012707570261625706,
                                 ngram_range=(1, 2), norm='l2',
                                 preprocessor=None, smooth_idf=True,
                                 stop_words=None, strip_accents=None,...
                                 vocabulary=None)),
                ('clf',
                 DecisionTreeClassifier(ccp_alpha=0.0, class_weight=None,
                                        criterion='gini', max_depth=49,
                                        max_features=0.56, max_leaf_nodes=316,
                                        min_impurity_decrease=1e-07,
                                        min_impurity_split=None,
                                        min_samples_leaf=4,
                                        min_samples_split=0.015198715678453873,
                                        min_weight_fraction_leaf=0.0,
                                        presort='deprecated', random_state=0,
                                        splitter='random'))],
         verbose=False), 'clf__bootstrap': True, 'clf__bootstrap_features': True, 'clf__max_features': 0.4008274816832872, 'clf__max_samples': 0.09869685745184587, 'clf__n_estimators': 328, 'clf__oob_score': False, 'clf__warm_start': False, 'vect__max_df': 0.7222222222222222, 'vect__min_df': 0.00012231017198945855, 'vect__ngram_range': (1, 2), 'vect__stop_words': None, 'vect__token_pattern': '\\w{1,}'}, {'clf__base_estimator': None, 'clf__bootstrap': True, 'clf__bootstrap_features': True, 'clf__max_features': 0.49538255661070274, 'clf__max_samples': 0.04808078048923714, 'clf__n_estimators': 305, 'clf__oob_score': True, 'clf__warm_start': False, 'vect__max_df': 0.5, 'vect__min_df': 0.0005612551158176723, 'vect__ngram_range': (1, 1), 'vect__stop_words': 'english', 'vect__token_pattern': '\\w{1,}'}, {'clf__base_estimator': Pipeline(memory=None,
         steps=[('vect',
                 TfidfVectorizer(analyzer='word', binary=False,
                                 decode_error='strict',
                                 dtype=<class 'numpy.float64'>,
                                 encoding='utf-8', input='content',
                                 lowercase=True, max_df=0.7222222222222222,
                                 max_features=None,
                                 min_df=0.00014705642569160768,
                                 ngram_range=(1, 1), norm='l2',
                                 preprocessor=None, smooth_idf=True,
                                 stop_words=None, strip_accents=None...
                                        max_features=0.01, max_leaf_nodes=None,
                                        max_samples=0.007808728196651904,
                                        min_impurity_decrease=3.1622776601683795e-10,
                                        min_impurity_split=None,
                                        min_samples_leaf=62,
                                        min_samples_split=0.0008966659727253451,
                                        min_weight_fraction_leaf=0.0007448459487372297,
                                        n_estimators=251, n_jobs=None,
                                        oob_score=False, random_state=0,
                                        verbose=True, warm_start=True))],
         verbose=False), 'clf__bootstrap': True, 'clf__bootstrap_features': False, 'clf__max_features': 0.3528695517314673, 'clf__max_samples': 0.376727221218914, 'clf__n_estimators': 303, 'clf__oob_score': True, 'clf__warm_start': True, 'vect__max_df': 0.8555555555555556, 'vect__min_df': 0.0002889041142593464, 'vect__ngram_range': (1, 2), 'vect__stop_words': None, 'vect__token_pattern': '\\w{2,}'}, {'clf__base_estimator': Pipeline(memory=None,
         steps=[('vect',
                 TfidfVectorizer(analyzer='word', binary=False,
                                 decode_error='strict',
                                 dtype=<class 'numpy.float64'>,
                                 encoding='utf-8', input='content',
                                 lowercase=True, max_df=0.8555555555555556,
                                 max_features=None,
                                 min_df=0.00016483617040605902,
                                 ngram_range=(1, 2), norm='l2',
                                 preprocessor=None, smooth_idf=True,
                                 stop_words='english', strip_accents...
                                 tokenizer=None, use_idf=True,
                                 vocabulary=None)),
                ('clf',
                 LogisticRegression(C=0.47946121772409134, class_weight=None,
                                    dual=False, fit_intercept=True,
                                    intercept_scaling=1,
                                    l1_ratio=0.9168495407673116, max_iter=75,
                                    multi_class='multinomial', n_jobs=None,
                                    penalty='none', random_state=0,
                                    solver='sag', tol=6.514227791666363e-07,
                                    verbose=0, warm_start=False))],
         verbose=False), 'clf__bootstrap': True, 'clf__bootstrap_features': True, 'clf__max_features': 0.9389390531664465, 'clf__max_samples': 0.9554426141881001, 'clf__n_estimators': 290, 'clf__oob_score': False, 'clf__warm_start': False, 'vect__max_df': 0.5444444444444444, 'vect__min_df': 0.00045934343801172197, 'vect__ngram_range': (1, 2), 'vect__stop_words': None, 'vect__token_pattern': '\\w{2,}'}, {'clf__base_estimator': Pipeline(memory=None,
         steps=[('vect',
                 TfidfVectorizer(analyzer='word', binary=False,
                                 decode_error='strict',
                                 dtype=<class 'numpy.float64'>,
                                 encoding='utf-8', input='content',
                                 lowercase=True, max_df=0.6777777777777778,
                                 max_features=None,
                                 min_df=0.0012707570261625706,
                                 ngram_range=(1, 2), norm='l2',
                                 preprocessor=None, smooth_idf=True,
                                 stop_words=None, strip_accents=None,...
                                 vocabulary=None)),
                ('clf',
                 DecisionTreeClassifier(ccp_alpha=0.0, class_weight=None,
                                        criterion='gini', max_depth=49,
                                        max_features=0.56, max_leaf_nodes=316,
                                        min_impurity_decrease=1e-07,
                                        min_impurity_split=None,
                                        min_samples_leaf=4,
                                        min_samples_split=0.015198715678453873,
                                        min_weight_fraction_leaf=0.0,
                                        presort='deprecated', random_state=0,
                                        splitter='random'))],
         verbose=False), 'clf__bootstrap': True, 'clf__bootstrap_features': False, 'clf__max_features': 0.7479062299476722, 'clf__max_samples': 0.29725303018461946, 'clf__n_estimators': 351, 'clf__oob_score': False, 'clf__warm_start': True, 'vect__max_df': 0.5444444444444444, 'vect__min_df': 0.00022473608112151246, 'vect__ngram_range': (1, 1), 'vect__stop_words': 'english', 'vect__token_pattern': '\\w{2,}'}, {'clf__base_estimator': None, 'clf__bootstrap': True, 'clf__bootstrap_features': True, 'clf__max_features': 0.638495410494338, 'clf__max_samples': 0.7548770895467611, 'clf__n_estimators': 46, 'clf__oob_score': True, 'clf__warm_start': False, 'vect__max_df': 0.7222222222222222, 'vect__min_df': 0.00022363202156393489, 'vect__ngram_range': (1, 2), 'vect__stop_words': None, 'vect__token_pattern': '\\w{1,}'}, {'clf__base_estimator': None, 'clf__bootstrap': False, 'clf__bootstrap_features': True, 'clf__max_features': 0.6432583119624558, 'clf__max_samples': 0.5644735982311048, 'clf__n_estimators': 297, 'clf__oob_score': True, 'clf__warm_start': False, 'vect__max_df': 0.5444444444444444, 'vect__min_df': 0.0026849386756358827, 'vect__ngram_range': (1, 2), 'vect__stop_words': 'english', 'vect__token_pattern': '\\w{1,}'}, {'clf__base_estimator': Pipeline(memory=None,
         steps=[('vect',
                 TfidfVectorizer(analyzer='word', binary=False,
                                 decode_error='strict',
                                 dtype=<class 'numpy.float64'>,
                                 encoding='utf-8', input='content',
                                 lowercase=True, max_df=0.6333333333333333,
                                 max_features=None,
                                 min_df=0.0010083260374073057,
                                 ngram_range=(1, 1), norm='l2',
                                 preprocessor=None, smooth_idf=True,
                                 stop_words='english', strip_accents=...
                                 tokenizer=None, use_idf=True,
                                 vocabulary=None)),
                ('clf',
                 AdaBoostClassifier(algorithm='SAMME',
                                    base_estimator=LinearSVC(C=1.0,
                                                             class_weight=None,
                                                             dual=True,
                                                             fit_intercept=True,
                                                             intercept_scaling=1,
                                                             loss='squared_hinge',
                                                             max_iter=1000,
                                                             multi_class='ovr',
                                                             penalty='l2',
                                                             random_state=None,
                                                             tol=0.0001,
                                                             verbose=0),
                                    learning_rate=0.40053644159285917,
                                    n_estimators=190, random_state=0))],
         verbose=False), 'clf__bootstrap': True, 'clf__bootstrap_features': True, 'clf__max_features': 0.7893894517715161, 'clf__max_samples': 0.8231380350068013, 'clf__n_estimators': 58, 'clf__oob_score': True, 'clf__warm_start': True, 'vect__max_df': 0.6777777777777778, 'vect__min_df': 0.005899296406890342, 'vect__ngram_range': (1, 2), 'vect__stop_words': 'english', 'vect__token_pattern': '\\w{2,}'}, {'clf__base_estimator': MultinomialNB(alpha=1.0, class_prior=None, fit_prior=True), 'clf__bootstrap': True, 'clf__bootstrap_features': False, 'clf__max_features': 0.5719363377342171, 'clf__max_samples': 0.5265974126524174, 'clf__n_estimators': 193, 'clf__oob_score': True, 'clf__warm_start': False, 'vect__max_df': 0.6777777777777778, 'vect__min_df': 0.00022641386006295574, 'vect__ngram_range': (1, 2), 'vect__stop_words': None, 'vect__token_pattern': '\\w{1,}'}, {'clf__base_estimator': Pipeline(memory=None,
         steps=[('vect',
                 TfidfVectorizer(analyzer='word', binary=False,
                                 decode_error='strict',
                                 dtype=<class 'numpy.float64'>,
                                 encoding='utf-8', input='content',
                                 lowercase=True, max_df=0.8555555555555556,
                                 max_features=None,
                                 min_df=0.00016483617040605902,
                                 ngram_range=(1, 2), norm='l2',
                                 preprocessor=None, smooth_idf=True,
                                 stop_words='english', strip_accents...
                                 tokenizer=None, use_idf=True,
                                 vocabulary=None)),
                ('clf',
                 LogisticRegression(C=0.47946121772409134, class_weight=None,
                                    dual=False, fit_intercept=True,
                                    intercept_scaling=1,
                                    l1_ratio=0.9168495407673116, max_iter=75,
                                    multi_class='multinomial', n_jobs=None,
                                    penalty='none', random_state=0,
                                    solver='sag', tol=6.514227791666363e-07,
                                    verbose=0, warm_start=False))],
         verbose=False), 'clf__bootstrap': True, 'clf__bootstrap_features': False, 'clf__max_features': 0.25478836014821593, 'clf__max_samples': 0.8922210588536693, 'clf__n_estimators': 133, 'clf__oob_score': True, 'clf__warm_start': True, 'vect__max_df': 0.8555555555555556, 'vect__min_df': 0.0001678236169140789, 'vect__ngram_range': (1, 2), 'vect__stop_words': None, 'vect__token_pattern': '\\w{2,}'}, {'clf__base_estimator': Pipeline(memory=None,
         steps=[('vect',
                 TfidfVectorizer(analyzer='word', binary=False,
                                 decode_error='strict',
                                 dtype=<class 'numpy.float64'>,
                                 encoding='utf-8', input='content',
                                 lowercase=True, max_df=0.7666666666666666,
                                 max_features=None,
                                 min_df=0.00010572508827796376,
                                 ngram_range=(1, 2), norm='l2',
                                 preprocessor=None, smooth_idf=True,
                                 stop_words='english', strip_accents=None,
                                 sublinear_tf=False, token_pattern='\\w{1,}',
                                 tokenizer=None, use_idf=True,
                                 vocabulary=None)),
                ('clf',
                 LinearSVC(C=99.37702864845689, class_weight=None, dual=False,
                           fit_intercept=False, intercept_scaling=1,
                           loss='squared_hinge', max_iter=1000,
                           multi_class='ovr', penalty='l2', random_state=0,
                           tol=3.8359242531998225e-06, verbose=0))],
         verbose=False), 'clf__bootstrap': True, 'clf__bootstrap_features': True, 'clf__max_features': 0.47727954840637454, 'clf__max_samples': 0.5825156734183312, 'clf__n_estimators': 32, 'clf__oob_score': False, 'clf__warm_start': False, 'vect__max_df': 0.6333333333333333, 'vect__min_df': 0.0003294164160658745, 'vect__ngram_range': (1, 1), 'vect__stop_words': 'english', 'vect__token_pattern': '\\w{2,}'}, {'clf__base_estimator': Pipeline(memory=None,
         steps=[('vect',
                 TfidfVectorizer(analyzer='word', binary=False,
                                 decode_error='strict',
                                 dtype=<class 'numpy.float64'>,
                                 encoding='utf-8', input='content',
                                 lowercase=True, max_df=0.7222222222222222,
                                 max_features=None,
                                 min_df=0.00014705642569160768,
                                 ngram_range=(1, 1), norm='l2',
                                 preprocessor=None, smooth_idf=True,
                                 stop_words=None, strip_accents=None...
                                        max_features=0.01, max_leaf_nodes=None,
                                        max_samples=0.007808728196651904,
                                        min_impurity_decrease=3.1622776601683795e-10,
                                        min_impurity_split=None,
                                        min_samples_leaf=62,
                                        min_samples_split=0.0008966659727253451,
                                        min_weight_fraction_leaf=0.0007448459487372297,
                                        n_estimators=251, n_jobs=None,
                                        oob_score=False, random_state=0,
                                        verbose=True, warm_start=True))],
         verbose=False), 'clf__bootstrap': False, 'clf__bootstrap_features': False, 'clf__max_features': 0.2976403548989238, 'clf__max_samples': 0.09151833628746264, 'clf__n_estimators': 165, 'clf__oob_score': False, 'clf__warm_start': True, 'vect__max_df': 0.8111111111111111, 'vect__min_df': 0.006787725076990622, 'vect__ngram_range': (1, 1), 'vect__stop_words': None, 'vect__token_pattern': '\\w{1,}'}, {'clf__base_estimator': Pipeline(memory=None,
         steps=[('vect',
                 TfidfVectorizer(analyzer='word', binary=False,
                                 decode_error='strict',
                                 dtype=<class 'numpy.float64'>,
                                 encoding='utf-8', input='content',
                                 lowercase=True, max_df=0.6333333333333333,
                                 max_features=None,
                                 min_df=0.0010083260374073057,
                                 ngram_range=(1, 1), norm='l2',
                                 preprocessor=None, smooth_idf=True,
                                 stop_words='english', strip_accents=...
                                 tokenizer=None, use_idf=True,
                                 vocabulary=None)),
                ('clf',
                 AdaBoostClassifier(algorithm='SAMME',
                                    base_estimator=LinearSVC(C=1.0,
                                                             class_weight=None,
                                                             dual=True,
                                                             fit_intercept=True,
                                                             intercept_scaling=1,
                                                             loss='squared_hinge',
                                                             max_iter=1000,
                                                             multi_class='ovr',
                                                             penalty='l2',
                                                             random_state=None,
                                                             tol=0.0001,
                                                             verbose=0),
                                    learning_rate=0.40053644159285917,
                                    n_estimators=190, random_state=0))],
         verbose=False), 'clf__bootstrap': False, 'clf__bootstrap_features': False, 'clf__max_features': 0.7595081502813072, 'clf__max_samples': 0.903855776861897, 'clf__n_estimators': 54, 'clf__oob_score': False, 'clf__warm_start': True, 'vect__max_df': 0.5888888888888889, 'vect__min_df': 0.0028374042784047823, 'vect__ngram_range': (1, 1), 'vect__stop_words': None, 'vect__token_pattern': '\\w{1,}'}, {'clf__base_estimator': Pipeline(memory=None,
         steps=[('vect',
                 TfidfVectorizer(analyzer='word', binary=False,
                                 decode_error='strict',
                                 dtype=<class 'numpy.float64'>,
                                 encoding='utf-8', input='content',
                                 lowercase=True, max_df=0.6333333333333333,
                                 max_features=None,
                                 min_df=0.0010083260374073057,
                                 ngram_range=(1, 1), norm='l2',
                                 preprocessor=None, smooth_idf=True,
                                 stop_words='english', strip_accents=...
                                 tokenizer=None, use_idf=True,
                                 vocabulary=None)),
                ('clf',
                 AdaBoostClassifier(algorithm='SAMME',
                                    base_estimator=LinearSVC(C=1.0,
                                                             class_weight=None,
                                                             dual=True,
                                                             fit_intercept=True,
                                                             intercept_scaling=1,
                                                             loss='squared_hinge',
                                                             max_iter=1000,
                                                             multi_class='ovr',
                                                             penalty='l2',
                                                             random_state=None,
                                                             tol=0.0001,
                                                             verbose=0),
                                    learning_rate=0.40053644159285917,
                                    n_estimators=190, random_state=0))],
         verbose=False), 'clf__bootstrap': True, 'clf__bootstrap_features': True, 'clf__max_features': 0.6435766743368304, 'clf__max_samples': 0.385892732539557, 'clf__n_estimators': 154, 'clf__oob_score': True, 'clf__warm_start': True, 'vect__max_df': 0.5888888888888889, 'vect__min_df': 0.00010110534491283214, 'vect__ngram_range': (1, 1), 'vect__stop_words': 'english', 'vect__token_pattern': '\\w{2,}'}, {'clf__base_estimator': Pipeline(memory=None,
         steps=[('vect',
                 TfidfVectorizer(analyzer='word', binary=False,
                                 decode_error='strict',
                                 dtype=<class 'numpy.float64'>,
                                 encoding='utf-8', input='content',
                                 lowercase=True, max_df=0.6777777777777778,
                                 max_features=None,
                                 min_df=0.0012707570261625706,
                                 ngram_range=(1, 2), norm='l2',
                                 preprocessor=None, smooth_idf=True,
                                 stop_words=None, strip_accents=None,...
                                 vocabulary=None)),
                ('clf',
                 DecisionTreeClassifier(ccp_alpha=0.0, class_weight=None,
                                        criterion='gini', max_depth=49,
                                        max_features=0.56, max_leaf_nodes=316,
                                        min_impurity_decrease=1e-07,
                                        min_impurity_split=None,
                                        min_samples_leaf=4,
                                        min_samples_split=0.015198715678453873,
                                        min_weight_fraction_leaf=0.0,
                                        presort='deprecated', random_state=0,
                                        splitter='random'))],
         verbose=False), 'clf__bootstrap': True, 'clf__bootstrap_features': False, 'clf__max_features': 0.008372228833342876, 'clf__max_samples': 0.415391809109534, 'clf__n_estimators': 66, 'clf__oob_score': False, 'clf__warm_start': False, 'vect__max_df': 0.7222222222222222, 'vect__min_df': 0.00011433204615976263, 'vect__ngram_range': (1, 1), 'vect__stop_words': 'english', 'vect__token_pattern': '\\w{2,}'}, {'clf__base_estimator': None, 'clf__bootstrap': True, 'clf__bootstrap_features': False, 'clf__max_features': 0.8444328241010104, 'clf__max_samples': 0.5985287428914258, 'clf__n_estimators': 82, 'clf__oob_score': True, 'clf__warm_start': False, 'vect__max_df': 0.8111111111111111, 'vect__min_df': 0.006039793362876566, 'vect__ngram_range': (1, 2), 'vect__stop_words': 'english', 'vect__token_pattern': '\\w{1,}'}, {'clf__base_estimator': Pipeline(memory=None,
         steps=[('vect',
                 TfidfVectorizer(analyzer='word', binary=False,
                                 decode_error='strict',
                                 dtype=<class 'numpy.float64'>,
                                 encoding='utf-8', input='content',
                                 lowercase=True, max_df=0.7666666666666666,
                                 max_features=None,
                                 min_df=0.00010572508827796376,
                                 ngram_range=(1, 2), norm='l2',
                                 preprocessor=None, smooth_idf=True,
                                 stop_words='english', strip_accents=None,
                                 sublinear_tf=False, token_pattern='\\w{1,}',
                                 tokenizer=None, use_idf=True,
                                 vocabulary=None)),
                ('clf',
                 LinearSVC(C=99.37702864845689, class_weight=None, dual=False,
                           fit_intercept=False, intercept_scaling=1,
                           loss='squared_hinge', max_iter=1000,
                           multi_class='ovr', penalty='l2', random_state=0,
                           tol=3.8359242531998225e-06, verbose=0))],
         verbose=False), 'clf__bootstrap': True, 'clf__bootstrap_features': True, 'clf__max_features': 0.4171102830987832, 'clf__max_samples': 0.18390903840461814, 'clf__n_estimators': 99, 'clf__oob_score': True, 'clf__warm_start': False, 'vect__max_df': 0.9, 'vect__min_df': 0.006245718070590018, 'vect__ngram_range': (1, 1), 'vect__stop_words': 'english', 'vect__token_pattern': '\\w{1,}'}, {'clf__base_estimator': Pipeline(memory=None,
         steps=[('vect',
                 TfidfVectorizer(analyzer='word', binary=False,
                                 decode_error='strict',
                                 dtype=<class 'numpy.float64'>,
                                 encoding='utf-8', input='content',
                                 lowercase=True, max_df=0.6777777777777778,
                                 max_features=None,
                                 min_df=0.0012707570261625706,
                                 ngram_range=(1, 2), norm='l2',
                                 preprocessor=None, smooth_idf=True,
                                 stop_words=None, strip_accents=None,...
                                 vocabulary=None)),
                ('clf',
                 DecisionTreeClassifier(ccp_alpha=0.0, class_weight=None,
                                        criterion='gini', max_depth=49,
                                        max_features=0.56, max_leaf_nodes=316,
                                        min_impurity_decrease=1e-07,
                                        min_impurity_split=None,
                                        min_samples_leaf=4,
                                        min_samples_split=0.015198715678453873,
                                        min_weight_fraction_leaf=0.0,
                                        presort='deprecated', random_state=0,
                                        splitter='random'))],
         verbose=False), 'clf__bootstrap': True, 'clf__bootstrap_features': False, 'clf__max_features': 0.37137232006455667, 'clf__max_samples': 0.023256728536597104, 'clf__n_estimators': 57, 'clf__oob_score': True, 'clf__warm_start': True, 'vect__max_df': 0.7222222222222222, 'vect__min_df': 0.0011176748476231888, 'vect__ngram_range': (1, 1), 'vect__stop_words': None, 'vect__token_pattern': '\\w{2,}'}, {'clf__base_estimator': MultinomialNB(alpha=1.0, class_prior=None, fit_prior=True), 'clf__bootstrap': False, 'clf__bootstrap_features': False, 'clf__max_features': 0.9120442729722144, 'clf__max_samples': 0.7892022585496922, 'clf__n_estimators': 209, 'clf__oob_score': False, 'clf__warm_start': False, 'vect__max_df': 0.6333333333333333, 'vect__min_df': 0.00011618768297197917, 'vect__ngram_range': (1, 2), 'vect__stop_words': 'english', 'vect__token_pattern': '\\w{2,}'}, {'clf__base_estimator': None, 'clf__bootstrap': False, 'clf__bootstrap_features': False, 'clf__max_features': 0.007139068602398213, 'clf__max_samples': 0.13171362073366055, 'clf__n_estimators': 389, 'clf__oob_score': True, 'clf__warm_start': True, 'vect__max_df': 0.5, 'vect__min_df': 0.00014619490533752404, 'vect__ngram_range': (1, 2), 'vect__stop_words': 'english', 'vect__token_pattern': '\\w{1,}'}, {'clf__base_estimator': Pipeline(memory=None,
         steps=[('vect',
                 TfidfVectorizer(analyzer='word', binary=False,
                                 decode_error='strict',
                                 dtype=<class 'numpy.float64'>,
                                 encoding='utf-8', input='content',
                                 lowercase=True, max_df=0.7666666666666666,
                                 max_features=None,
                                 min_df=0.00010572508827796376,
                                 ngram_range=(1, 2), norm='l2',
                                 preprocessor=None, smooth_idf=True,
                                 stop_words='english', strip_accents=None,
                                 sublinear_tf=False, token_pattern='\\w{1,}',
                                 tokenizer=None, use_idf=True,
                                 vocabulary=None)),
                ('clf',
                 LinearSVC(C=99.37702864845689, class_weight=None, dual=False,
                           fit_intercept=False, intercept_scaling=1,
                           loss='squared_hinge', max_iter=1000,
                           multi_class='ovr', penalty='l2', random_state=0,
                           tol=3.8359242531998225e-06, verbose=0))],
         verbose=False), 'clf__bootstrap': True, 'clf__bootstrap_features': False, 'clf__max_features': 0.09277274417469461, 'clf__max_samples': 0.6258048798011573, 'clf__n_estimators': 185, 'clf__oob_score': False, 'clf__warm_start': True, 'vect__max_df': 0.5888888888888889, 'vect__min_df': 0.008728335560411056, 'vect__ngram_range': (1, 1), 'vect__stop_words': 'english', 'vect__token_pattern': '\\w{1,}'}, {'clf__base_estimator': Pipeline(memory=None,
         steps=[('vect',
                 TfidfVectorizer(analyzer='word', binary=False,
                                 decode_error='strict',
                                 dtype=<class 'numpy.float64'>,
                                 encoding='utf-8', input='content',
                                 lowercase=True, max_df=0.8555555555555556,
                                 max_features=None,
                                 min_df=0.00016483617040605902,
                                 ngram_range=(1, 2), norm='l2',
                                 preprocessor=None, smooth_idf=True,
                                 stop_words='english', strip_accents...
                                 tokenizer=None, use_idf=True,
                                 vocabulary=None)),
                ('clf',
                 LogisticRegression(C=0.47946121772409134, class_weight=None,
                                    dual=False, fit_intercept=True,
                                    intercept_scaling=1,
                                    l1_ratio=0.9168495407673116, max_iter=75,
                                    multi_class='multinomial', n_jobs=None,
                                    penalty='none', random_state=0,
                                    solver='sag', tol=6.514227791666363e-07,
                                    verbose=0, warm_start=False))],
         verbose=False), 'clf__bootstrap': True, 'clf__bootstrap_features': True, 'clf__max_features': 0.4250279746264455, 'clf__max_samples': 0.751836587228482, 'clf__n_estimators': 29, 'clf__oob_score': True, 'clf__warm_start': False, 'vect__max_df': 0.7222222222222222, 'vect__min_df': 0.0010956337685385177, 'vect__ngram_range': (1, 2), 'vect__stop_words': None, 'vect__token_pattern': '\\w{2,}'}, {'clf__base_estimator': None, 'clf__bootstrap': True, 'clf__bootstrap_features': True, 'clf__max_features': 0.5293424173908581, 'clf__max_samples': 0.9110648211081864, 'clf__n_estimators': 164, 'clf__oob_score': False, 'clf__warm_start': False, 'vect__max_df': 0.9, 'vect__min_df': 0.00043848197430785386, 'vect__ngram_range': (1, 1), 'vect__stop_words': 'english', 'vect__token_pattern': '\\w{1,}'}, {'clf__base_estimator': Pipeline(memory=None,
         steps=[('vect',
                 TfidfVectorizer(analyzer='word', binary=False,
                                 decode_error='strict',
                                 dtype=<class 'numpy.float64'>,
                                 encoding='utf-8', input='content',
                                 lowercase=True, max_df=0.6333333333333333,
                                 max_features=None,
                                 min_df=0.0010083260374073057,
                                 ngram_range=(1, 1), norm='l2',
                                 preprocessor=None, smooth_idf=True,
                                 stop_words='english', strip_accents=...
                                 tokenizer=None, use_idf=True,
                                 vocabulary=None)),
                ('clf',
                 AdaBoostClassifier(algorithm='SAMME',
                                    base_estimator=LinearSVC(C=1.0,
                                                             class_weight=None,
                                                             dual=True,
                                                             fit_intercept=True,
                                                             intercept_scaling=1,
                                                             loss='squared_hinge',
                                                             max_iter=1000,
                                                             multi_class='ovr',
                                                             penalty='l2',
                                                             random_state=None,
                                                             tol=0.0001,
                                                             verbose=0),
                                    learning_rate=0.40053644159285917,
                                    n_estimators=190, random_state=0))],
         verbose=False), 'clf__bootstrap': True, 'clf__bootstrap_features': False, 'clf__max_features': 0.10771967723621545, 'clf__max_samples': 0.2572248357701825, 'clf__n_estimators': 299, 'clf__oob_score': True, 'clf__warm_start': False, 'vect__max_df': 0.8111111111111111, 'vect__min_df': 0.0005036350174824408, 'vect__ngram_range': (1, 2), 'vect__stop_words': 'english', 'vect__token_pattern': '\\w{2,}'}, {'clf__base_estimator': Pipeline(memory=None,
         steps=[('vect',
                 TfidfVectorizer(analyzer='word', binary=False,
                                 decode_error='strict',
                                 dtype=<class 'numpy.float64'>,
                                 encoding='utf-8', input='content',
                                 lowercase=True, max_df=0.7222222222222222,
                                 max_features=None,
                                 min_df=0.00014705642569160768,
                                 ngram_range=(1, 1), norm='l2',
                                 preprocessor=None, smooth_idf=True,
                                 stop_words=None, strip_accents=None...
                                        max_features=0.01, max_leaf_nodes=None,
                                        max_samples=0.007808728196651904,
                                        min_impurity_decrease=3.1622776601683795e-10,
                                        min_impurity_split=None,
                                        min_samples_leaf=62,
                                        min_samples_split=0.0008966659727253451,
                                        min_weight_fraction_leaf=0.0007448459487372297,
                                        n_estimators=251, n_jobs=None,
                                        oob_score=False, random_state=0,
                                        verbose=True, warm_start=True))],
         verbose=False), 'clf__bootstrap': False, 'clf__bootstrap_features': False, 'clf__max_features': 0.45507498143373115, 'clf__max_samples': 0.8548262172292455, 'clf__n_estimators': 300, 'clf__oob_score': False, 'clf__warm_start': False, 'vect__max_df': 0.8111111111111111, 'vect__min_df': 0.000992503800405849, 'vect__ngram_range': (1, 2), 'vect__stop_words': None, 'vect__token_pattern': '\\w{1,}'}, {'clf__base_estimator': Pipeline(memory=None,
         steps=[('vect',
                 TfidfVectorizer(analyzer='word', binary=False,
                                 decode_error='strict',
                                 dtype=<class 'numpy.float64'>,
                                 encoding='utf-8', input='content',
                                 lowercase=True, max_df=0.7222222222222222,
                                 max_features=None,
                                 min_df=0.00014705642569160768,
                                 ngram_range=(1, 1), norm='l2',
                                 preprocessor=None, smooth_idf=True,
                                 stop_words=None, strip_accents=None...
                                        max_features=0.01, max_leaf_nodes=None,
                                        max_samples=0.007808728196651904,
                                        min_impurity_decrease=3.1622776601683795e-10,
                                        min_impurity_split=None,
                                        min_samples_leaf=62,
                                        min_samples_split=0.0008966659727253451,
                                        min_weight_fraction_leaf=0.0007448459487372297,
                                        n_estimators=251, n_jobs=None,
                                        oob_score=False, random_state=0,
                                        verbose=True, warm_start=True))],
         verbose=False), 'clf__bootstrap': False, 'clf__bootstrap_features': True, 'clf__max_features': 0.03511115590254876, 'clf__max_samples': 0.3552915209323252, 'clf__n_estimators': 129, 'clf__oob_score': True, 'clf__warm_start': True, 'vect__max_df': 0.6333333333333333, 'vect__min_df': 0.0002743292348975704, 'vect__ngram_range': (1, 1), 'vect__stop_words': 'english', 'vect__token_pattern': '\\w{1,}'}, {'clf__base_estimator': Pipeline(memory=None,
         steps=[('vect',
                 TfidfVectorizer(analyzer='word', binary=False,
                                 decode_error='strict',
                                 dtype=<class 'numpy.float64'>,
                                 encoding='utf-8', input='content',
                                 lowercase=True, max_df=0.6333333333333333,
                                 max_features=None,
                                 min_df=0.0010083260374073057,
                                 ngram_range=(1, 1), norm='l2',
                                 preprocessor=None, smooth_idf=True,
                                 stop_words='english', strip_accents=...
                                 tokenizer=None, use_idf=True,
                                 vocabulary=None)),
                ('clf',
                 AdaBoostClassifier(algorithm='SAMME',
                                    base_estimator=LinearSVC(C=1.0,
                                                             class_weight=None,
                                                             dual=True,
                                                             fit_intercept=True,
                                                             intercept_scaling=1,
                                                             loss='squared_hinge',
                                                             max_iter=1000,
                                                             multi_class='ovr',
                                                             penalty='l2',
                                                             random_state=None,
                                                             tol=0.0001,
                                                             verbose=0),
                                    learning_rate=0.40053644159285917,
                                    n_estimators=190, random_state=0))],
         verbose=False), 'clf__bootstrap': True, 'clf__bootstrap_features': False, 'clf__max_features': 0.17421674979066692, 'clf__max_samples': 0.8113971982389097, 'clf__n_estimators': 263, 'clf__oob_score': True, 'clf__warm_start': True, 'vect__max_df': 0.6333333333333333, 'vect__min_df': 0.004144680082476916, 'vect__ngram_range': (1, 2), 'vect__stop_words': 'english', 'vect__token_pattern': '\\w{1,}'}, {'clf__base_estimator': Pipeline(memory=None,
         steps=[('vect',
                 TfidfVectorizer(analyzer='word', binary=False,
                                 decode_error='strict',
                                 dtype=<class 'numpy.float64'>,
                                 encoding='utf-8', input='content',
                                 lowercase=True, max_df=0.8555555555555556,
                                 max_features=None,
                                 min_df=0.00016483617040605902,
                                 ngram_range=(1, 2), norm='l2',
                                 preprocessor=None, smooth_idf=True,
                                 stop_words='english', strip_accents...
                                 tokenizer=None, use_idf=True,
                                 vocabulary=None)),
                ('clf',
                 LogisticRegression(C=0.47946121772409134, class_weight=None,
                                    dual=False, fit_intercept=True,
                                    intercept_scaling=1,
                                    l1_ratio=0.9168495407673116, max_iter=75,
                                    multi_class='multinomial', n_jobs=None,
                                    penalty='none', random_state=0,
                                    solver='sag', tol=6.514227791666363e-07,
                                    verbose=0, warm_start=False))],
         verbose=False), 'clf__bootstrap': True, 'clf__bootstrap_features': False, 'clf__max_features': 0.47871284706891626, 'clf__max_samples': 0.8133637224031106, 'clf__n_estimators': 25, 'clf__oob_score': False, 'clf__warm_start': True, 'vect__max_df': 0.5, 'vect__min_df': 0.0038753426137013193, 'vect__ngram_range': (1, 1), 'vect__stop_words': 'english', 'vect__token_pattern': '\\w{2,}'}, {'clf__base_estimator': Pipeline(memory=None,
         steps=[('vect',
                 TfidfVectorizer(analyzer='word', binary=False,
                                 decode_error='strict',
                                 dtype=<class 'numpy.float64'>,
                                 encoding='utf-8', input='content',
                                 lowercase=True, max_df=0.7222222222222222,
                                 max_features=None,
                                 min_df=0.00014705642569160768,
                                 ngram_range=(1, 1), norm='l2',
                                 preprocessor=None, smooth_idf=True,
                                 stop_words=None, strip_accents=None...
                                        max_features=0.01, max_leaf_nodes=None,
                                        max_samples=0.007808728196651904,
                                        min_impurity_decrease=3.1622776601683795e-10,
                                        min_impurity_split=None,
                                        min_samples_leaf=62,
                                        min_samples_split=0.0008966659727253451,
                                        min_weight_fraction_leaf=0.0007448459487372297,
                                        n_estimators=251, n_jobs=None,
                                        oob_score=False, random_state=0,
                                        verbose=True, warm_start=True))],
         verbose=False), 'clf__bootstrap': False, 'clf__bootstrap_features': False, 'clf__max_features': 0.26869196415566154, 'clf__max_samples': 0.6456113691567699, 'clf__n_estimators': 219, 'clf__oob_score': True, 'clf__warm_start': True, 'vect__max_df': 0.9, 'vect__min_df': 0.007863381012280839, 'vect__ngram_range': (1, 1), 'vect__stop_words': 'english', 'vect__token_pattern': '\\w{1,}'}, {'clf__base_estimator': Pipeline(memory=None,
         steps=[('vect',
                 TfidfVectorizer(analyzer='word', binary=False,
                                 decode_error='strict',
                                 dtype=<class 'numpy.float64'>,
                                 encoding='utf-8', input='content',
                                 lowercase=True, max_df=0.6333333333333333,
                                 max_features=None,
                                 min_df=0.0010083260374073057,
                                 ngram_range=(1, 1), norm='l2',
                                 preprocessor=None, smooth_idf=True,
                                 stop_words='english', strip_accents=...
                                 tokenizer=None, use_idf=True,
                                 vocabulary=None)),
                ('clf',
                 AdaBoostClassifier(algorithm='SAMME',
                                    base_estimator=LinearSVC(C=1.0,
                                                             class_weight=None,
                                                             dual=True,
                                                             fit_intercept=True,
                                                             intercept_scaling=1,
                                                             loss='squared_hinge',
                                                             max_iter=1000,
                                                             multi_class='ovr',
                                                             penalty='l2',
                                                             random_state=None,
                                                             tol=0.0001,
                                                             verbose=0),
                                    learning_rate=0.40053644159285917,
                                    n_estimators=190, random_state=0))],
         verbose=False), 'clf__bootstrap': True, 'clf__bootstrap_features': False, 'clf__max_features': 0.8377209430590781, 'clf__max_samples': 0.07558349525571029, 'clf__n_estimators': 154, 'clf__oob_score': True, 'clf__warm_start': True, 'vect__max_df': 0.6777777777777778, 'vect__min_df': 0.005846593202055442, 'vect__ngram_range': (1, 2), 'vect__stop_words': 'english', 'vect__token_pattern': '\\w{2,}'}, {'clf__base_estimator': Pipeline(memory=None,
         steps=[('vect',
                 TfidfVectorizer(analyzer='word', binary=False,
                                 decode_error='strict',
                                 dtype=<class 'numpy.float64'>,
                                 encoding='utf-8', input='content',
                                 lowercase=True, max_df=0.7666666666666666,
                                 max_features=None,
                                 min_df=0.00010572508827796376,
                                 ngram_range=(1, 2), norm='l2',
                                 preprocessor=None, smooth_idf=True,
                                 stop_words='english', strip_accents=None,
                                 sublinear_tf=False, token_pattern='\\w{1,}',
                                 tokenizer=None, use_idf=True,
                                 vocabulary=None)),
                ('clf',
                 LinearSVC(C=99.37702864845689, class_weight=None, dual=False,
                           fit_intercept=False, intercept_scaling=1,
                           loss='squared_hinge', max_iter=1000,
                           multi_class='ovr', penalty='l2', random_state=0,
                           tol=3.8359242531998225e-06, verbose=0))],
         verbose=False), 'clf__bootstrap': True, 'clf__bootstrap_features': True, 'clf__max_features': 0.778318222225744, 'clf__max_samples': 0.40002243880555655, 'clf__n_estimators': 114, 'clf__oob_score': False, 'clf__warm_start': False, 'vect__max_df': 0.9, 'vect__min_df': 0.00012082491075995292, 'vect__ngram_range': (1, 2), 'vect__stop_words': None, 'vect__token_pattern': '\\w{1,}'}, {'clf__base_estimator': None, 'clf__bootstrap': True, 'clf__bootstrap_features': True, 'clf__max_features': 0.9072196124544909, 'clf__max_samples': 0.41884548470668737, 'clf__n_estimators': 398, 'clf__oob_score': False, 'clf__warm_start': False, 'vect__max_df': 0.7222222222222222, 'vect__min_df': 0.00011345558386229489, 'vect__ngram_range': (1, 2), 'vect__stop_words': 'english', 'vect__token_pattern': '\\w{2,}'}, {'clf__base_estimator': MultinomialNB(alpha=1.0, class_prior=None, fit_prior=True), 'clf__bootstrap': False, 'clf__bootstrap_features': True, 'clf__max_features': 0.544834583549896, 'clf__max_samples': 0.5409697872786783, 'clf__n_estimators': 112, 'clf__oob_score': True, 'clf__warm_start': True, 'vect__max_df': 0.7666666666666666, 'vect__min_df': 0.00037494029853853107, 'vect__ngram_range': (1, 2), 'vect__stop_words': None, 'vect__token_pattern': '\\w{1,}'}, {'clf__base_estimator': Pipeline(memory=None,
         steps=[('vect',
                 TfidfVectorizer(analyzer='word', binary=False,
                                 decode_error='strict',
                                 dtype=<class 'numpy.float64'>,
                                 encoding='utf-8', input='content',
                                 lowercase=True, max_df=0.6333333333333333,
                                 max_features=None,
                                 min_df=0.0010083260374073057,
                                 ngram_range=(1, 1), norm='l2',
                                 preprocessor=None, smooth_idf=True,
                                 stop_words='english', strip_accents=...
                                 tokenizer=None, use_idf=True,
                                 vocabulary=None)),
                ('clf',
                 AdaBoostClassifier(algorithm='SAMME',
                                    base_estimator=LinearSVC(C=1.0,
                                                             class_weight=None,
                                                             dual=True,
                                                             fit_intercept=True,
                                                             intercept_scaling=1,
                                                             loss='squared_hinge',
                                                             max_iter=1000,
                                                             multi_class='ovr',
                                                             penalty='l2',
                                                             random_state=None,
                                                             tol=0.0001,
                                                             verbose=0),
                                    learning_rate=0.40053644159285917,
                                    n_estimators=190, random_state=0))],
         verbose=False), 'clf__bootstrap': True, 'clf__bootstrap_features': True, 'clf__max_features': 0.7317945393233404, 'clf__max_samples': 0.8536505542381208, 'clf__n_estimators': 326, 'clf__oob_score': False, 'clf__warm_start': False, 'vect__max_df': 0.7222222222222222, 'vect__min_df': 0.0026528583442507123, 'vect__ngram_range': (1, 1), 'vect__stop_words': None, 'vect__token_pattern': '\\w{1,}'}, {'clf__base_estimator': Pipeline(memory=None,
         steps=[('vect',
                 TfidfVectorizer(analyzer='word', binary=False,
                                 decode_error='strict',
                                 dtype=<class 'numpy.float64'>,
                                 encoding='utf-8', input='content',
                                 lowercase=True, max_df=0.8555555555555556,
                                 max_features=None,
                                 min_df=0.00016483617040605902,
                                 ngram_range=(1, 2), norm='l2',
                                 preprocessor=None, smooth_idf=True,
                                 stop_words='english', strip_accents...
                                 tokenizer=None, use_idf=True,
                                 vocabulary=None)),
                ('clf',
                 LogisticRegression(C=0.47946121772409134, class_weight=None,
                                    dual=False, fit_intercept=True,
                                    intercept_scaling=1,
                                    l1_ratio=0.9168495407673116, max_iter=75,
                                    multi_class='multinomial', n_jobs=None,
                                    penalty='none', random_state=0,
                                    solver='sag', tol=6.514227791666363e-07,
                                    verbose=0, warm_start=False))],
         verbose=False), 'clf__bootstrap': True, 'clf__bootstrap_features': True, 'clf__max_features': 0.6188687452697725, 'clf__max_samples': 0.3040236097275262, 'clf__n_estimators': 350, 'clf__oob_score': True, 'clf__warm_start': False, 'vect__max_df': 0.9, 'vect__min_df': 0.0029827045998427337, 'vect__ngram_range': (1, 1), 'vect__stop_words': 'english', 'vect__token_pattern': '\\w{2,}'}, {'clf__base_estimator': Pipeline(memory=None,
         steps=[('vect',
                 TfidfVectorizer(analyzer='word', binary=False,
                                 decode_error='strict',
                                 dtype=<class 'numpy.float64'>,
                                 encoding='utf-8', input='content',
                                 lowercase=True, max_df=0.8555555555555556,
                                 max_features=None,
                                 min_df=0.00016483617040605902,
                                 ngram_range=(1, 2), norm='l2',
                                 preprocessor=None, smooth_idf=True,
                                 stop_words='english', strip_accents...
                                 tokenizer=None, use_idf=True,
                                 vocabulary=None)),
                ('clf',
                 LogisticRegression(C=0.47946121772409134, class_weight=None,
                                    dual=False, fit_intercept=True,
                                    intercept_scaling=1,
                                    l1_ratio=0.9168495407673116, max_iter=75,
                                    multi_class='multinomial', n_jobs=None,
                                    penalty='none', random_state=0,
                                    solver='sag', tol=6.514227791666363e-07,
                                    verbose=0, warm_start=False))],
         verbose=False), 'clf__bootstrap': False, 'clf__bootstrap_features': False, 'clf__max_features': 0.447107103317256, 'clf__max_samples': 0.779694010923419, 'clf__n_estimators': 240, 'clf__oob_score': False, 'clf__warm_start': True, 'vect__max_df': 0.5888888888888889, 'vect__min_df': 0.005738348214456635, 'vect__ngram_range': (1, 2), 'vect__stop_words': 'english', 'vect__token_pattern': '\\w{2,}'}, {'clf__base_estimator': Pipeline(memory=None,
         steps=[('vect',
                 TfidfVectorizer(analyzer='word', binary=False,
                                 decode_error='strict',
                                 dtype=<class 'numpy.float64'>,
                                 encoding='utf-8', input='content',
                                 lowercase=True, max_df=0.7222222222222222,
                                 max_features=None,
                                 min_df=0.00014705642569160768,
                                 ngram_range=(1, 1), norm='l2',
                                 preprocessor=None, smooth_idf=True,
                                 stop_words=None, strip_accents=None...
                                        max_features=0.01, max_leaf_nodes=None,
                                        max_samples=0.007808728196651904,
                                        min_impurity_decrease=3.1622776601683795e-10,
                                        min_impurity_split=None,
                                        min_samples_leaf=62,
                                        min_samples_split=0.0008966659727253451,
                                        min_weight_fraction_leaf=0.0007448459487372297,
                                        n_estimators=251, n_jobs=None,
                                        oob_score=False, random_state=0,
                                        verbose=True, warm_start=True))],
         verbose=False), 'clf__bootstrap': False, 'clf__bootstrap_features': True, 'clf__max_features': 0.9723809986161742, 'clf__max_samples': 0.2891853260660555, 'clf__n_estimators': 264, 'clf__oob_score': False, 'clf__warm_start': True, 'vect__max_df': 0.6777777777777778, 'vect__min_df': 0.0010709985959376131, 'vect__ngram_range': (1, 1), 'vect__stop_words': None, 'vect__token_pattern': '\\w{2,}'}, {'clf__base_estimator': MultinomialNB(alpha=1.0, class_prior=None, fit_prior=True), 'clf__bootstrap': True, 'clf__bootstrap_features': True, 'clf__max_features': 0.049286860290485235, 'clf__max_samples': 0.9289830976379987, 'clf__n_estimators': 15, 'clf__oob_score': True, 'clf__warm_start': True, 'vect__max_df': 0.5888888888888889, 'vect__min_df': 0.006147816198218495, 'vect__ngram_range': (1, 1), 'vect__stop_words': 'english', 'vect__token_pattern': '\\w{2,}'}, {'clf__base_estimator': None, 'clf__bootstrap': True, 'clf__bootstrap_features': True, 'clf__max_features': 0.18810275688821476, 'clf__max_samples': 0.5160259286386163, 'clf__n_estimators': 38, 'clf__oob_score': True, 'clf__warm_start': True, 'vect__max_df': 0.5, 'vect__min_df': 0.0006848767115033545, 'vect__ngram_range': (1, 1), 'vect__stop_words': 'english', 'vect__token_pattern': '\\w{2,}'}, {'clf__base_estimator': Pipeline(memory=None,
         steps=[('vect',
                 TfidfVectorizer(analyzer='word', binary=False,
                                 decode_error='strict',
                                 dtype=<class 'numpy.float64'>,
                                 encoding='utf-8', input='content',
                                 lowercase=True, max_df=0.6333333333333333,
                                 max_features=None,
                                 min_df=0.0010083260374073057,
                                 ngram_range=(1, 1), norm='l2',
                                 preprocessor=None, smooth_idf=True,
                                 stop_words='english', strip_accents=...
                                 tokenizer=None, use_idf=True,
                                 vocabulary=None)),
                ('clf',
                 AdaBoostClassifier(algorithm='SAMME',
                                    base_estimator=LinearSVC(C=1.0,
                                                             class_weight=None,
                                                             dual=True,
                                                             fit_intercept=True,
                                                             intercept_scaling=1,
                                                             loss='squared_hinge',
                                                             max_iter=1000,
                                                             multi_class='ovr',
                                                             penalty='l2',
                                                             random_state=None,
                                                             tol=0.0001,
                                                             verbose=0),
                                    learning_rate=0.40053644159285917,
                                    n_estimators=190, random_state=0))],
         verbose=False), 'clf__bootstrap': True, 'clf__bootstrap_features': False, 'clf__max_features': 0.08096147040951684, 'clf__max_samples': 0.5475849271889746, 'clf__n_estimators': 43, 'clf__oob_score': True, 'clf__warm_start': False, 'vect__max_df': 0.5, 'vect__min_df': 0.00026567793682637046, 'vect__ngram_range': (1, 1), 'vect__stop_words': 'english', 'vect__token_pattern': '\\w{2,}'}, {'clf__base_estimator': None, 'clf__bootstrap': False, 'clf__bootstrap_features': True, 'clf__max_features': 0.8701302701171609, 'clf__max_samples': 0.9608173209766312, 'clf__n_estimators': 372, 'clf__oob_score': True, 'clf__warm_start': False, 'vect__max_df': 0.7222222222222222, 'vect__min_df': 0.0016465959583930765, 'vect__ngram_range': (1, 1), 'vect__stop_words': None, 'vect__token_pattern': '\\w{2,}'}, {'clf__base_estimator': Pipeline(memory=None,
         steps=[('vect',
                 TfidfVectorizer(analyzer='word', binary=False,
                                 decode_error='strict',
                                 dtype=<class 'numpy.float64'>,
                                 encoding='utf-8', input='content',
                                 lowercase=True, max_df=0.6777777777777778,
                                 max_features=None,
                                 min_df=0.0012707570261625706,
                                 ngram_range=(1, 2), norm='l2',
                                 preprocessor=None, smooth_idf=True,
                                 stop_words=None, strip_accents=None,...
                                 vocabulary=None)),
                ('clf',
                 DecisionTreeClassifier(ccp_alpha=0.0, class_weight=None,
                                        criterion='gini', max_depth=49,
                                        max_features=0.56, max_leaf_nodes=316,
                                        min_impurity_decrease=1e-07,
                                        min_impurity_split=None,
                                        min_samples_leaf=4,
                                        min_samples_split=0.015198715678453873,
                                        min_weight_fraction_leaf=0.0,
                                        presort='deprecated', random_state=0,
                                        splitter='random'))],
         verbose=False), 'clf__bootstrap': True, 'clf__bootstrap_features': False, 'clf__max_features': 0.560947528673034, 'clf__max_samples': 0.9009107711343867, 'clf__n_estimators': 306, 'clf__oob_score': False, 'clf__warm_start': False, 'vect__max_df': 0.5888888888888889, 'vect__min_df': 0.0001291782627833387, 'vect__ngram_range': (1, 2), 'vect__stop_words': None, 'vect__token_pattern': '\\w{1,}'}, {'clf__base_estimator': Pipeline(memory=None,
         steps=[('vect',
                 TfidfVectorizer(analyzer='word', binary=False,
                                 decode_error='strict',
                                 dtype=<class 'numpy.float64'>,
                                 encoding='utf-8', input='content',
                                 lowercase=True, max_df=0.7222222222222222,
                                 max_features=None,
                                 min_df=0.00014705642569160768,
                                 ngram_range=(1, 1), norm='l2',
                                 preprocessor=None, smooth_idf=True,
                                 stop_words=None, strip_accents=None...
                                        max_features=0.01, max_leaf_nodes=None,
                                        max_samples=0.007808728196651904,
                                        min_impurity_decrease=3.1622776601683795e-10,
                                        min_impurity_split=None,
                                        min_samples_leaf=62,
                                        min_samples_split=0.0008966659727253451,
                                        min_weight_fraction_leaf=0.0007448459487372297,
                                        n_estimators=251, n_jobs=None,
                                        oob_score=False, random_state=0,
                                        verbose=True, warm_start=True))],
         verbose=False), 'clf__bootstrap': False, 'clf__bootstrap_features': False, 'clf__max_features': 0.940100635503031, 'clf__max_samples': 0.5325217897887943, 'clf__n_estimators': 344, 'clf__oob_score': False, 'clf__warm_start': False, 'vect__max_df': 0.8555555555555556, 'vect__min_df': 0.00989684672758612, 'vect__ngram_range': (1, 2), 'vect__stop_words': None, 'vect__token_pattern': '\\w{1,}'}, {'clf__base_estimator': None, 'clf__bootstrap': False, 'clf__bootstrap_features': False, 'clf__max_features': 0.8355917434025741, 'clf__max_samples': 0.7994091389532858, 'clf__n_estimators': 42, 'clf__oob_score': False, 'clf__warm_start': False, 'vect__max_df': 0.9, 'vect__min_df': 0.00028988551370307105, 'vect__ngram_range': (1, 1), 'vect__stop_words': 'english', 'vect__token_pattern': '\\w{1,}'}, {'clf__base_estimator': Pipeline(memory=None,
         steps=[('vect',
                 TfidfVectorizer(analyzer='word', binary=False,
                                 decode_error='strict',
                                 dtype=<class 'numpy.float64'>,
                                 encoding='utf-8', input='content',
                                 lowercase=True, max_df=0.6333333333333333,
                                 max_features=None,
                                 min_df=0.0010083260374073057,
                                 ngram_range=(1, 1), norm='l2',
                                 preprocessor=None, smooth_idf=True,
                                 stop_words='english', strip_accents=...
                                 tokenizer=None, use_idf=True,
                                 vocabulary=None)),
                ('clf',
                 AdaBoostClassifier(algorithm='SAMME',
                                    base_estimator=LinearSVC(C=1.0,
                                                             class_weight=None,
                                                             dual=True,
                                                             fit_intercept=True,
                                                             intercept_scaling=1,
                                                             loss='squared_hinge',
                                                             max_iter=1000,
                                                             multi_class='ovr',
                                                             penalty='l2',
                                                             random_state=None,
                                                             tol=0.0001,
                                                             verbose=0),
                                    learning_rate=0.40053644159285917,
                                    n_estimators=190, random_state=0))],
         verbose=False), 'clf__bootstrap': False, 'clf__bootstrap_features': True, 'clf__max_features': 0.7116185081049804, 'clf__max_samples': 0.36896434791804444, 'clf__n_estimators': 203, 'clf__oob_score': True, 'clf__warm_start': False, 'vect__max_df': 0.6333333333333333, 'vect__min_df': 0.00011126768996562946, 'vect__ngram_range': (1, 1), 'vect__stop_words': 'english', 'vect__token_pattern': '\\w{2,}'}, {'clf__base_estimator': MultinomialNB(alpha=1.0, class_prior=None, fit_prior=True), 'clf__bootstrap': False, 'clf__bootstrap_features': False, 'clf__max_features': 0.5076258896806638, 'clf__max_samples': 0.8166343061196542, 'clf__n_estimators': 121, 'clf__oob_score': True, 'clf__warm_start': False, 'vect__max_df': 0.7222222222222222, 'vect__min_df': 0.00042943981412846646, 'vect__ngram_range': (1, 2), 'vect__stop_words': None, 'vect__token_pattern': '\\w{2,}'}, {'clf__base_estimator': Pipeline(memory=None,
         steps=[('vect',
                 TfidfVectorizer(analyzer='word', binary=False,
                                 decode_error='strict',
                                 dtype=<class 'numpy.float64'>,
                                 encoding='utf-8', input='content',
                                 lowercase=True, max_df=0.8555555555555556,
                                 max_features=None,
                                 min_df=0.00016483617040605902,
                                 ngram_range=(1, 2), norm='l2',
                                 preprocessor=None, smooth_idf=True,
                                 stop_words='english', strip_accents...
                                 tokenizer=None, use_idf=True,
                                 vocabulary=None)),
                ('clf',
                 LogisticRegression(C=0.47946121772409134, class_weight=None,
                                    dual=False, fit_intercept=True,
                                    intercept_scaling=1,
                                    l1_ratio=0.9168495407673116, max_iter=75,
                                    multi_class='multinomial', n_jobs=None,
                                    penalty='none', random_state=0,
                                    solver='sag', tol=6.514227791666363e-07,
                                    verbose=0, warm_start=False))],
         verbose=False), 'clf__bootstrap': True, 'clf__bootstrap_features': True, 'clf__max_features': 0.25496068933087734, 'clf__max_samples': 0.9577830686880459, 'clf__n_estimators': 232, 'clf__oob_score': False, 'clf__warm_start': True, 'vect__max_df': 0.5444444444444444, 'vect__min_df': 0.007683120333114825, 'vect__ngram_range': (1, 2), 'vect__stop_words': 'english', 'vect__token_pattern': '\\w{1,}'}, {'clf__base_estimator': Pipeline(memory=None,
         steps=[('vect',
                 TfidfVectorizer(analyzer='word', binary=False,
                                 decode_error='strict',
                                 dtype=<class 'numpy.float64'>,
                                 encoding='utf-8', input='content',
                                 lowercase=True, max_df=0.6777777777777778,
                                 max_features=None,
                                 min_df=0.0012707570261625706,
                                 ngram_range=(1, 2), norm='l2',
                                 preprocessor=None, smooth_idf=True,
                                 stop_words=None, strip_accents=None,...
                                 vocabulary=None)),
                ('clf',
                 DecisionTreeClassifier(ccp_alpha=0.0, class_weight=None,
                                        criterion='gini', max_depth=49,
                                        max_features=0.56, max_leaf_nodes=316,
                                        min_impurity_decrease=1e-07,
                                        min_impurity_split=None,
                                        min_samples_leaf=4,
                                        min_samples_split=0.015198715678453873,
                                        min_weight_fraction_leaf=0.0,
                                        presort='deprecated', random_state=0,
                                        splitter='random'))],
         verbose=False), 'clf__bootstrap': False, 'clf__bootstrap_features': False, 'clf__max_features': 0.9293175369230119, 'clf__max_samples': 0.6850037078777709, 'clf__n_estimators': 132, 'clf__oob_score': True, 'clf__warm_start': False, 'vect__max_df': 0.5, 'vect__min_df': 0.0076293976855326025, 'vect__ngram_range': (1, 1), 'vect__stop_words': 'english', 'vect__token_pattern': '\\w{1,}'}, {'clf__base_estimator': Pipeline(memory=None,
         steps=[('vect',
                 TfidfVectorizer(analyzer='word', binary=False,
                                 decode_error='strict',
                                 dtype=<class 'numpy.float64'>,
                                 encoding='utf-8', input='content',
                                 lowercase=True, max_df=0.7222222222222222,
                                 max_features=None,
                                 min_df=0.00014705642569160768,
                                 ngram_range=(1, 1), norm='l2',
                                 preprocessor=None, smooth_idf=True,
                                 stop_words=None, strip_accents=None...
                                        max_features=0.01, max_leaf_nodes=None,
                                        max_samples=0.007808728196651904,
                                        min_impurity_decrease=3.1622776601683795e-10,
                                        min_impurity_split=None,
                                        min_samples_leaf=62,
                                        min_samples_split=0.0008966659727253451,
                                        min_weight_fraction_leaf=0.0007448459487372297,
                                        n_estimators=251, n_jobs=None,
                                        oob_score=False, random_state=0,
                                        verbose=True, warm_start=True))],
         verbose=False), 'clf__bootstrap': True, 'clf__bootstrap_features': False, 'clf__max_features': 0.47414643880097385, 'clf__max_samples': 0.9238676540591991, 'clf__n_estimators': 342, 'clf__oob_score': True, 'clf__warm_start': True, 'vect__max_df': 0.7222222222222222, 'vect__min_df': 0.0002753203215764797, 'vect__ngram_range': (1, 2), 'vect__stop_words': 'english', 'vect__token_pattern': '\\w{1,}'}, {'clf__base_estimator': MultinomialNB(alpha=1.0, class_prior=None, fit_prior=True), 'clf__bootstrap': True, 'clf__bootstrap_features': False, 'clf__max_features': 0.8818974495685915, 'clf__max_samples': 0.634404928538508, 'clf__n_estimators': 327, 'clf__oob_score': True, 'clf__warm_start': False, 'vect__max_df': 0.6777777777777778, 'vect__min_df': 0.0031498664850481417, 'vect__ngram_range': (1, 1), 'vect__stop_words': 'english', 'vect__token_pattern': '\\w{1,}'}, {'clf__base_estimator': Pipeline(memory=None,
         steps=[('vect',
                 TfidfVectorizer(analyzer='word', binary=False,
                                 decode_error='strict',
                                 dtype=<class 'numpy.float64'>,
                                 encoding='utf-8', input='content',
                                 lowercase=True, max_df=0.8555555555555556,
                                 max_features=None,
                                 min_df=0.00016483617040605902,
                                 ngram_range=(1, 2), norm='l2',
                                 preprocessor=None, smooth_idf=True,
                                 stop_words='english', strip_accents...
                                 tokenizer=None, use_idf=True,
                                 vocabulary=None)),
                ('clf',
                 LogisticRegression(C=0.47946121772409134, class_weight=None,
                                    dual=False, fit_intercept=True,
                                    intercept_scaling=1,
                                    l1_ratio=0.9168495407673116, max_iter=75,
                                    multi_class='multinomial', n_jobs=None,
                                    penalty='none', random_state=0,
                                    solver='sag', tol=6.514227791666363e-07,
                                    verbose=0, warm_start=False))],
         verbose=False), 'clf__bootstrap': False, 'clf__bootstrap_features': True, 'clf__max_features': 0.3424883506569144, 'clf__max_samples': 0.4639065009028883, 'clf__n_estimators': 307, 'clf__oob_score': False, 'clf__warm_start': True, 'vect__max_df': 0.9, 'vect__min_df': 0.001114695738465579, 'vect__ngram_range': (1, 1), 'vect__stop_words': None, 'vect__token_pattern': '\\w{1,}'}, {'clf__base_estimator': MultinomialNB(alpha=1.0, class_prior=None, fit_prior=True), 'clf__bootstrap': False, 'clf__bootstrap_features': True, 'clf__max_features': 0.5194764278959346, 'clf__max_samples': 0.9564303427545765, 'clf__n_estimators': 325, 'clf__oob_score': True, 'clf__warm_start': True, 'vect__max_df': 0.5, 'vect__min_df': 0.008083209625476819, 'vect__ngram_range': (1, 1), 'vect__stop_words': None, 'vect__token_pattern': '\\w{2,}'}, {'clf__base_estimator': Pipeline(memory=None,
         steps=[('vect',
                 TfidfVectorizer(analyzer='word', binary=False,
                                 decode_error='strict',
                                 dtype=<class 'numpy.float64'>,
                                 encoding='utf-8', input='content',
                                 lowercase=True, max_df=0.6333333333333333,
                                 max_features=None,
                                 min_df=0.0010083260374073057,
                                 ngram_range=(1, 1), norm='l2',
                                 preprocessor=None, smooth_idf=True,
                                 stop_words='english', strip_accents=...
                                 tokenizer=None, use_idf=True,
                                 vocabulary=None)),
                ('clf',
                 AdaBoostClassifier(algorithm='SAMME',
                                    base_estimator=LinearSVC(C=1.0,
                                                             class_weight=None,
                                                             dual=True,
                                                             fit_intercept=True,
                                                             intercept_scaling=1,
                                                             loss='squared_hinge',
                                                             max_iter=1000,
                                                             multi_class='ovr',
                                                             penalty='l2',
                                                             random_state=None,
                                                             tol=0.0001,
                                                             verbose=0),
                                    learning_rate=0.40053644159285917,
                                    n_estimators=190, random_state=0))],
         verbose=False), 'clf__bootstrap': False, 'clf__bootstrap_features': True, 'clf__max_features': 0.11499687169032813, 'clf__max_samples': 0.7180155333841137, 'clf__n_estimators': 52, 'clf__oob_score': False, 'clf__warm_start': False, 'vect__max_df': 0.8555555555555556, 'vect__min_df': 0.0016247695336201564, 'vect__ngram_range': (1, 2), 'vect__stop_words': 'english', 'vect__token_pattern': '\\w{1,}'}, {'clf__base_estimator': MultinomialNB(alpha=1.0, class_prior=None, fit_prior=True), 'clf__bootstrap': True, 'clf__bootstrap_features': True, 'clf__max_features': 0.9782128223612612, 'clf__max_samples': 0.9577613868555912, 'clf__n_estimators': 369, 'clf__oob_score': True, 'clf__warm_start': False, 'vect__max_df': 0.7666666666666666, 'vect__min_df': 0.0043253901386184485, 'vect__ngram_range': (1, 2), 'vect__stop_words': None, 'vect__token_pattern': '\\w{2,}'}, {'clf__base_estimator': None, 'clf__bootstrap': False, 'clf__bootstrap_features': True, 'clf__max_features': 0.5654622682897744, 'clf__max_samples': 0.1230210707248105, 'clf__n_estimators': 164, 'clf__oob_score': True, 'clf__warm_start': True, 'vect__max_df': 0.6777777777777778, 'vect__min_df': 0.0002207695978457939, 'vect__ngram_range': (1, 1), 'vect__stop_words': 'english', 'vect__token_pattern': '\\w{1,}'}, {'clf__base_estimator': MultinomialNB(alpha=1.0, class_prior=None, fit_prior=True), 'clf__bootstrap': False, 'clf__bootstrap_features': True, 'clf__max_features': 0.054473449691950004, 'clf__max_samples': 0.7501543774411317, 'clf__n_estimators': 177, 'clf__oob_score': True, 'clf__warm_start': True, 'vect__max_df': 0.9, 'vect__min_df': 0.0001826073778712324, 'vect__ngram_range': (1, 2), 'vect__stop_words': 'english', 'vect__token_pattern': '\\w{1,}'}, {'clf__base_estimator': Pipeline(memory=None,
         steps=[('vect',
                 TfidfVectorizer(analyzer='word', binary=False,
                                 decode_error='strict',
                                 dtype=<class 'numpy.float64'>,
                                 encoding='utf-8', input='content',
                                 lowercase=True, max_df=0.7222222222222222,
                                 max_features=None,
                                 min_df=0.00014705642569160768,
                                 ngram_range=(1, 1), norm='l2',
                                 preprocessor=None, smooth_idf=True,
                                 stop_words=None, strip_accents=None...
                                        max_features=0.01, max_leaf_nodes=None,
                                        max_samples=0.007808728196651904,
                                        min_impurity_decrease=3.1622776601683795e-10,
                                        min_impurity_split=None,
                                        min_samples_leaf=62,
                                        min_samples_split=0.0008966659727253451,
                                        min_weight_fraction_leaf=0.0007448459487372297,
                                        n_estimators=251, n_jobs=None,
                                        oob_score=False, random_state=0,
                                        verbose=True, warm_start=True))],
         verbose=False), 'clf__bootstrap': True, 'clf__bootstrap_features': False, 'clf__max_features': 0.5218093408741838, 'clf__max_samples': 0.4031891535290265, 'clf__n_estimators': 314, 'clf__oob_score': False, 'clf__warm_start': False, 'vect__max_df': 0.5444444444444444, 'vect__min_df': 0.0006683588665809419, 'vect__ngram_range': (1, 2), 'vect__stop_words': 'english', 'vect__token_pattern': '\\w{2,}'}, {'clf__base_estimator': MultinomialNB(alpha=1.0, class_prior=None, fit_prior=True), 'clf__bootstrap': False, 'clf__bootstrap_features': True, 'clf__max_features': 0.2234393375689152, 'clf__max_samples': 0.23411411928859738, 'clf__n_estimators': 22, 'clf__oob_score': False, 'clf__warm_start': False, 'vect__max_df': 0.5888888888888889, 'vect__min_df': 0.0001801829439669374, 'vect__ngram_range': (1, 1), 'vect__stop_words': 'english', 'vect__token_pattern': '\\w{1,}'}, {'clf__base_estimator': Pipeline(memory=None,
         steps=[('vect',
                 TfidfVectorizer(analyzer='word', binary=False,
                                 decode_error='strict',
                                 dtype=<class 'numpy.float64'>,
                                 encoding='utf-8', input='content',
                                 lowercase=True, max_df=0.7666666666666666,
                                 max_features=None,
                                 min_df=0.00010572508827796376,
                                 ngram_range=(1, 2), norm='l2',
                                 preprocessor=None, smooth_idf=True,
                                 stop_words='english', strip_accents=None,
                                 sublinear_tf=False, token_pattern='\\w{1,}',
                                 tokenizer=None, use_idf=True,
                                 vocabulary=None)),
                ('clf',
                 LinearSVC(C=99.37702864845689, class_weight=None, dual=False,
                           fit_intercept=False, intercept_scaling=1,
                           loss='squared_hinge', max_iter=1000,
                           multi_class='ovr', penalty='l2', random_state=0,
                           tol=3.8359242531998225e-06, verbose=0))],
         verbose=False), 'clf__bootstrap': True, 'clf__bootstrap_features': False, 'clf__max_features': 0.49753536407653187, 'clf__max_samples': 0.35598884371772344, 'clf__n_estimators': 396, 'clf__oob_score': True, 'clf__warm_start': False, 'vect__max_df': 0.8555555555555556, 'vect__min_df': 0.005607715332818271, 'vect__ngram_range': (1, 2), 'vect__stop_words': 'english', 'vect__token_pattern': '\\w{2,}'}, {'clf__base_estimator': Pipeline(memory=None,
         steps=[('vect',
                 TfidfVectorizer(analyzer='word', binary=False,
                                 decode_error='strict',
                                 dtype=<class 'numpy.float64'>,
                                 encoding='utf-8', input='content',
                                 lowercase=True, max_df=0.7666666666666666,
                                 max_features=None,
                                 min_df=0.00010572508827796376,
                                 ngram_range=(1, 2), norm='l2',
                                 preprocessor=None, smooth_idf=True,
                                 stop_words='english', strip_accents=None,
                                 sublinear_tf=False, token_pattern='\\w{1,}',
                                 tokenizer=None, use_idf=True,
                                 vocabulary=None)),
                ('clf',
                 LinearSVC(C=99.37702864845689, class_weight=None, dual=False,
                           fit_intercept=False, intercept_scaling=1,
                           loss='squared_hinge', max_iter=1000,
                           multi_class='ovr', penalty='l2', random_state=0,
                           tol=3.8359242531998225e-06, verbose=0))],
         verbose=False), 'clf__bootstrap': False, 'clf__bootstrap_features': False, 'clf__max_features': 0.40839279404424256, 'clf__max_samples': 0.0787144659548229, 'clf__n_estimators': 114, 'clf__oob_score': True, 'clf__warm_start': False, 'vect__max_df': 0.5444444444444444, 'vect__min_df': 0.0010298144292754932, 'vect__ngram_range': (1, 1), 'vect__stop_words': None, 'vect__token_pattern': '\\w{2,}'}, {'clf__base_estimator': MultinomialNB(alpha=1.0, class_prior=None, fit_prior=True), 'clf__bootstrap': False, 'clf__bootstrap_features': False, 'clf__max_features': 0.7780680470435732, 'clf__max_samples': 0.37774934648073233, 'clf__n_estimators': 51, 'clf__oob_score': False, 'clf__warm_start': True, 'vect__max_df': 0.6777777777777778, 'vect__min_df': 0.002278032055081042, 'vect__ngram_range': (1, 1), 'vect__stop_words': 'english', 'vect__token_pattern': '\\w{1,}'}, {'clf__base_estimator': Pipeline(memory=None,
         steps=[('vect',
                 TfidfVectorizer(analyzer='word', binary=False,
                                 decode_error='strict',
                                 dtype=<class 'numpy.float64'>,
                                 encoding='utf-8', input='content',
                                 lowercase=True, max_df=0.7666666666666666,
                                 max_features=None,
                                 min_df=0.00010572508827796376,
                                 ngram_range=(1, 2), norm='l2',
                                 preprocessor=None, smooth_idf=True,
                                 stop_words='english', strip_accents=None,
                                 sublinear_tf=False, token_pattern='\\w{1,}',
                                 tokenizer=None, use_idf=True,
                                 vocabulary=None)),
                ('clf',
                 LinearSVC(C=99.37702864845689, class_weight=None, dual=False,
                           fit_intercept=False, intercept_scaling=1,
                           loss='squared_hinge', max_iter=1000,
                           multi_class='ovr', penalty='l2', random_state=0,
                           tol=3.8359242531998225e-06, verbose=0))],
         verbose=False), 'clf__bootstrap': False, 'clf__bootstrap_features': False, 'clf__max_features': 0.6064518812101772, 'clf__max_samples': 0.43757160838575504, 'clf__n_estimators': 183, 'clf__oob_score': False, 'clf__warm_start': True, 'vect__max_df': 0.5, 'vect__min_df': 0.00042708290951816094, 'vect__ngram_range': (1, 2), 'vect__stop_words': 'english', 'vect__token_pattern': '\\w{1,}'}, {'clf__base_estimator': None, 'clf__bootstrap': True, 'clf__bootstrap_features': True, 'clf__max_features': 0.3429916807127845, 'clf__max_samples': 0.08253199552734625, 'clf__n_estimators': 336, 'clf__oob_score': True, 'clf__warm_start': False, 'vect__max_df': 0.8111111111111111, 'vect__min_df': 0.0016037351754642133, 'vect__ngram_range': (1, 2), 'vect__stop_words': None, 'vect__token_pattern': '\\w{1,}'}, {'clf__base_estimator': Pipeline(memory=None,
         steps=[('vect',
                 TfidfVectorizer(analyzer='word', binary=False,
                                 decode_error='strict',
                                 dtype=<class 'numpy.float64'>,
                                 encoding='utf-8', input='content',
                                 lowercase=True, max_df=0.7666666666666666,
                                 max_features=None,
                                 min_df=0.00010572508827796376,
                                 ngram_range=(1, 2), norm='l2',
                                 preprocessor=None, smooth_idf=True,
                                 stop_words='english', strip_accents=None,
                                 sublinear_tf=False, token_pattern='\\w{1,}',
                                 tokenizer=None, use_idf=True,
                                 vocabulary=None)),
                ('clf',
                 LinearSVC(C=99.37702864845689, class_weight=None, dual=False,
                           fit_intercept=False, intercept_scaling=1,
                           loss='squared_hinge', max_iter=1000,
                           multi_class='ovr', penalty='l2', random_state=0,
                           tol=3.8359242531998225e-06, verbose=0))],
         verbose=False), 'clf__bootstrap': True, 'clf__bootstrap_features': False, 'clf__max_features': 0.31232291817990376, 'clf__max_samples': 0.43190422151338614, 'clf__n_estimators': 356, 'clf__oob_score': False, 'clf__warm_start': False, 'vect__max_df': 0.7666666666666666, 'vect__min_df': 0.0014637209476171746, 'vect__ngram_range': (1, 2), 'vect__stop_words': 'english', 'vect__token_pattern': '\\w{1,}'}, {'clf__base_estimator': MultinomialNB(alpha=1.0, class_prior=None, fit_prior=True), 'clf__bootstrap': False, 'clf__bootstrap_features': True, 'clf__max_features': 0.10666852510585423, 'clf__max_samples': 0.27495234241576205, 'clf__n_estimators': 58, 'clf__oob_score': True, 'clf__warm_start': False, 'vect__max_df': 0.9, 'vect__min_df': 0.0013054375511533823, 'vect__ngram_range': (1, 1), 'vect__stop_words': 'english', 'vect__token_pattern': '\\w{2,}'}, {'clf__base_estimator': Pipeline(memory=None,
         steps=[('vect',
                 TfidfVectorizer(analyzer='word', binary=False,
                                 decode_error='strict',
                                 dtype=<class 'numpy.float64'>,
                                 encoding='utf-8', input='content',
                                 lowercase=True, max_df=0.7666666666666666,
                                 max_features=None,
                                 min_df=0.00010572508827796376,
                                 ngram_range=(1, 2), norm='l2',
                                 preprocessor=None, smooth_idf=True,
                                 stop_words='english', strip_accents=None,
                                 sublinear_tf=False, token_pattern='\\w{1,}',
                                 tokenizer=None, use_idf=True,
                                 vocabulary=None)),
                ('clf',
                 LinearSVC(C=99.37702864845689, class_weight=None, dual=False,
                           fit_intercept=False, intercept_scaling=1,
                           loss='squared_hinge', max_iter=1000,
                           multi_class='ovr', penalty='l2', random_state=0,
                           tol=3.8359242531998225e-06, verbose=0))],
         verbose=False), 'clf__bootstrap': False, 'clf__bootstrap_features': False, 'clf__max_features': 0.053395335722344894, 'clf__max_samples': 0.6690228827883734, 'clf__n_estimators': 164, 'clf__oob_score': False, 'clf__warm_start': True, 'vect__max_df': 0.8111111111111111, 'vect__min_df': 0.0003408411815757151, 'vect__ngram_range': (1, 2), 'vect__stop_words': 'english', 'vect__token_pattern': '\\w{2,}'}, {'clf__base_estimator': Pipeline(memory=None,
         steps=[('vect',
                 TfidfVectorizer(analyzer='word', binary=False,
                                 decode_error='strict',
                                 dtype=<class 'numpy.float64'>,
                                 encoding='utf-8', input='content',
                                 lowercase=True, max_df=0.6777777777777778,
                                 max_features=None,
                                 min_df=0.0012707570261625706,
                                 ngram_range=(1, 2), norm='l2',
                                 preprocessor=None, smooth_idf=True,
                                 stop_words=None, strip_accents=None,...
                                 vocabulary=None)),
                ('clf',
                 DecisionTreeClassifier(ccp_alpha=0.0, class_weight=None,
                                        criterion='gini', max_depth=49,
                                        max_features=0.56, max_leaf_nodes=316,
                                        min_impurity_decrease=1e-07,
                                        min_impurity_split=None,
                                        min_samples_leaf=4,
                                        min_samples_split=0.015198715678453873,
                                        min_weight_fraction_leaf=0.0,
                                        presort='deprecated', random_state=0,
                                        splitter='random'))],
         verbose=False), 'clf__bootstrap': True, 'clf__bootstrap_features': False, 'clf__max_features': 0.7740715959711809, 'clf__max_samples': 0.3002715380483616, 'clf__n_estimators': 55, 'clf__oob_score': False, 'clf__warm_start': True, 'vect__max_df': 0.5888888888888889, 'vect__min_df': 0.00038759450399179264, 'vect__ngram_range': (1, 2), 'vect__stop_words': 'english', 'vect__token_pattern': '\\w{2,}'}, {'clf__base_estimator': Pipeline(memory=None,
         steps=[('vect',
                 TfidfVectorizer(analyzer='word', binary=False,
                                 decode_error='strict',
                                 dtype=<class 'numpy.float64'>,
                                 encoding='utf-8', input='content',
                                 lowercase=True, max_df=0.7666666666666666,
                                 max_features=None,
                                 min_df=0.00010572508827796376,
                                 ngram_range=(1, 2), norm='l2',
                                 preprocessor=None, smooth_idf=True,
                                 stop_words='english', strip_accents=None,
                                 sublinear_tf=False, token_pattern='\\w{1,}',
                                 tokenizer=None, use_idf=True,
                                 vocabulary=None)),
                ('clf',
                 LinearSVC(C=99.37702864845689, class_weight=None, dual=False,
                           fit_intercept=False, intercept_scaling=1,
                           loss='squared_hinge', max_iter=1000,
                           multi_class='ovr', penalty='l2', random_state=0,
                           tol=3.8359242531998225e-06, verbose=0))],
         verbose=False), 'clf__bootstrap': True, 'clf__bootstrap_features': True, 'clf__max_features': 0.6957299722211582, 'clf__max_samples': 0.32029010770056976, 'clf__n_estimators': 135, 'clf__oob_score': True, 'clf__warm_start': True, 'vect__max_df': 0.5, 'vect__min_df': 0.0003973804491633026, 'vect__ngram_range': (1, 1), 'vect__stop_words': None, 'vect__token_pattern': '\\w{2,}'}, {'clf__base_estimator': Pipeline(memory=None,
         steps=[('vect',
                 TfidfVectorizer(analyzer='word', binary=False,
                                 decode_error='strict',
                                 dtype=<class 'numpy.float64'>,
                                 encoding='utf-8', input='content',
                                 lowercase=True, max_df=0.7666666666666666,
                                 max_features=None,
                                 min_df=0.00010572508827796376,
                                 ngram_range=(1, 2), norm='l2',
                                 preprocessor=None, smooth_idf=True,
                                 stop_words='english', strip_accents=None,
                                 sublinear_tf=False, token_pattern='\\w{1,}',
                                 tokenizer=None, use_idf=True,
                                 vocabulary=None)),
                ('clf',
                 LinearSVC(C=99.37702864845689, class_weight=None, dual=False,
                           fit_intercept=False, intercept_scaling=1,
                           loss='squared_hinge', max_iter=1000,
                           multi_class='ovr', penalty='l2', random_state=0,
                           tol=3.8359242531998225e-06, verbose=0))],
         verbose=False), 'clf__bootstrap': True, 'clf__bootstrap_features': True, 'clf__max_features': 0.12315876133004233, 'clf__max_samples': 0.7420284747619605, 'clf__n_estimators': 399, 'clf__oob_score': True, 'clf__warm_start': True, 'vect__max_df': 0.6333333333333333, 'vect__min_df': 0.00018024024198573567, 'vect__ngram_range': (1, 2), 'vect__stop_words': None, 'vect__token_pattern': '\\w{1,}'}, {'clf__base_estimator': MultinomialNB(alpha=1.0, class_prior=None, fit_prior=True), 'clf__bootstrap': False, 'clf__bootstrap_features': False, 'clf__max_features': 0.6071894635555929, 'clf__max_samples': 0.632082096196872, 'clf__n_estimators': 388, 'clf__oob_score': False, 'clf__warm_start': True, 'vect__max_df': 0.7222222222222222, 'vect__min_df': 0.0004678219292177023, 'vect__ngram_range': (1, 2), 'vect__stop_words': 'english', 'vect__token_pattern': '\\w{2,}'}, {'clf__base_estimator': None, 'clf__bootstrap': False, 'clf__bootstrap_features': True, 'clf__max_features': 0.851390828998649, 'clf__max_samples': 0.3054621284374154, 'clf__n_estimators': 31, 'clf__oob_score': True, 'clf__warm_start': False, 'vect__max_df': 0.8555555555555556, 'vect__min_df': 0.0008076504222293942, 'vect__ngram_range': (1, 2), 'vect__stop_words': 'english', 'vect__token_pattern': '\\w{1,}'}, {'clf__base_estimator': None, 'clf__bootstrap': False, 'clf__bootstrap_features': False, 'clf__max_features': 0.5455645453535705, 'clf__max_samples': 0.9266099422801278, 'clf__n_estimators': 368, 'clf__oob_score': True, 'clf__warm_start': False, 'vect__max_df': 0.7222222222222222, 'vect__min_df': 0.00627687027381037, 'vect__ngram_range': (1, 2), 'vect__stop_words': None, 'vect__token_pattern': '\\w{2,}'}, {'clf__base_estimator': MultinomialNB(alpha=1.0, class_prior=None, fit_prior=True), 'clf__bootstrap': False, 'clf__bootstrap_features': False, 'clf__max_features': 0.5724143997436254, 'clf__max_samples': 0.32967129045097576, 'clf__n_estimators': 223, 'clf__oob_score': False, 'clf__warm_start': False, 'vect__max_df': 0.8555555555555556, 'vect__min_df': 0.0009098737101808315, 'vect__ngram_range': (1, 2), 'vect__stop_words': 'english', 'vect__token_pattern': '\\w{2,}'}, {'clf__base_estimator': Pipeline(memory=None,
         steps=[('vect',
                 TfidfVectorizer(analyzer='word', binary=False,
                                 decode_error='strict',
                                 dtype=<class 'numpy.float64'>,
                                 encoding='utf-8', input='content',
                                 lowercase=True, max_df=0.8555555555555556,
                                 max_features=None,
                                 min_df=0.00016483617040605902,
                                 ngram_range=(1, 2), norm='l2',
                                 preprocessor=None, smooth_idf=True,
                                 stop_words='english', strip_accents...
                                 tokenizer=None, use_idf=True,
                                 vocabulary=None)),
                ('clf',
                 LogisticRegression(C=0.47946121772409134, class_weight=None,
                                    dual=False, fit_intercept=True,
                                    intercept_scaling=1,
                                    l1_ratio=0.9168495407673116, max_iter=75,
                                    multi_class='multinomial', n_jobs=None,
                                    penalty='none', random_state=0,
                                    solver='sag', tol=6.514227791666363e-07,
                                    verbose=0, warm_start=False))],
         verbose=False), 'clf__bootstrap': False, 'clf__bootstrap_features': True, 'clf__max_features': 0.9419918734182068, 'clf__max_samples': 0.4048020821937648, 'clf__n_estimators': 348, 'clf__oob_score': False, 'clf__warm_start': True, 'vect__max_df': 0.5888888888888889, 'vect__min_df': 0.00010058370526151803, 'vect__ngram_range': (1, 2), 'vect__stop_words': 'english', 'vect__token_pattern': '\\w{1,}'}, {'clf__base_estimator': Pipeline(memory=None,
         steps=[('vect',
                 TfidfVectorizer(analyzer='word', binary=False,
                                 decode_error='strict',
                                 dtype=<class 'numpy.float64'>,
                                 encoding='utf-8', input='content',
                                 lowercase=True, max_df=0.7666666666666666,
                                 max_features=None,
                                 min_df=0.00010572508827796376,
                                 ngram_range=(1, 2), norm='l2',
                                 preprocessor=None, smooth_idf=True,
                                 stop_words='english', strip_accents=None,
                                 sublinear_tf=False, token_pattern='\\w{1,}',
                                 tokenizer=None, use_idf=True,
                                 vocabulary=None)),
                ('clf',
                 LinearSVC(C=99.37702864845689, class_weight=None, dual=False,
                           fit_intercept=False, intercept_scaling=1,
                           loss='squared_hinge', max_iter=1000,
                           multi_class='ovr', penalty='l2', random_state=0,
                           tol=3.8359242531998225e-06, verbose=0))],
         verbose=False), 'clf__bootstrap': True, 'clf__bootstrap_features': True, 'clf__max_features': 0.5569099301284081, 'clf__max_samples': 0.572056115291836, 'clf__n_estimators': 296, 'clf__oob_score': False, 'clf__warm_start': False, 'vect__max_df': 0.9, 'vect__min_df': 0.00275719602829668, 'vect__ngram_range': (1, 2), 'vect__stop_words': None, 'vect__token_pattern': '\\w{2,}'}, {'clf__base_estimator': Pipeline(memory=None,
         steps=[('vect',
                 TfidfVectorizer(analyzer='word', binary=False,
                                 decode_error='strict',
                                 dtype=<class 'numpy.float64'>,
                                 encoding='utf-8', input='content',
                                 lowercase=True, max_df=0.6333333333333333,
                                 max_features=None,
                                 min_df=0.0010083260374073057,
                                 ngram_range=(1, 1), norm='l2',
                                 preprocessor=None, smooth_idf=True,
                                 stop_words='english', strip_accents=...
                                 tokenizer=None, use_idf=True,
                                 vocabulary=None)),
                ('clf',
                 AdaBoostClassifier(algorithm='SAMME',
                                    base_estimator=LinearSVC(C=1.0,
                                                             class_weight=None,
                                                             dual=True,
                                                             fit_intercept=True,
                                                             intercept_scaling=1,
                                                             loss='squared_hinge',
                                                             max_iter=1000,
                                                             multi_class='ovr',
                                                             penalty='l2',
                                                             random_state=None,
                                                             tol=0.0001,
                                                             verbose=0),
                                    learning_rate=0.40053644159285917,
                                    n_estimators=190, random_state=0))],
         verbose=False), 'clf__bootstrap': True, 'clf__bootstrap_features': False, 'clf__max_features': 0.5271951341226369, 'clf__max_samples': 0.021974094763683283, 'clf__n_estimators': 15, 'clf__oob_score': True, 'clf__warm_start': True, 'vect__max_df': 0.5, 'vect__min_df': 0.00036729757885476164, 'vect__ngram_range': (1, 1), 'vect__stop_words': 'english', 'vect__token_pattern': '\\w{2,}'}, {'clf__base_estimator': Pipeline(memory=None,
         steps=[('vect',
                 TfidfVectorizer(analyzer='word', binary=False,
                                 decode_error='strict',
                                 dtype=<class 'numpy.float64'>,
                                 encoding='utf-8', input='content',
                                 lowercase=True, max_df=0.8555555555555556,
                                 max_features=None,
                                 min_df=0.00016483617040605902,
                                 ngram_range=(1, 2), norm='l2',
                                 preprocessor=None, smooth_idf=True,
                                 stop_words='english', strip_accents...
                                 tokenizer=None, use_idf=True,
                                 vocabulary=None)),
                ('clf',
                 LogisticRegression(C=0.47946121772409134, class_weight=None,
                                    dual=False, fit_intercept=True,
                                    intercept_scaling=1,
                                    l1_ratio=0.9168495407673116, max_iter=75,
                                    multi_class='multinomial', n_jobs=None,
                                    penalty='none', random_state=0,
                                    solver='sag', tol=6.514227791666363e-07,
                                    verbose=0, warm_start=False))],
         verbose=False), 'clf__bootstrap': True, 'clf__bootstrap_features': False, 'clf__max_features': 0.7085529287479299, 'clf__max_samples': 0.4322614741936128, 'clf__n_estimators': 253, 'clf__oob_score': True, 'clf__warm_start': False, 'vect__max_df': 0.7222222222222222, 'vect__min_df': 0.0002565244324090726, 'vect__ngram_range': (1, 2), 'vect__stop_words': 'english', 'vect__token_pattern': '\\w{2,}'}, {'clf__base_estimator': Pipeline(memory=None,
         steps=[('vect',
                 TfidfVectorizer(analyzer='word', binary=False,
                                 decode_error='strict',
                                 dtype=<class 'numpy.float64'>,
                                 encoding='utf-8', input='content',
                                 lowercase=True, max_df=0.6777777777777778,
                                 max_features=None,
                                 min_df=0.0012707570261625706,
                                 ngram_range=(1, 2), norm='l2',
                                 preprocessor=None, smooth_idf=True,
                                 stop_words=None, strip_accents=None,...
                                 vocabulary=None)),
                ('clf',
                 DecisionTreeClassifier(ccp_alpha=0.0, class_weight=None,
                                        criterion='gini', max_depth=49,
                                        max_features=0.56, max_leaf_nodes=316,
                                        min_impurity_decrease=1e-07,
                                        min_impurity_split=None,
                                        min_samples_leaf=4,
                                        min_samples_split=0.015198715678453873,
                                        min_weight_fraction_leaf=0.0,
                                        presort='deprecated', random_state=0,
                                        splitter='random'))],
         verbose=False), 'clf__bootstrap': True, 'clf__bootstrap_features': False, 'clf__max_features': 0.34708335091212694, 'clf__max_samples': 0.4306742289733726, 'clf__n_estimators': 55, 'clf__oob_score': True, 'clf__warm_start': False, 'vect__max_df': 0.6333333333333333, 'vect__min_df': 0.002218649004451763, 'vect__ngram_range': (1, 1), 'vect__stop_words': 'english', 'vect__token_pattern': '\\w{1,}'}, {'clf__base_estimator': Pipeline(memory=None,
         steps=[('vect',
                 TfidfVectorizer(analyzer='word', binary=False,
                                 decode_error='strict',
                                 dtype=<class 'numpy.float64'>,
                                 encoding='utf-8', input='content',
                                 lowercase=True, max_df=0.6777777777777778,
                                 max_features=None,
                                 min_df=0.0012707570261625706,
                                 ngram_range=(1, 2), norm='l2',
                                 preprocessor=None, smooth_idf=True,
                                 stop_words=None, strip_accents=None,...
                                 vocabulary=None)),
                ('clf',
                 DecisionTreeClassifier(ccp_alpha=0.0, class_weight=None,
                                        criterion='gini', max_depth=49,
                                        max_features=0.56, max_leaf_nodes=316,
                                        min_impurity_decrease=1e-07,
                                        min_impurity_split=None,
                                        min_samples_leaf=4,
                                        min_samples_split=0.015198715678453873,
                                        min_weight_fraction_leaf=0.0,
                                        presort='deprecated', random_state=0,
                                        splitter='random'))],
         verbose=False), 'clf__bootstrap': False, 'clf__bootstrap_features': True, 'clf__max_features': 0.924683570865367, 'clf__max_samples': 0.18329344051423435, 'clf__n_estimators': 152, 'clf__oob_score': False, 'clf__warm_start': False, 'vect__max_df': 0.9, 'vect__min_df': 0.007541691599883028, 'vect__ngram_range': (1, 2), 'vect__stop_words': None, 'vect__token_pattern': '\\w{2,}'}, {'clf__base_estimator': None, 'clf__bootstrap': True, 'clf__bootstrap_features': False, 'clf__max_features': 0.9538123156103031, 'clf__max_samples': 0.24830564326047055, 'clf__n_estimators': 273, 'clf__oob_score': False, 'clf__warm_start': False, 'vect__max_df': 0.6333333333333333, 'vect__min_df': 0.00016147777227596658, 'vect__ngram_range': (1, 1), 'vect__stop_words': None, 'vect__token_pattern': '\\w{2,}'}, {'clf__base_estimator': Pipeline(memory=None,
         steps=[('vect',
                 TfidfVectorizer(analyzer='word', binary=False,
                                 decode_error='strict',
                                 dtype=<class 'numpy.float64'>,
                                 encoding='utf-8', input='content',
                                 lowercase=True, max_df=0.7222222222222222,
                                 max_features=None,
                                 min_df=0.00014705642569160768,
                                 ngram_range=(1, 1), norm='l2',
                                 preprocessor=None, smooth_idf=True,
                                 stop_words=None, strip_accents=None...
                                        max_features=0.01, max_leaf_nodes=None,
                                        max_samples=0.007808728196651904,
                                        min_impurity_decrease=3.1622776601683795e-10,
                                        min_impurity_split=None,
                                        min_samples_leaf=62,
                                        min_samples_split=0.0008966659727253451,
                                        min_weight_fraction_leaf=0.0007448459487372297,
                                        n_estimators=251, n_jobs=None,
                                        oob_score=False, random_state=0,
                                        verbose=True, warm_start=True))],
         verbose=False), 'clf__bootstrap': True, 'clf__bootstrap_features': True, 'clf__max_features': 0.9786387618449437, 'clf__max_samples': 0.07823710419563101, 'clf__n_estimators': 212, 'clf__oob_score': True, 'clf__warm_start': False, 'vect__max_df': 0.7222222222222222, 'vect__min_df': 0.0005357260075745206, 'vect__ngram_range': (1, 2), 'vect__stop_words': 'english', 'vect__token_pattern': '\\w{2,}'}, {'clf__base_estimator': Pipeline(memory=None,
         steps=[('vect',
                 TfidfVectorizer(analyzer='word', binary=False,
                                 decode_error='strict',
                                 dtype=<class 'numpy.float64'>,
                                 encoding='utf-8', input='content',
                                 lowercase=True, max_df=0.7222222222222222,
                                 max_features=None,
                                 min_df=0.00014705642569160768,
                                 ngram_range=(1, 1), norm='l2',
                                 preprocessor=None, smooth_idf=True,
                                 stop_words=None, strip_accents=None...
                                        max_features=0.01, max_leaf_nodes=None,
                                        max_samples=0.007808728196651904,
                                        min_impurity_decrease=3.1622776601683795e-10,
                                        min_impurity_split=None,
                                        min_samples_leaf=62,
                                        min_samples_split=0.0008966659727253451,
                                        min_weight_fraction_leaf=0.0007448459487372297,
                                        n_estimators=251, n_jobs=None,
                                        oob_score=False, random_state=0,
                                        verbose=True, warm_start=True))],
         verbose=False), 'clf__bootstrap': True, 'clf__bootstrap_features': False, 'clf__max_features': 0.39954510752324923, 'clf__max_samples': 0.6663561509390348, 'clf__n_estimators': 225, 'clf__oob_score': False, 'clf__warm_start': False, 'vect__max_df': 0.7666666666666666, 'vect__min_df': 0.0017424910007500895, 'vect__ngram_range': (1, 2), 'vect__stop_words': None, 'vect__token_pattern': '\\w{2,}'}, {'clf__base_estimator': Pipeline(memory=None,
         steps=[('vect',
                 TfidfVectorizer(analyzer='word', binary=False,
                                 decode_error='strict',
                                 dtype=<class 'numpy.float64'>,
                                 encoding='utf-8', input='content',
                                 lowercase=True, max_df=0.6777777777777778,
                                 max_features=None,
                                 min_df=0.0012707570261625706,
                                 ngram_range=(1, 2), norm='l2',
                                 preprocessor=None, smooth_idf=True,
                                 stop_words=None, strip_accents=None,...
                                 vocabulary=None)),
                ('clf',
                 DecisionTreeClassifier(ccp_alpha=0.0, class_weight=None,
                                        criterion='gini', max_depth=49,
                                        max_features=0.56, max_leaf_nodes=316,
                                        min_impurity_decrease=1e-07,
                                        min_impurity_split=None,
                                        min_samples_leaf=4,
                                        min_samples_split=0.015198715678453873,
                                        min_weight_fraction_leaf=0.0,
                                        presort='deprecated', random_state=0,
                                        splitter='random'))],
         verbose=False), 'clf__bootstrap': False, 'clf__bootstrap_features': False, 'clf__max_features': 0.9654803645813562, 'clf__max_samples': 0.2697257518287005, 'clf__n_estimators': 379, 'clf__oob_score': False, 'clf__warm_start': False, 'vect__max_df': 0.8111111111111111, 'vect__min_df': 0.00021567281452032302, 'vect__ngram_range': (1, 2), 'vect__stop_words': 'english', 'vect__token_pattern': '\\w{2,}'}, {'clf__base_estimator': Pipeline(memory=None,
         steps=[('vect',
                 TfidfVectorizer(analyzer='word', binary=False,
                                 decode_error='strict',
                                 dtype=<class 'numpy.float64'>,
                                 encoding='utf-8', input='content',
                                 lowercase=True, max_df=0.6333333333333333,
                                 max_features=None,
                                 min_df=0.0010083260374073057,
                                 ngram_range=(1, 1), norm='l2',
                                 preprocessor=None, smooth_idf=True,
                                 stop_words='english', strip_accents=...
                                 tokenizer=None, use_idf=True,
                                 vocabulary=None)),
                ('clf',
                 AdaBoostClassifier(algorithm='SAMME',
                                    base_estimator=LinearSVC(C=1.0,
                                                             class_weight=None,
                                                             dual=True,
                                                             fit_intercept=True,
                                                             intercept_scaling=1,
                                                             loss='squared_hinge',
                                                             max_iter=1000,
                                                             multi_class='ovr',
                                                             penalty='l2',
                                                             random_state=None,
                                                             tol=0.0001,
                                                             verbose=0),
                                    learning_rate=0.40053644159285917,
                                    n_estimators=190, random_state=0))],
         verbose=False), 'clf__bootstrap': False, 'clf__bootstrap_features': True, 'clf__max_features': 0.2957195072027342, 'clf__max_samples': 0.7275811653186909, 'clf__n_estimators': 137, 'clf__oob_score': True, 'clf__warm_start': False, 'vect__max_df': 0.5888888888888889, 'vect__min_df': 0.004324650568284658, 'vect__ngram_range': (1, 1), 'vect__stop_words': None, 'vect__token_pattern': '\\w{1,}'}, {'clf__base_estimator': Pipeline(memory=None,
         steps=[('vect',
                 TfidfVectorizer(analyzer='word', binary=False,
                                 decode_error='strict',
                                 dtype=<class 'numpy.float64'>,
                                 encoding='utf-8', input='content',
                                 lowercase=True, max_df=0.7666666666666666,
                                 max_features=None,
                                 min_df=0.00010572508827796376,
                                 ngram_range=(1, 2), norm='l2',
                                 preprocessor=None, smooth_idf=True,
                                 stop_words='english', strip_accents=None,
                                 sublinear_tf=False, token_pattern='\\w{1,}',
                                 tokenizer=None, use_idf=True,
                                 vocabulary=None)),
                ('clf',
                 LinearSVC(C=99.37702864845689, class_weight=None, dual=False,
                           fit_intercept=False, intercept_scaling=1,
                           loss='squared_hinge', max_iter=1000,
                           multi_class='ovr', penalty='l2', random_state=0,
                           tol=3.8359242531998225e-06, verbose=0))],
         verbose=False), 'clf__bootstrap': True, 'clf__bootstrap_features': False, 'clf__max_features': 0.17555351180611867, 'clf__max_samples': 0.10951785116950674, 'clf__n_estimators': 63, 'clf__oob_score': True, 'clf__warm_start': True, 'vect__max_df': 0.6777777777777778, 'vect__min_df': 0.00014154668451425088, 'vect__ngram_range': (1, 1), 'vect__stop_words': None, 'vect__token_pattern': '\\w{2,}'}, {'clf__base_estimator': Pipeline(memory=None,
         steps=[('vect',
                 TfidfVectorizer(analyzer='word', binary=False,
                                 decode_error='strict',
                                 dtype=<class 'numpy.float64'>,
                                 encoding='utf-8', input='content',
                                 lowercase=True, max_df=0.6333333333333333,
                                 max_features=None,
                                 min_df=0.0010083260374073057,
                                 ngram_range=(1, 1), norm='l2',
                                 preprocessor=None, smooth_idf=True,
                                 stop_words='english', strip_accents=...
                                 tokenizer=None, use_idf=True,
                                 vocabulary=None)),
                ('clf',
                 AdaBoostClassifier(algorithm='SAMME',
                                    base_estimator=LinearSVC(C=1.0,
                                                             class_weight=None,
                                                             dual=True,
                                                             fit_intercept=True,
                                                             intercept_scaling=1,
                                                             loss='squared_hinge',
                                                             max_iter=1000,
                                                             multi_class='ovr',
                                                             penalty='l2',
                                                             random_state=None,
                                                             tol=0.0001,
                                                             verbose=0),
                                    learning_rate=0.40053644159285917,
                                    n_estimators=190, random_state=0))],
         verbose=False), 'clf__bootstrap': False, 'clf__bootstrap_features': False, 'clf__max_features': 0.704675151726222, 'clf__max_samples': 0.2418837340599238, 'clf__n_estimators': 283, 'clf__oob_score': False, 'clf__warm_start': True, 'vect__max_df': 0.7222222222222222, 'vect__min_df': 0.004809084251460617, 'vect__ngram_range': (1, 2), 'vect__stop_words': None, 'vect__token_pattern': '\\w{1,}'}, {'clf__base_estimator': Pipeline(memory=None,
         steps=[('vect',
                 TfidfVectorizer(analyzer='word', binary=False,
                                 decode_error='strict',
                                 dtype=<class 'numpy.float64'>,
                                 encoding='utf-8', input='content',
                                 lowercase=True, max_df=0.8555555555555556,
                                 max_features=None,
                                 min_df=0.00016483617040605902,
                                 ngram_range=(1, 2), norm='l2',
                                 preprocessor=None, smooth_idf=True,
                                 stop_words='english', strip_accents...
                                 tokenizer=None, use_idf=True,
                                 vocabulary=None)),
                ('clf',
                 LogisticRegression(C=0.47946121772409134, class_weight=None,
                                    dual=False, fit_intercept=True,
                                    intercept_scaling=1,
                                    l1_ratio=0.9168495407673116, max_iter=75,
                                    multi_class='multinomial', n_jobs=None,
                                    penalty='none', random_state=0,
                                    solver='sag', tol=6.514227791666363e-07,
                                    verbose=0, warm_start=False))],
         verbose=False), 'clf__bootstrap': False, 'clf__bootstrap_features': False, 'clf__max_features': 0.7384145269866882, 'clf__max_samples': 0.16924641650506766, 'clf__n_estimators': 118, 'clf__oob_score': False, 'clf__warm_start': False, 'vect__max_df': 0.6333333333333333, 'vect__min_df': 0.0001135674029086469, 'vect__ngram_range': (1, 2), 'vect__stop_words': 'english', 'vect__token_pattern': '\\w{1,}'}, {'clf__base_estimator': Pipeline(memory=None,
         steps=[('vect',
                 TfidfVectorizer(analyzer='word', binary=False,
                                 decode_error='strict',
                                 dtype=<class 'numpy.float64'>,
                                 encoding='utf-8', input='content',
                                 lowercase=True, max_df=0.8555555555555556,
                                 max_features=None,
                                 min_df=0.00016483617040605902,
                                 ngram_range=(1, 2), norm='l2',
                                 preprocessor=None, smooth_idf=True,
                                 stop_words='english', strip_accents...
                                 tokenizer=None, use_idf=True,
                                 vocabulary=None)),
                ('clf',
                 LogisticRegression(C=0.47946121772409134, class_weight=None,
                                    dual=False, fit_intercept=True,
                                    intercept_scaling=1,
                                    l1_ratio=0.9168495407673116, max_iter=75,
                                    multi_class='multinomial', n_jobs=None,
                                    penalty='none', random_state=0,
                                    solver='sag', tol=6.514227791666363e-07,
                                    verbose=0, warm_start=False))],
         verbose=False), 'clf__bootstrap': False, 'clf__bootstrap_features': False, 'clf__max_features': 0.13755702703560413, 'clf__max_samples': 0.2345568286488937, 'clf__n_estimators': 177, 'clf__oob_score': True, 'clf__warm_start': True, 'vect__max_df': 0.6333333333333333, 'vect__min_df': 0.0015532522182295793, 'vect__ngram_range': (1, 1), 'vect__stop_words': 'english', 'vect__token_pattern': '\\w{1,}'}, {'clf__base_estimator': Pipeline(memory=None,
         steps=[('vect',
                 TfidfVectorizer(analyzer='word', binary=False,
                                 decode_error='strict',
                                 dtype=<class 'numpy.float64'>,
                                 encoding='utf-8', input='content',
                                 lowercase=True, max_df=0.7666666666666666,
                                 max_features=None,
                                 min_df=0.00010572508827796376,
                                 ngram_range=(1, 2), norm='l2',
                                 preprocessor=None, smooth_idf=True,
                                 stop_words='english', strip_accents=None,
                                 sublinear_tf=False, token_pattern='\\w{1,}',
                                 tokenizer=None, use_idf=True,
                                 vocabulary=None)),
                ('clf',
                 LinearSVC(C=99.37702864845689, class_weight=None, dual=False,
                           fit_intercept=False, intercept_scaling=1,
                           loss='squared_hinge', max_iter=1000,
                           multi_class='ovr', penalty='l2', random_state=0,
                           tol=3.8359242531998225e-06, verbose=0))],
         verbose=False), 'clf__bootstrap': False, 'clf__bootstrap_features': False, 'clf__max_features': 0.9092099774779366, 'clf__max_samples': 0.5099495424132175, 'clf__n_estimators': 333, 'clf__oob_score': False, 'clf__warm_start': True, 'vect__max_df': 0.7666666666666666, 'vect__min_df': 0.00031451792822378554, 'vect__ngram_range': (1, 1), 'vect__stop_words': 'english', 'vect__token_pattern': '\\w{2,}'}, {'clf__base_estimator': Pipeline(memory=None,
         steps=[('vect',
                 TfidfVectorizer(analyzer='word', binary=False,
                                 decode_error='strict',
                                 dtype=<class 'numpy.float64'>,
                                 encoding='utf-8', input='content',
                                 lowercase=True, max_df=0.7222222222222222,
                                 max_features=None,
                                 min_df=0.00014705642569160768,
                                 ngram_range=(1, 1), norm='l2',
                                 preprocessor=None, smooth_idf=True,
                                 stop_words=None, strip_accents=None...
                                        max_features=0.01, max_leaf_nodes=None,
                                        max_samples=0.007808728196651904,
                                        min_impurity_decrease=3.1622776601683795e-10,
                                        min_impurity_split=None,
                                        min_samples_leaf=62,
                                        min_samples_split=0.0008966659727253451,
                                        min_weight_fraction_leaf=0.0007448459487372297,
                                        n_estimators=251, n_jobs=None,
                                        oob_score=False, random_state=0,
                                        verbose=True, warm_start=True))],
         verbose=False), 'clf__bootstrap': False, 'clf__bootstrap_features': False, 'clf__max_features': 0.8042730110735646, 'clf__max_samples': 0.0316723526150523, 'clf__n_estimators': 193, 'clf__oob_score': False, 'clf__warm_start': True, 'vect__max_df': 0.6333333333333333, 'vect__min_df': 0.00268465581437476, 'vect__ngram_range': (1, 1), 'vect__stop_words': 'english', 'vect__token_pattern': '\\w{2,}'}, {'clf__base_estimator': Pipeline(memory=None,
         steps=[('vect',
                 TfidfVectorizer(analyzer='word', binary=False,
                                 decode_error='strict',
                                 dtype=<class 'numpy.float64'>,
                                 encoding='utf-8', input='content',
                                 lowercase=True, max_df=0.8555555555555556,
                                 max_features=None,
                                 min_df=0.00016483617040605902,
                                 ngram_range=(1, 2), norm='l2',
                                 preprocessor=None, smooth_idf=True,
                                 stop_words='english', strip_accents...
                                 tokenizer=None, use_idf=True,
                                 vocabulary=None)),
                ('clf',
                 LogisticRegression(C=0.47946121772409134, class_weight=None,
                                    dual=False, fit_intercept=True,
                                    intercept_scaling=1,
                                    l1_ratio=0.9168495407673116, max_iter=75,
                                    multi_class='multinomial', n_jobs=None,
                                    penalty='none', random_state=0,
                                    solver='sag', tol=6.514227791666363e-07,
                                    verbose=0, warm_start=False))],
         verbose=False), 'clf__bootstrap': True, 'clf__bootstrap_features': False, 'clf__max_features': 0.9067509207291913, 'clf__max_samples': 0.07565498164459505, 'clf__n_estimators': 162, 'clf__oob_score': True, 'clf__warm_start': False, 'vect__max_df': 0.6777777777777778, 'vect__min_df': 0.0033308596826294792, 'vect__ngram_range': (1, 1), 'vect__stop_words': None, 'vect__token_pattern': '\\w{1,}'}, {'clf__base_estimator': Pipeline(memory=None,
         steps=[('vect',
                 TfidfVectorizer(analyzer='word', binary=False,
                                 decode_error='strict',
                                 dtype=<class 'numpy.float64'>,
                                 encoding='utf-8', input='content',
                                 lowercase=True, max_df=0.6333333333333333,
                                 max_features=None,
                                 min_df=0.0010083260374073057,
                                 ngram_range=(1, 1), norm='l2',
                                 preprocessor=None, smooth_idf=True,
                                 stop_words='english', strip_accents=...
                                 tokenizer=None, use_idf=True,
                                 vocabulary=None)),
                ('clf',
                 AdaBoostClassifier(algorithm='SAMME',
                                    base_estimator=LinearSVC(C=1.0,
                                                             class_weight=None,
                                                             dual=True,
                                                             fit_intercept=True,
                                                             intercept_scaling=1,
                                                             loss='squared_hinge',
                                                             max_iter=1000,
                                                             multi_class='ovr',
                                                             penalty='l2',
                                                             random_state=None,
                                                             tol=0.0001,
                                                             verbose=0),
                                    learning_rate=0.40053644159285917,
                                    n_estimators=190, random_state=0))],
         verbose=False), 'clf__bootstrap': False, 'clf__bootstrap_features': False, 'clf__max_features': 0.012365978156541946, 'clf__max_samples': 0.09566562095242104, 'clf__n_estimators': 56, 'clf__oob_score': True, 'clf__warm_start': False, 'vect__max_df': 0.7222222222222222, 'vect__min_df': 0.008383686129095453, 'vect__ngram_range': (1, 1), 'vect__stop_words': None, 'vect__token_pattern': '\\w{2,}'}, {'clf__base_estimator': MultinomialNB(alpha=1.0, class_prior=None, fit_prior=True), 'clf__bootstrap': False, 'clf__bootstrap_features': True, 'clf__max_features': 0.7906509778233803, 'clf__max_samples': 0.16631826740608846, 'clf__n_estimators': 361, 'clf__oob_score': False, 'clf__warm_start': True, 'vect__max_df': 0.7666666666666666, 'vect__min_df': 0.009913783614138921, 'vect__ngram_range': (1, 1), 'vect__stop_words': None, 'vect__token_pattern': '\\w{1,}'}, {'clf__base_estimator': Pipeline(memory=None,
         steps=[('vect',
                 TfidfVectorizer(analyzer='word', binary=False,
                                 decode_error='strict',
                                 dtype=<class 'numpy.float64'>,
                                 encoding='utf-8', input='content',
                                 lowercase=True, max_df=0.6333333333333333,
                                 max_features=None,
                                 min_df=0.0010083260374073057,
                                 ngram_range=(1, 1), norm='l2',
                                 preprocessor=None, smooth_idf=True,
                                 stop_words='english', strip_accents=...
                                 tokenizer=None, use_idf=True,
                                 vocabulary=None)),
                ('clf',
                 AdaBoostClassifier(algorithm='SAMME',
                                    base_estimator=LinearSVC(C=1.0,
                                                             class_weight=None,
                                                             dual=True,
                                                             fit_intercept=True,
                                                             intercept_scaling=1,
                                                             loss='squared_hinge',
                                                             max_iter=1000,
                                                             multi_class='ovr',
                                                             penalty='l2',
                                                             random_state=None,
                                                             tol=0.0001,
                                                             verbose=0),
                                    learning_rate=0.40053644159285917,
                                    n_estimators=190, random_state=0))],
         verbose=False), 'clf__bootstrap': False, 'clf__bootstrap_features': False, 'clf__max_features': 0.7918667656201579, 'clf__max_samples': 0.9811373833490997, 'clf__n_estimators': 82, 'clf__oob_score': True, 'clf__warm_start': False, 'vect__max_df': 0.9, 'vect__min_df': 0.002180891700964028, 'vect__ngram_range': (1, 2), 'vect__stop_words': None, 'vect__token_pattern': '\\w{2,}'}, {'clf__base_estimator': Pipeline(memory=None,
         steps=[('vect',
                 TfidfVectorizer(analyzer='word', binary=False,
                                 decode_error='strict',
                                 dtype=<class 'numpy.float64'>,
                                 encoding='utf-8', input='content',
                                 lowercase=True, max_df=0.7666666666666666,
                                 max_features=None,
                                 min_df=0.00010572508827796376,
                                 ngram_range=(1, 2), norm='l2',
                                 preprocessor=None, smooth_idf=True,
                                 stop_words='english', strip_accents=None,
                                 sublinear_tf=False, token_pattern='\\w{1,}',
                                 tokenizer=None, use_idf=True,
                                 vocabulary=None)),
                ('clf',
                 LinearSVC(C=99.37702864845689, class_weight=None, dual=False,
                           fit_intercept=False, intercept_scaling=1,
                           loss='squared_hinge', max_iter=1000,
                           multi_class='ovr', penalty='l2', random_state=0,
                           tol=3.8359242531998225e-06, verbose=0))],
         verbose=False), 'clf__bootstrap': False, 'clf__bootstrap_features': False, 'clf__max_features': 0.7999896543473607, 'clf__max_samples': 0.542419495758003, 'clf__n_estimators': 116, 'clf__oob_score': False, 'clf__warm_start': True, 'vect__max_df': 0.6333333333333333, 'vect__min_df': 0.001731777435889714, 'vect__ngram_range': (1, 1), 'vect__stop_words': 'english', 'vect__token_pattern': '\\w{2,}'}, {'clf__base_estimator': Pipeline(memory=None,
         steps=[('vect',
                 TfidfVectorizer(analyzer='word', binary=False,
                                 decode_error='strict',
                                 dtype=<class 'numpy.float64'>,
                                 encoding='utf-8', input='content',
                                 lowercase=True, max_df=0.6777777777777778,
                                 max_features=None,
                                 min_df=0.0012707570261625706,
                                 ngram_range=(1, 2), norm='l2',
                                 preprocessor=None, smooth_idf=True,
                                 stop_words=None, strip_accents=None,...
                                 vocabulary=None)),
                ('clf',
                 DecisionTreeClassifier(ccp_alpha=0.0, class_weight=None,
                                        criterion='gini', max_depth=49,
                                        max_features=0.56, max_leaf_nodes=316,
                                        min_impurity_decrease=1e-07,
                                        min_impurity_split=None,
                                        min_samples_leaf=4,
                                        min_samples_split=0.015198715678453873,
                                        min_weight_fraction_leaf=0.0,
                                        presort='deprecated', random_state=0,
                                        splitter='random'))],
         verbose=False), 'clf__bootstrap': True, 'clf__bootstrap_features': True, 'clf__max_features': 0.25162272665336205, 'clf__max_samples': 0.7455963612487158, 'clf__n_estimators': 225, 'clf__oob_score': False, 'clf__warm_start': True, 'vect__max_df': 0.8111111111111111, 'vect__min_df': 0.00048804517689075166, 'vect__ngram_range': (1, 1), 'vect__stop_words': 'english', 'vect__token_pattern': '\\w{2,}'}, {'clf__base_estimator': Pipeline(memory=None,
         steps=[('vect',
                 TfidfVectorizer(analyzer='word', binary=False,
                                 decode_error='strict',
                                 dtype=<class 'numpy.float64'>,
                                 encoding='utf-8', input='content',
                                 lowercase=True, max_df=0.7222222222222222,
                                 max_features=None,
                                 min_df=0.00014705642569160768,
                                 ngram_range=(1, 1), norm='l2',
                                 preprocessor=None, smooth_idf=True,
                                 stop_words=None, strip_accents=None...
                                        max_features=0.01, max_leaf_nodes=None,
                                        max_samples=0.007808728196651904,
                                        min_impurity_decrease=3.1622776601683795e-10,
                                        min_impurity_split=None,
                                        min_samples_leaf=62,
                                        min_samples_split=0.0008966659727253451,
                                        min_weight_fraction_leaf=0.0007448459487372297,
                                        n_estimators=251, n_jobs=None,
                                        oob_score=False, random_state=0,
                                        verbose=True, warm_start=True))],
         verbose=False), 'clf__bootstrap': True, 'clf__bootstrap_features': True, 'clf__max_features': 0.4298021923059766, 'clf__max_samples': 0.9716220566919046, 'clf__n_estimators': 28, 'clf__oob_score': True, 'clf__warm_start': False, 'vect__max_df': 0.7222222222222222, 'vect__min_df': 0.0002874151321146624, 'vect__ngram_range': (1, 2), 'vect__stop_words': 'english', 'vect__token_pattern': '\\w{2,}'}, {'clf__base_estimator': Pipeline(memory=None,
         steps=[('vect',
                 TfidfVectorizer(analyzer='word', binary=False,
                                 decode_error='strict',
                                 dtype=<class 'numpy.float64'>,
                                 encoding='utf-8', input='content',
                                 lowercase=True, max_df=0.8555555555555556,
                                 max_features=None,
                                 min_df=0.00016483617040605902,
                                 ngram_range=(1, 2), norm='l2',
                                 preprocessor=None, smooth_idf=True,
                                 stop_words='english', strip_accents...
                                 tokenizer=None, use_idf=True,
                                 vocabulary=None)),
                ('clf',
                 LogisticRegression(C=0.47946121772409134, class_weight=None,
                                    dual=False, fit_intercept=True,
                                    intercept_scaling=1,
                                    l1_ratio=0.9168495407673116, max_iter=75,
                                    multi_class='multinomial', n_jobs=None,
                                    penalty='none', random_state=0,
                                    solver='sag', tol=6.514227791666363e-07,
                                    verbose=0, warm_start=False))],
         verbose=False), 'clf__bootstrap': False, 'clf__bootstrap_features': False, 'clf__max_features': 0.27931808606511144, 'clf__max_samples': 0.16184021655008674, 'clf__n_estimators': 389, 'clf__oob_score': False, 'clf__warm_start': True, 'vect__max_df': 0.6333333333333333, 'vect__min_df': 0.004732211910256779, 'vect__ngram_range': (1, 1), 'vect__stop_words': 'english', 'vect__token_pattern': '\\w{1,}'}, {'clf__base_estimator': Pipeline(memory=None,
         steps=[('vect',
                 TfidfVectorizer(analyzer='word', binary=False,
                                 decode_error='strict',
                                 dtype=<class 'numpy.float64'>,
                                 encoding='utf-8', input='content',
                                 lowercase=True, max_df=0.7666666666666666,
                                 max_features=None,
                                 min_df=0.00010572508827796376,
                                 ngram_range=(1, 2), norm='l2',
                                 preprocessor=None, smooth_idf=True,
                                 stop_words='english', strip_accents=None,
                                 sublinear_tf=False, token_pattern='\\w{1,}',
                                 tokenizer=None, use_idf=True,
                                 vocabulary=None)),
                ('clf',
                 LinearSVC(C=99.37702864845689, class_weight=None, dual=False,
                           fit_intercept=False, intercept_scaling=1,
                           loss='squared_hinge', max_iter=1000,
                           multi_class='ovr', penalty='l2', random_state=0,
                           tol=3.8359242531998225e-06, verbose=0))],
         verbose=False), 'clf__bootstrap': False, 'clf__bootstrap_features': False, 'clf__max_features': 0.5048419906666137, 'clf__max_samples': 0.775440371777647, 'clf__n_estimators': 44, 'clf__oob_score': True, 'clf__warm_start': True, 'vect__max_df': 0.5444444444444444, 'vect__min_df': 0.0008879383365991017, 'vect__ngram_range': (1, 1), 'vect__stop_words': None, 'vect__token_pattern': '\\w{1,}'}, {'clf__base_estimator': Pipeline(memory=None,
         steps=[('vect',
                 TfidfVectorizer(analyzer='word', binary=False,
                                 decode_error='strict',
                                 dtype=<class 'numpy.float64'>,
                                 encoding='utf-8', input='content',
                                 lowercase=True, max_df=0.6777777777777778,
                                 max_features=None,
                                 min_df=0.0012707570261625706,
                                 ngram_range=(1, 2), norm='l2',
                                 preprocessor=None, smooth_idf=True,
                                 stop_words=None, strip_accents=None,...
                                 vocabulary=None)),
                ('clf',
                 DecisionTreeClassifier(ccp_alpha=0.0, class_weight=None,
                                        criterion='gini', max_depth=49,
                                        max_features=0.56, max_leaf_nodes=316,
                                        min_impurity_decrease=1e-07,
                                        min_impurity_split=None,
                                        min_samples_leaf=4,
                                        min_samples_split=0.015198715678453873,
                                        min_weight_fraction_leaf=0.0,
                                        presort='deprecated', random_state=0,
                                        splitter='random'))],
         verbose=False), 'clf__bootstrap': True, 'clf__bootstrap_features': True, 'clf__max_features': 0.9020546184530749, 'clf__max_samples': 0.5220241186319174, 'clf__n_estimators': 304, 'clf__oob_score': True, 'clf__warm_start': True, 'vect__max_df': 0.7222222222222222, 'vect__min_df': 0.00024193889788516666, 'vect__ngram_range': (1, 2), 'vect__stop_words': 'english', 'vect__token_pattern': '\\w{1,}'}, {'clf__base_estimator': None, 'clf__bootstrap': True, 'clf__bootstrap_features': False, 'clf__max_features': 0.8163514228866441, 'clf__max_samples': 0.4981032487442846, 'clf__n_estimators': 212, 'clf__oob_score': True, 'clf__warm_start': True, 'vect__max_df': 0.5, 'vect__min_df': 0.003323986531247918, 'vect__ngram_range': (1, 1), 'vect__stop_words': 'english', 'vect__token_pattern': '\\w{2,}'}, {'clf__base_estimator': Pipeline(memory=None,
         steps=[('vect',
                 TfidfVectorizer(analyzer='word', binary=False,
                                 decode_error='strict',
                                 dtype=<class 'numpy.float64'>,
                                 encoding='utf-8', input='content',
                                 lowercase=True, max_df=0.6777777777777778,
                                 max_features=None,
                                 min_df=0.0012707570261625706,
                                 ngram_range=(1, 2), norm='l2',
                                 preprocessor=None, smooth_idf=True,
                                 stop_words=None, strip_accents=None,...
                                 vocabulary=None)),
                ('clf',
                 DecisionTreeClassifier(ccp_alpha=0.0, class_weight=None,
                                        criterion='gini', max_depth=49,
                                        max_features=0.56, max_leaf_nodes=316,
                                        min_impurity_decrease=1e-07,
                                        min_impurity_split=None,
                                        min_samples_leaf=4,
                                        min_samples_split=0.015198715678453873,
                                        min_weight_fraction_leaf=0.0,
                                        presort='deprecated', random_state=0,
                                        splitter='random'))],
         verbose=False), 'clf__bootstrap': False, 'clf__bootstrap_features': True, 'clf__max_features': 0.864386195652989, 'clf__max_samples': 0.1530630987579954, 'clf__n_estimators': 296, 'clf__oob_score': True, 'clf__warm_start': False, 'vect__max_df': 0.7666666666666666, 'vect__min_df': 0.003383252045944012, 'vect__ngram_range': (1, 2), 'vect__stop_words': 'english', 'vect__token_pattern': '\\w{2,}'}, {'clf__base_estimator': Pipeline(memory=None,
         steps=[('vect',
                 TfidfVectorizer(analyzer='word', binary=False,
                                 decode_error='strict',
                                 dtype=<class 'numpy.float64'>,
                                 encoding='utf-8', input='content',
                                 lowercase=True, max_df=0.8555555555555556,
                                 max_features=None,
                                 min_df=0.00016483617040605902,
                                 ngram_range=(1, 2), norm='l2',
                                 preprocessor=None, smooth_idf=True,
                                 stop_words='english', strip_accents...
                                 tokenizer=None, use_idf=True,
                                 vocabulary=None)),
                ('clf',
                 LogisticRegression(C=0.47946121772409134, class_weight=None,
                                    dual=False, fit_intercept=True,
                                    intercept_scaling=1,
                                    l1_ratio=0.9168495407673116, max_iter=75,
                                    multi_class='multinomial', n_jobs=None,
                                    penalty='none', random_state=0,
                                    solver='sag', tol=6.514227791666363e-07,
                                    verbose=0, warm_start=False))],
         verbose=False), 'clf__bootstrap': True, 'clf__bootstrap_features': True, 'clf__max_features': 0.12807400406726588, 'clf__max_samples': 0.3387168797872051, 'clf__n_estimators': 289, 'clf__oob_score': False, 'clf__warm_start': True, 'vect__max_df': 0.5444444444444444, 'vect__min_df': 0.001004915748974247, 'vect__ngram_range': (1, 1), 'vect__stop_words': None, 'vect__token_pattern': '\\w{2,}'}, {'clf__base_estimator': Pipeline(memory=None,
         steps=[('vect',
                 TfidfVectorizer(analyzer='word', binary=False,
                                 decode_error='strict',
                                 dtype=<class 'numpy.float64'>,
                                 encoding='utf-8', input='content',
                                 lowercase=True, max_df=0.6777777777777778,
                                 max_features=None,
                                 min_df=0.0012707570261625706,
                                 ngram_range=(1, 2), norm='l2',
                                 preprocessor=None, smooth_idf=True,
                                 stop_words=None, strip_accents=None,...
                                 vocabulary=None)),
                ('clf',
                 DecisionTreeClassifier(ccp_alpha=0.0, class_weight=None,
                                        criterion='gini', max_depth=49,
                                        max_features=0.56, max_leaf_nodes=316,
                                        min_impurity_decrease=1e-07,
                                        min_impurity_split=None,
                                        min_samples_leaf=4,
                                        min_samples_split=0.015198715678453873,
                                        min_weight_fraction_leaf=0.0,
                                        presort='deprecated', random_state=0,
                                        splitter='random'))],
         verbose=False), 'clf__bootstrap': False, 'clf__bootstrap_features': False, 'clf__max_features': 0.21087698441783165, 'clf__max_samples': 0.02044580954985431, 'clf__n_estimators': 74, 'clf__oob_score': False, 'clf__warm_start': True, 'vect__max_df': 0.8555555555555556, 'vect__min_df': 0.0034121217143816373, 'vect__ngram_range': (1, 2), 'vect__stop_words': None, 'vect__token_pattern': '\\w{1,}'}, {'clf__base_estimator': Pipeline(memory=None,
         steps=[('vect',
                 TfidfVectorizer(analyzer='word', binary=False,
                                 decode_error='strict',
                                 dtype=<class 'numpy.float64'>,
                                 encoding='utf-8', input='content',
                                 lowercase=True, max_df=0.7222222222222222,
                                 max_features=None,
                                 min_df=0.00014705642569160768,
                                 ngram_range=(1, 1), norm='l2',
                                 preprocessor=None, smooth_idf=True,
                                 stop_words=None, strip_accents=None...
                                        max_features=0.01, max_leaf_nodes=None,
                                        max_samples=0.007808728196651904,
                                        min_impurity_decrease=3.1622776601683795e-10,
                                        min_impurity_split=None,
                                        min_samples_leaf=62,
                                        min_samples_split=0.0008966659727253451,
                                        min_weight_fraction_leaf=0.0007448459487372297,
                                        n_estimators=251, n_jobs=None,
                                        oob_score=False, random_state=0,
                                        verbose=True, warm_start=True))],
         verbose=False), 'clf__bootstrap': False, 'clf__bootstrap_features': True, 'clf__max_features': 0.9546438994928883, 'clf__max_samples': 0.6696029915573883, 'clf__n_estimators': 245, 'clf__oob_score': False, 'clf__warm_start': True, 'vect__max_df': 0.6777777777777778, 'vect__min_df': 0.008133989202752023, 'vect__ngram_range': (1, 2), 'vect__stop_words': None, 'vect__token_pattern': '\\w{2,}'}, {'clf__base_estimator': Pipeline(memory=None,
         steps=[('vect',
                 TfidfVectorizer(analyzer='word', binary=False,
                                 decode_error='strict',
                                 dtype=<class 'numpy.float64'>,
                                 encoding='utf-8', input='content',
                                 lowercase=True, max_df=0.8555555555555556,
                                 max_features=None,
                                 min_df=0.00016483617040605902,
                                 ngram_range=(1, 2), norm='l2',
                                 preprocessor=None, smooth_idf=True,
                                 stop_words='english', strip_accents...
                                 tokenizer=None, use_idf=True,
                                 vocabulary=None)),
                ('clf',
                 LogisticRegression(C=0.47946121772409134, class_weight=None,
                                    dual=False, fit_intercept=True,
                                    intercept_scaling=1,
                                    l1_ratio=0.9168495407673116, max_iter=75,
                                    multi_class='multinomial', n_jobs=None,
                                    penalty='none', random_state=0,
                                    solver='sag', tol=6.514227791666363e-07,
                                    verbose=0, warm_start=False))],
         verbose=False), 'clf__bootstrap': False, 'clf__bootstrap_features': True, 'clf__max_features': 0.47618990370366787, 'clf__max_samples': 0.8686838168058331, 'clf__n_estimators': 171, 'clf__oob_score': True, 'clf__warm_start': True, 'vect__max_df': 0.6333333333333333, 'vect__min_df': 0.0001824953888680803, 'vect__ngram_range': (1, 2), 'vect__stop_words': None, 'vect__token_pattern': '\\w{1,}'}, {'clf__base_estimator': Pipeline(memory=None,
         steps=[('vect',
                 TfidfVectorizer(analyzer='word', binary=False,
                                 decode_error='strict',
                                 dtype=<class 'numpy.float64'>,
                                 encoding='utf-8', input='content',
                                 lowercase=True, max_df=0.7222222222222222,
                                 max_features=None,
                                 min_df=0.00014705642569160768,
                                 ngram_range=(1, 1), norm='l2',
                                 preprocessor=None, smooth_idf=True,
                                 stop_words=None, strip_accents=None...
                                        max_features=0.01, max_leaf_nodes=None,
                                        max_samples=0.007808728196651904,
                                        min_impurity_decrease=3.1622776601683795e-10,
                                        min_impurity_split=None,
                                        min_samples_leaf=62,
                                        min_samples_split=0.0008966659727253451,
                                        min_weight_fraction_leaf=0.0007448459487372297,
                                        n_estimators=251, n_jobs=None,
                                        oob_score=False, random_state=0,
                                        verbose=True, warm_start=True))],
         verbose=False), 'clf__bootstrap': True, 'clf__bootstrap_features': False, 'clf__max_features': 0.5817778153348276, 'clf__max_samples': 0.23230537090633374, 'clf__n_estimators': 259, 'clf__oob_score': False, 'clf__warm_start': True, 'vect__max_df': 0.5444444444444444, 'vect__min_df': 0.009429671739097084, 'vect__ngram_range': (1, 2), 'vect__stop_words': 'english', 'vect__token_pattern': '\\w{1,}'}, {'clf__base_estimator': Pipeline(memory=None,
         steps=[('vect',
                 TfidfVectorizer(analyzer='word', binary=False,
                                 decode_error='strict',
                                 dtype=<class 'numpy.float64'>,
                                 encoding='utf-8', input='content',
                                 lowercase=True, max_df=0.7666666666666666,
                                 max_features=None,
                                 min_df=0.00010572508827796376,
                                 ngram_range=(1, 2), norm='l2',
                                 preprocessor=None, smooth_idf=True,
                                 stop_words='english', strip_accents=None,
                                 sublinear_tf=False, token_pattern='\\w{1,}',
                                 tokenizer=None, use_idf=True,
                                 vocabulary=None)),
                ('clf',
                 LinearSVC(C=99.37702864845689, class_weight=None, dual=False,
                           fit_intercept=False, intercept_scaling=1,
                           loss='squared_hinge', max_iter=1000,
                           multi_class='ovr', penalty='l2', random_state=0,
                           tol=3.8359242531998225e-06, verbose=0))],
         verbose=False), 'clf__bootstrap': False, 'clf__bootstrap_features': True, 'clf__max_features': 0.08686111066153979, 'clf__max_samples': 0.03300628965408958, 'clf__n_estimators': 126, 'clf__oob_score': False, 'clf__warm_start': False, 'vect__max_df': 0.6777777777777778, 'vect__min_df': 0.0004638053111666062, 'vect__ngram_range': (1, 2), 'vect__stop_words': None, 'vect__token_pattern': '\\w{1,}'}, {'clf__base_estimator': Pipeline(memory=None,
         steps=[('vect',
                 TfidfVectorizer(analyzer='word', binary=False,
                                 decode_error='strict',
                                 dtype=<class 'numpy.float64'>,
                                 encoding='utf-8', input='content',
                                 lowercase=True, max_df=0.8555555555555556,
                                 max_features=None,
                                 min_df=0.00016483617040605902,
                                 ngram_range=(1, 2), norm='l2',
                                 preprocessor=None, smooth_idf=True,
                                 stop_words='english', strip_accents...
                                 tokenizer=None, use_idf=True,
                                 vocabulary=None)),
                ('clf',
                 LogisticRegression(C=0.47946121772409134, class_weight=None,
                                    dual=False, fit_intercept=True,
                                    intercept_scaling=1,
                                    l1_ratio=0.9168495407673116, max_iter=75,
                                    multi_class='multinomial', n_jobs=None,
                                    penalty='none', random_state=0,
                                    solver='sag', tol=6.514227791666363e-07,
                                    verbose=0, warm_start=False))],
         verbose=False), 'clf__bootstrap': False, 'clf__bootstrap_features': False, 'clf__max_features': 0.6599270267321241, 'clf__max_samples': 0.5792412774287145, 'clf__n_estimators': 33, 'clf__oob_score': True, 'clf__warm_start': False, 'vect__max_df': 0.6777777777777778, 'vect__min_df': 0.0018334549837277117, 'vect__ngram_range': (1, 1), 'vect__stop_words': 'english', 'vect__token_pattern': '\\w{2,}'}, {'clf__base_estimator': Pipeline(memory=None,
         steps=[('vect',
                 TfidfVectorizer(analyzer='word', binary=False,
                                 decode_error='strict',
                                 dtype=<class 'numpy.float64'>,
                                 encoding='utf-8', input='content',
                                 lowercase=True, max_df=0.7222222222222222,
                                 max_features=None,
                                 min_df=0.00014705642569160768,
                                 ngram_range=(1, 1), norm='l2',
                                 preprocessor=None, smooth_idf=True,
                                 stop_words=None, strip_accents=None...
                                        max_features=0.01, max_leaf_nodes=None,
                                        max_samples=0.007808728196651904,
                                        min_impurity_decrease=3.1622776601683795e-10,
                                        min_impurity_split=None,
                                        min_samples_leaf=62,
                                        min_samples_split=0.0008966659727253451,
                                        min_weight_fraction_leaf=0.0007448459487372297,
                                        n_estimators=251, n_jobs=None,
                                        oob_score=False, random_state=0,
                                        verbose=True, warm_start=True))],
         verbose=False), 'clf__bootstrap': True, 'clf__bootstrap_features': True, 'clf__max_features': 0.054843499661434536, 'clf__max_samples': 0.35605675920268476, 'clf__n_estimators': 60, 'clf__oob_score': True, 'clf__warm_start': True, 'vect__max_df': 0.7222222222222222, 'vect__min_df': 0.00856091324002353, 'vect__ngram_range': (1, 1), 'vect__stop_words': 'english', 'vect__token_pattern': '\\w{1,}'}, {'clf__base_estimator': Pipeline(memory=None,
         steps=[('vect',
                 TfidfVectorizer(analyzer='word', binary=False,
                                 decode_error='strict',
                                 dtype=<class 'numpy.float64'>,
                                 encoding='utf-8', input='content',
                                 lowercase=True, max_df=0.6333333333333333,
                                 max_features=None,
                                 min_df=0.0010083260374073057,
                                 ngram_range=(1, 1), norm='l2',
                                 preprocessor=None, smooth_idf=True,
                                 stop_words='english', strip_accents=...
                                 tokenizer=None, use_idf=True,
                                 vocabulary=None)),
                ('clf',
                 AdaBoostClassifier(algorithm='SAMME',
                                    base_estimator=LinearSVC(C=1.0,
                                                             class_weight=None,
                                                             dual=True,
                                                             fit_intercept=True,
                                                             intercept_scaling=1,
                                                             loss='squared_hinge',
                                                             max_iter=1000,
                                                             multi_class='ovr',
                                                             penalty='l2',
                                                             random_state=None,
                                                             tol=0.0001,
                                                             verbose=0),
                                    learning_rate=0.40053644159285917,
                                    n_estimators=190, random_state=0))],
         verbose=False), 'clf__bootstrap': False, 'clf__bootstrap_features': False, 'clf__max_features': 0.7994901838837157, 'clf__max_samples': 0.9243549643085232, 'clf__n_estimators': 103, 'clf__oob_score': True, 'clf__warm_start': True, 'vect__max_df': 0.7222222222222222, 'vect__min_df': 0.0006614152214088263, 'vect__ngram_range': (1, 1), 'vect__stop_words': None, 'vect__token_pattern': '\\w{2,}'}, {'clf__base_estimator': None, 'clf__bootstrap': True, 'clf__bootstrap_features': True, 'clf__max_features': 0.5336550066058341, 'clf__max_samples': 0.08530926639771286, 'clf__n_estimators': 129, 'clf__oob_score': True, 'clf__warm_start': True, 'vect__max_df': 0.8555555555555556, 'vect__min_df': 0.0008982512872686716, 'vect__ngram_range': (1, 2), 'vect__stop_words': None, 'vect__token_pattern': '\\w{1,}'}, {'clf__base_estimator': Pipeline(memory=None,
         steps=[('vect',
                 TfidfVectorizer(analyzer='word', binary=False,
                                 decode_error='strict',
                                 dtype=<class 'numpy.float64'>,
                                 encoding='utf-8', input='content',
                                 lowercase=True, max_df=0.8555555555555556,
                                 max_features=None,
                                 min_df=0.00016483617040605902,
                                 ngram_range=(1, 2), norm='l2',
                                 preprocessor=None, smooth_idf=True,
                                 stop_words='english', strip_accents...
                                 tokenizer=None, use_idf=True,
                                 vocabulary=None)),
                ('clf',
                 LogisticRegression(C=0.47946121772409134, class_weight=None,
                                    dual=False, fit_intercept=True,
                                    intercept_scaling=1,
                                    l1_ratio=0.9168495407673116, max_iter=75,
                                    multi_class='multinomial', n_jobs=None,
                                    penalty='none', random_state=0,
                                    solver='sag', tol=6.514227791666363e-07,
                                    verbose=0, warm_start=False))],
         verbose=False), 'clf__bootstrap': False, 'clf__bootstrap_features': False, 'clf__max_features': 0.36784273545426227, 'clf__max_samples': 0.4769452750834302, 'clf__n_estimators': 292, 'clf__oob_score': True, 'clf__warm_start': True, 'vect__max_df': 0.9, 'vect__min_df': 0.00027265839554225287, 'vect__ngram_range': (1, 1), 'vect__stop_words': None, 'vect__token_pattern': '\\w{2,}'}, {'clf__base_estimator': MultinomialNB(alpha=1.0, class_prior=None, fit_prior=True), 'clf__bootstrap': True, 'clf__bootstrap_features': True, 'clf__max_features': 0.7148884596336798, 'clf__max_samples': 0.41352797682613096, 'clf__n_estimators': 133, 'clf__oob_score': False, 'clf__warm_start': True, 'vect__max_df': 0.6777777777777778, 'vect__min_df': 0.00040628383281475673, 'vect__ngram_range': (1, 1), 'vect__stop_words': 'english', 'vect__token_pattern': '\\w{1,}'}, {'clf__base_estimator': Pipeline(memory=None,
         steps=[('vect',
                 TfidfVectorizer(analyzer='word', binary=False,
                                 decode_error='strict',
                                 dtype=<class 'numpy.float64'>,
                                 encoding='utf-8', input='content',
                                 lowercase=True, max_df=0.7222222222222222,
                                 max_features=None,
                                 min_df=0.00014705642569160768,
                                 ngram_range=(1, 1), norm='l2',
                                 preprocessor=None, smooth_idf=True,
                                 stop_words=None, strip_accents=None...
                                        max_features=0.01, max_leaf_nodes=None,
                                        max_samples=0.007808728196651904,
                                        min_impurity_decrease=3.1622776601683795e-10,
                                        min_impurity_split=None,
                                        min_samples_leaf=62,
                                        min_samples_split=0.0008966659727253451,
                                        min_weight_fraction_leaf=0.0007448459487372297,
                                        n_estimators=251, n_jobs=None,
                                        oob_score=False, random_state=0,
                                        verbose=True, warm_start=True))],
         verbose=False), 'clf__bootstrap': True, 'clf__bootstrap_features': True, 'clf__max_features': 0.9867872743739655, 'clf__max_samples': 0.7760959526282329, 'clf__n_estimators': 224, 'clf__oob_score': True, 'clf__warm_start': True, 'vect__max_df': 0.5888888888888889, 'vect__min_df': 0.0004155394657802238, 'vect__ngram_range': (1, 1), 'vect__stop_words': 'english', 'vect__token_pattern': '\\w{2,}'}, {'clf__base_estimator': Pipeline(memory=None,
         steps=[('vect',
                 TfidfVectorizer(analyzer='word', binary=False,
                                 decode_error='strict',
                                 dtype=<class 'numpy.float64'>,
                                 encoding='utf-8', input='content',
                                 lowercase=True, max_df=0.6777777777777778,
                                 max_features=None,
                                 min_df=0.0012707570261625706,
                                 ngram_range=(1, 2), norm='l2',
                                 preprocessor=None, smooth_idf=True,
                                 stop_words=None, strip_accents=None,...
                                 vocabulary=None)),
                ('clf',
                 DecisionTreeClassifier(ccp_alpha=0.0, class_weight=None,
                                        criterion='gini', max_depth=49,
                                        max_features=0.56, max_leaf_nodes=316,
                                        min_impurity_decrease=1e-07,
                                        min_impurity_split=None,
                                        min_samples_leaf=4,
                                        min_samples_split=0.015198715678453873,
                                        min_weight_fraction_leaf=0.0,
                                        presort='deprecated', random_state=0,
                                        splitter='random'))],
         verbose=False), 'clf__bootstrap': True, 'clf__bootstrap_features': False, 'clf__max_features': 0.7102830777566337, 'clf__max_samples': 0.6298283816488258, 'clf__n_estimators': 62, 'clf__oob_score': True, 'clf__warm_start': True, 'vect__max_df': 0.6777777777777778, 'vect__min_df': 0.0004465185289583982, 'vect__ngram_range': (1, 2), 'vect__stop_words': None, 'vect__token_pattern': '\\w{1,}'}, {'clf__base_estimator': Pipeline(memory=None,
         steps=[('vect',
                 TfidfVectorizer(analyzer='word', binary=False,
                                 decode_error='strict',
                                 dtype=<class 'numpy.float64'>,
                                 encoding='utf-8', input='content',
                                 lowercase=True, max_df=0.6333333333333333,
                                 max_features=None,
                                 min_df=0.0010083260374073057,
                                 ngram_range=(1, 1), norm='l2',
                                 preprocessor=None, smooth_idf=True,
                                 stop_words='english', strip_accents=...
                                 tokenizer=None, use_idf=True,
                                 vocabulary=None)),
                ('clf',
                 AdaBoostClassifier(algorithm='SAMME',
                                    base_estimator=LinearSVC(C=1.0,
                                                             class_weight=None,
                                                             dual=True,
                                                             fit_intercept=True,
                                                             intercept_scaling=1,
                                                             loss='squared_hinge',
                                                             max_iter=1000,
                                                             multi_class='ovr',
                                                             penalty='l2',
                                                             random_state=None,
                                                             tol=0.0001,
                                                             verbose=0),
                                    learning_rate=0.40053644159285917,
                                    n_estimators=190, random_state=0))],
         verbose=False), 'clf__bootstrap': True, 'clf__bootstrap_features': False, 'clf__max_features': 0.6766866162111446, 'clf__max_samples': 0.504557234462983, 'clf__n_estimators': 22, 'clf__oob_score': False, 'clf__warm_start': False, 'vect__max_df': 0.7222222222222222, 'vect__min_df': 0.00022796749450944482, 'vect__ngram_range': (1, 2), 'vect__stop_words': 'english', 'vect__token_pattern': '\\w{1,}'}, {'clf__base_estimator': None, 'clf__bootstrap': True, 'clf__bootstrap_features': True, 'clf__max_features': 0.3832492030551987, 'clf__max_samples': 0.17044675051988067, 'clf__n_estimators': 279, 'clf__oob_score': True, 'clf__warm_start': True, 'vect__max_df': 0.5888888888888889, 'vect__min_df': 0.0002179771330414835, 'vect__ngram_range': (1, 1), 'vect__stop_words': None, 'vect__token_pattern': '\\w{2,}'}, {'clf__base_estimator': Pipeline(memory=None,
         steps=[('vect',
                 TfidfVectorizer(analyzer='word', binary=False,
                                 decode_error='strict',
                                 dtype=<class 'numpy.float64'>,
                                 encoding='utf-8', input='content',
                                 lowercase=True, max_df=0.7666666666666666,
                                 max_features=None,
                                 min_df=0.00010572508827796376,
                                 ngram_range=(1, 2), norm='l2',
                                 preprocessor=None, smooth_idf=True,
                                 stop_words='english', strip_accents=None,
                                 sublinear_tf=False, token_pattern='\\w{1,}',
                                 tokenizer=None, use_idf=True,
                                 vocabulary=None)),
                ('clf',
                 LinearSVC(C=99.37702864845689, class_weight=None, dual=False,
                           fit_intercept=False, intercept_scaling=1,
                           loss='squared_hinge', max_iter=1000,
                           multi_class='ovr', penalty='l2', random_state=0,
                           tol=3.8359242531998225e-06, verbose=0))],
         verbose=False), 'clf__bootstrap': True, 'clf__bootstrap_features': True, 'clf__max_features': 0.3208357572921381, 'clf__max_samples': 0.2271161672886154, 'clf__n_estimators': 169, 'clf__oob_score': True, 'clf__warm_start': True, 'vect__max_df': 0.8111111111111111, 'vect__min_df': 0.00223152980203543, 'vect__ngram_range': (1, 1), 'vect__stop_words': 'english', 'vect__token_pattern': '\\w{1,}'}, {'clf__base_estimator': None, 'clf__bootstrap': False, 'clf__bootstrap_features': True, 'clf__max_features': 0.25629116388267914, 'clf__max_samples': 0.5811305728842577, 'clf__n_estimators': 322, 'clf__oob_score': False, 'clf__warm_start': True, 'vect__max_df': 0.7666666666666666, 'vect__min_df': 0.0029140391892371613, 'vect__ngram_range': (1, 1), 'vect__stop_words': None, 'vect__token_pattern': '\\w{2,}'}, {'clf__base_estimator': None, 'clf__bootstrap': False, 'clf__bootstrap_features': False, 'clf__max_features': 0.5121798562061132, 'clf__max_samples': 0.6038420250659001, 'clf__n_estimators': 331, 'clf__oob_score': True, 'clf__warm_start': True, 'vect__max_df': 0.5888888888888889, 'vect__min_df': 0.0034539434995552493, 'vect__ngram_range': (1, 1), 'vect__stop_words': 'english', 'vect__token_pattern': '\\w{2,}'}, {'clf__base_estimator': None, 'clf__bootstrap': False, 'clf__bootstrap_features': True, 'clf__max_features': 0.2730893536359167, 'clf__max_samples': 0.5398046144481868, 'clf__n_estimators': 109, 'clf__oob_score': True, 'clf__warm_start': True, 'vect__max_df': 0.5444444444444444, 'vect__min_df': 0.0002935459794421058, 'vect__ngram_range': (1, 1), 'vect__stop_words': None, 'vect__token_pattern': '\\w{1,}'}, {'clf__base_estimator': Pipeline(memory=None,
         steps=[('vect',
                 TfidfVectorizer(analyzer='word', binary=False,
                                 decode_error='strict',
                                 dtype=<class 'numpy.float64'>,
                                 encoding='utf-8', input='content',
                                 lowercase=True, max_df=0.7666666666666666,
                                 max_features=None,
                                 min_df=0.00010572508827796376,
                                 ngram_range=(1, 2), norm='l2',
                                 preprocessor=None, smooth_idf=True,
                                 stop_words='english', strip_accents=None,
                                 sublinear_tf=False, token_pattern='\\w{1,}',
                                 tokenizer=None, use_idf=True,
                                 vocabulary=None)),
                ('clf',
                 LinearSVC(C=99.37702864845689, class_weight=None, dual=False,
                           fit_intercept=False, intercept_scaling=1,
                           loss='squared_hinge', max_iter=1000,
                           multi_class='ovr', penalty='l2', random_state=0,
                           tol=3.8359242531998225e-06, verbose=0))],
         verbose=False), 'clf__bootstrap': False, 'clf__bootstrap_features': True, 'clf__max_features': 0.4455065984560159, 'clf__max_samples': 0.22089896507423956, 'clf__n_estimators': 108, 'clf__oob_score': True, 'clf__warm_start': False, 'vect__max_df': 0.9, 'vect__min_df': 0.002190870002485152, 'vect__ngram_range': (1, 2), 'vect__stop_words': 'english', 'vect__token_pattern': '\\w{2,}'}, {'clf__base_estimator': None, 'clf__bootstrap': False, 'clf__bootstrap_features': True, 'clf__max_features': 0.7366223518910722, 'clf__max_samples': 0.3051935497101078, 'clf__n_estimators': 120, 'clf__oob_score': True, 'clf__warm_start': True, 'vect__max_df': 0.5, 'vect__min_df': 0.0003291500480486555, 'vect__ngram_range': (1, 2), 'vect__stop_words': 'english', 'vect__token_pattern': '\\w{1,}'}, {'clf__base_estimator': MultinomialNB(alpha=1.0, class_prior=None, fit_prior=True), 'clf__bootstrap': False, 'clf__bootstrap_features': True, 'clf__max_features': 0.151452389878599, 'clf__max_samples': 0.40181354789084767, 'clf__n_estimators': 156, 'clf__oob_score': True, 'clf__warm_start': False, 'vect__max_df': 0.7222222222222222, 'vect__min_df': 0.0001624870226360064, 'vect__ngram_range': (1, 2), 'vect__stop_words': 'english', 'vect__token_pattern': '\\w{1,}'}, {'clf__base_estimator': MultinomialNB(alpha=1.0, class_prior=None, fit_prior=True), 'clf__bootstrap': True, 'clf__bootstrap_features': True, 'clf__max_features': 0.027759669235459472, 'clf__max_samples': 0.96603655740548, 'clf__n_estimators': 95, 'clf__oob_score': False, 'clf__warm_start': False, 'vect__max_df': 0.7666666666666666, 'vect__min_df': 0.000501393193226579, 'vect__ngram_range': (1, 2), 'vect__stop_words': None, 'vect__token_pattern': '\\w{1,}'}, {'clf__base_estimator': Pipeline(memory=None,
         steps=[('vect',
                 TfidfVectorizer(analyzer='word', binary=False,
                                 decode_error='strict',
                                 dtype=<class 'numpy.float64'>,
                                 encoding='utf-8', input='content',
                                 lowercase=True, max_df=0.6333333333333333,
                                 max_features=None,
                                 min_df=0.0010083260374073057,
                                 ngram_range=(1, 1), norm='l2',
                                 preprocessor=None, smooth_idf=True,
                                 stop_words='english', strip_accents=...
                                 tokenizer=None, use_idf=True,
                                 vocabulary=None)),
                ('clf',
                 AdaBoostClassifier(algorithm='SAMME',
                                    base_estimator=LinearSVC(C=1.0,
                                                             class_weight=None,
                                                             dual=True,
                                                             fit_intercept=True,
                                                             intercept_scaling=1,
                                                             loss='squared_hinge',
                                                             max_iter=1000,
                                                             multi_class='ovr',
                                                             penalty='l2',
                                                             random_state=None,
                                                             tol=0.0001,
                                                             verbose=0),
                                    learning_rate=0.40053644159285917,
                                    n_estimators=190, random_state=0))],
         verbose=False), 'clf__bootstrap': False, 'clf__bootstrap_features': False, 'clf__max_features': 0.6908098720302277, 'clf__max_samples': 0.8785934455288432, 'clf__n_estimators': 181, 'clf__oob_score': False, 'clf__warm_start': True, 'vect__max_df': 0.6777777777777778, 'vect__min_df': 0.005727546994389818, 'vect__ngram_range': (1, 1), 'vect__stop_words': None, 'vect__token_pattern': '\\w{1,}'}, {'clf__base_estimator': MultinomialNB(alpha=1.0, class_prior=None, fit_prior=True), 'clf__bootstrap': False, 'clf__bootstrap_features': True, 'clf__max_features': 0.8005217670598533, 'clf__max_samples': 0.8184205630546396, 'clf__n_estimators': 346, 'clf__oob_score': False, 'clf__warm_start': False, 'vect__max_df': 0.8111111111111111, 'vect__min_df': 0.004897717319979126, 'vect__ngram_range': (1, 1), 'vect__stop_words': None, 'vect__token_pattern': '\\w{1,}'}, {'clf__base_estimator': Pipeline(memory=None,
         steps=[('vect',
                 TfidfVectorizer(analyzer='word', binary=False,
                                 decode_error='strict',
                                 dtype=<class 'numpy.float64'>,
                                 encoding='utf-8', input='content',
                                 lowercase=True, max_df=0.6333333333333333,
                                 max_features=None,
                                 min_df=0.0010083260374073057,
                                 ngram_range=(1, 1), norm='l2',
                                 preprocessor=None, smooth_idf=True,
                                 stop_words='english', strip_accents=...
                                 tokenizer=None, use_idf=True,
                                 vocabulary=None)),
                ('clf',
                 AdaBoostClassifier(algorithm='SAMME',
                                    base_estimator=LinearSVC(C=1.0,
                                                             class_weight=None,
                                                             dual=True,
                                                             fit_intercept=True,
                                                             intercept_scaling=1,
                                                             loss='squared_hinge',
                                                             max_iter=1000,
                                                             multi_class='ovr',
                                                             penalty='l2',
                                                             random_state=None,
                                                             tol=0.0001,
                                                             verbose=0),
                                    learning_rate=0.40053644159285917,
                                    n_estimators=190, random_state=0))],
         verbose=False), 'clf__bootstrap': False, 'clf__bootstrap_features': False, 'clf__max_features': 0.587391438566732, 'clf__max_samples': 0.49608767756915506, 'clf__n_estimators': 327, 'clf__oob_score': False, 'clf__warm_start': True, 'vect__max_df': 0.5, 'vect__min_df': 0.00028564644614490625, 'vect__ngram_range': (1, 2), 'vect__stop_words': 'english', 'vect__token_pattern': '\\w{1,}'}, {'clf__base_estimator': Pipeline(memory=None,
         steps=[('vect',
                 TfidfVectorizer(analyzer='word', binary=False,
                                 decode_error='strict',
                                 dtype=<class 'numpy.float64'>,
                                 encoding='utf-8', input='content',
                                 lowercase=True, max_df=0.6777777777777778,
                                 max_features=None,
                                 min_df=0.0012707570261625706,
                                 ngram_range=(1, 2), norm='l2',
                                 preprocessor=None, smooth_idf=True,
                                 stop_words=None, strip_accents=None,...
                                 vocabulary=None)),
                ('clf',
                 DecisionTreeClassifier(ccp_alpha=0.0, class_weight=None,
                                        criterion='gini', max_depth=49,
                                        max_features=0.56, max_leaf_nodes=316,
                                        min_impurity_decrease=1e-07,
                                        min_impurity_split=None,
                                        min_samples_leaf=4,
                                        min_samples_split=0.015198715678453873,
                                        min_weight_fraction_leaf=0.0,
                                        presort='deprecated', random_state=0,
                                        splitter='random'))],
         verbose=False), 'clf__bootstrap': True, 'clf__bootstrap_features': True, 'clf__max_features': 0.9635043441286752, 'clf__max_samples': 0.4115411324141588, 'clf__n_estimators': 259, 'clf__oob_score': True, 'clf__warm_start': False, 'vect__max_df': 0.9, 'vect__min_df': 0.0025895745931076627, 'vect__ngram_range': (1, 2), 'vect__stop_words': None, 'vect__token_pattern': '\\w{2,}'}, {'clf__base_estimator': Pipeline(memory=None,
         steps=[('vect',
                 TfidfVectorizer(analyzer='word', binary=False,
                                 decode_error='strict',
                                 dtype=<class 'numpy.float64'>,
                                 encoding='utf-8', input='content',
                                 lowercase=True, max_df=0.7222222222222222,
                                 max_features=None,
                                 min_df=0.00014705642569160768,
                                 ngram_range=(1, 1), norm='l2',
                                 preprocessor=None, smooth_idf=True,
                                 stop_words=None, strip_accents=None...
                                        max_features=0.01, max_leaf_nodes=None,
                                        max_samples=0.007808728196651904,
                                        min_impurity_decrease=3.1622776601683795e-10,
                                        min_impurity_split=None,
                                        min_samples_leaf=62,
                                        min_samples_split=0.0008966659727253451,
                                        min_weight_fraction_leaf=0.0007448459487372297,
                                        n_estimators=251, n_jobs=None,
                                        oob_score=False, random_state=0,
                                        verbose=True, warm_start=True))],
         verbose=False), 'clf__bootstrap': True, 'clf__bootstrap_features': False, 'clf__max_features': 0.10559982955473901, 'clf__max_samples': 0.07520421819798329, 'clf__n_estimators': 31, 'clf__oob_score': False, 'clf__warm_start': False, 'vect__max_df': 0.9, 'vect__min_df': 0.00039207508208072734, 'vect__ngram_range': (1, 2), 'vect__stop_words': 'english', 'vect__token_pattern': '\\w{1,}'}, {'clf__base_estimator': Pipeline(memory=None,
         steps=[('vect',
                 TfidfVectorizer(analyzer='word', binary=False,
                                 decode_error='strict',
                                 dtype=<class 'numpy.float64'>,
                                 encoding='utf-8', input='content',
                                 lowercase=True, max_df=0.7666666666666666,
                                 max_features=None,
                                 min_df=0.00010572508827796376,
                                 ngram_range=(1, 2), norm='l2',
                                 preprocessor=None, smooth_idf=True,
                                 stop_words='english', strip_accents=None,
                                 sublinear_tf=False, token_pattern='\\w{1,}',
                                 tokenizer=None, use_idf=True,
                                 vocabulary=None)),
                ('clf',
                 LinearSVC(C=99.37702864845689, class_weight=None, dual=False,
                           fit_intercept=False, intercept_scaling=1,
                           loss='squared_hinge', max_iter=1000,
                           multi_class='ovr', penalty='l2', random_state=0,
                           tol=3.8359242531998225e-06, verbose=0))],
         verbose=False), 'clf__bootstrap': False, 'clf__bootstrap_features': True, 'clf__max_features': 0.7661370927239434, 'clf__max_samples': 0.6681498229166721, 'clf__n_estimators': 205, 'clf__oob_score': False, 'clf__warm_start': False, 'vect__max_df': 0.8555555555555556, 'vect__min_df': 0.0013472353657205279, 'vect__ngram_range': (1, 1), 'vect__stop_words': None, 'vect__token_pattern': '\\w{1,}'}, {'clf__base_estimator': Pipeline(memory=None,
         steps=[('vect',
                 TfidfVectorizer(analyzer='word', binary=False,
                                 decode_error='strict',
                                 dtype=<class 'numpy.float64'>,
                                 encoding='utf-8', input='content',
                                 lowercase=True, max_df=0.6333333333333333,
                                 max_features=None,
                                 min_df=0.0010083260374073057,
                                 ngram_range=(1, 1), norm='l2',
                                 preprocessor=None, smooth_idf=True,
                                 stop_words='english', strip_accents=...
                                 tokenizer=None, use_idf=True,
                                 vocabulary=None)),
                ('clf',
                 AdaBoostClassifier(algorithm='SAMME',
                                    base_estimator=LinearSVC(C=1.0,
                                                             class_weight=None,
                                                             dual=True,
                                                             fit_intercept=True,
                                                             intercept_scaling=1,
                                                             loss='squared_hinge',
                                                             max_iter=1000,
                                                             multi_class='ovr',
                                                             penalty='l2',
                                                             random_state=None,
                                                             tol=0.0001,
                                                             verbose=0),
                                    learning_rate=0.40053644159285917,
                                    n_estimators=190, random_state=0))],
         verbose=False), 'clf__bootstrap': False, 'clf__bootstrap_features': True, 'clf__max_features': 0.44835000500279676, 'clf__max_samples': 0.6646105446978837, 'clf__n_estimators': 347, 'clf__oob_score': False, 'clf__warm_start': True, 'vect__max_df': 0.9, 'vect__min_df': 0.0002250241561866282, 'vect__ngram_range': (1, 1), 'vect__stop_words': 'english', 'vect__token_pattern': '\\w{2,}'}, {'clf__base_estimator': MultinomialNB(alpha=1.0, class_prior=None, fit_prior=True), 'clf__bootstrap': True, 'clf__bootstrap_features': True, 'clf__max_features': 0.45847923467041496, 'clf__max_samples': 0.500718691417731, 'clf__n_estimators': 131, 'clf__oob_score': True, 'clf__warm_start': True, 'vect__max_df': 0.7666666666666666, 'vect__min_df': 0.004214472271523116, 'vect__ngram_range': (1, 2), 'vect__stop_words': 'english', 'vect__token_pattern': '\\w{2,}'}, {'clf__base_estimator': Pipeline(memory=None,
         steps=[('vect',
                 TfidfVectorizer(analyzer='word', binary=False,
                                 decode_error='strict',
                                 dtype=<class 'numpy.float64'>,
                                 encoding='utf-8', input='content',
                                 lowercase=True, max_df=0.6777777777777778,
                                 max_features=None,
                                 min_df=0.0012707570261625706,
                                 ngram_range=(1, 2), norm='l2',
                                 preprocessor=None, smooth_idf=True,
                                 stop_words=None, strip_accents=None,...
                                 vocabulary=None)),
                ('clf',
                 DecisionTreeClassifier(ccp_alpha=0.0, class_weight=None,
                                        criterion='gini', max_depth=49,
                                        max_features=0.56, max_leaf_nodes=316,
                                        min_impurity_decrease=1e-07,
                                        min_impurity_split=None,
                                        min_samples_leaf=4,
                                        min_samples_split=0.015198715678453873,
                                        min_weight_fraction_leaf=0.0,
                                        presort='deprecated', random_state=0,
                                        splitter='random'))],
         verbose=False), 'clf__bootstrap': True, 'clf__bootstrap_features': False, 'clf__max_features': 0.975599621470175, 'clf__max_samples': 0.4810484923142846, 'clf__n_estimators': 202, 'clf__oob_score': False, 'clf__warm_start': True, 'vect__max_df': 0.5888888888888889, 'vect__min_df': 0.00038803308003453486, 'vect__ngram_range': (1, 2), 'vect__stop_words': 'english', 'vect__token_pattern': '\\w{2,}'}, {'clf__base_estimator': MultinomialNB(alpha=1.0, class_prior=None, fit_prior=True), 'clf__bootstrap': True, 'clf__bootstrap_features': False, 'clf__max_features': 0.978099744278984, 'clf__max_samples': 0.27356893723065945, 'clf__n_estimators': 118, 'clf__oob_score': False, 'clf__warm_start': False, 'vect__max_df': 0.5888888888888889, 'vect__min_df': 0.0008287171043350852, 'vect__ngram_range': (1, 1), 'vect__stop_words': None, 'vect__token_pattern': '\\w{1,}'}, {'clf__base_estimator': Pipeline(memory=None,
         steps=[('vect',
                 TfidfVectorizer(analyzer='word', binary=False,
                                 decode_error='strict',
                                 dtype=<class 'numpy.float64'>,
                                 encoding='utf-8', input='content',
                                 lowercase=True, max_df=0.6777777777777778,
                                 max_features=None,
                                 min_df=0.0012707570261625706,
                                 ngram_range=(1, 2), norm='l2',
                                 preprocessor=None, smooth_idf=True,
                                 stop_words=None, strip_accents=None,...
                                 vocabulary=None)),
                ('clf',
                 DecisionTreeClassifier(ccp_alpha=0.0, class_weight=None,
                                        criterion='gini', max_depth=49,
                                        max_features=0.56, max_leaf_nodes=316,
                                        min_impurity_decrease=1e-07,
                                        min_impurity_split=None,
                                        min_samples_leaf=4,
                                        min_samples_split=0.015198715678453873,
                                        min_weight_fraction_leaf=0.0,
                                        presort='deprecated', random_state=0,
                                        splitter='random'))],
         verbose=False), 'clf__bootstrap': False, 'clf__bootstrap_features': False, 'clf__max_features': 0.5927072780383205, 'clf__max_samples': 0.5701921655306, 'clf__n_estimators': 357, 'clf__oob_score': True, 'clf__warm_start': True, 'vect__max_df': 0.5888888888888889, 'vect__min_df': 0.0005530373739804331, 'vect__ngram_range': (1, 2), 'vect__stop_words': None, 'vect__token_pattern': '\\w{2,}'}, {'clf__base_estimator': None, 'clf__bootstrap': False, 'clf__bootstrap_features': True, 'clf__max_features': 0.9997654954939011, 'clf__max_samples': 0.13206277631704466, 'clf__n_estimators': 359, 'clf__oob_score': True, 'clf__warm_start': True, 'vect__max_df': 0.5, 'vect__min_df': 0.0001513802744815044, 'vect__ngram_range': (1, 1), 'vect__stop_words': None, 'vect__token_pattern': '\\w{1,}'}, {'clf__base_estimator': Pipeline(memory=None,
         steps=[('vect',
                 TfidfVectorizer(analyzer='word', binary=False,
                                 decode_error='strict',
                                 dtype=<class 'numpy.float64'>,
                                 encoding='utf-8', input='content',
                                 lowercase=True, max_df=0.8555555555555556,
                                 max_features=None,
                                 min_df=0.00016483617040605902,
                                 ngram_range=(1, 2), norm='l2',
                                 preprocessor=None, smooth_idf=True,
                                 stop_words='english', strip_accents...
                                 tokenizer=None, use_idf=True,
                                 vocabulary=None)),
                ('clf',
                 LogisticRegression(C=0.47946121772409134, class_weight=None,
                                    dual=False, fit_intercept=True,
                                    intercept_scaling=1,
                                    l1_ratio=0.9168495407673116, max_iter=75,
                                    multi_class='multinomial', n_jobs=None,
                                    penalty='none', random_state=0,
                                    solver='sag', tol=6.514227791666363e-07,
                                    verbose=0, warm_start=False))],
         verbose=False), 'clf__bootstrap': True, 'clf__bootstrap_features': False, 'clf__max_features': 0.43779661375638634, 'clf__max_samples': 0.010096817993019314, 'clf__n_estimators': 233, 'clf__oob_score': True, 'clf__warm_start': True, 'vect__max_df': 0.5444444444444444, 'vect__min_df': 0.005598766364727352, 'vect__ngram_range': (1, 1), 'vect__stop_words': 'english', 'vect__token_pattern': '\\w{2,}'}, {'clf__base_estimator': MultinomialNB(alpha=1.0, class_prior=None, fit_prior=True), 'clf__bootstrap': False, 'clf__bootstrap_features': True, 'clf__max_features': 0.9216901742057149, 'clf__max_samples': 0.4487298713740224, 'clf__n_estimators': 80, 'clf__oob_score': True, 'clf__warm_start': False, 'vect__max_df': 0.6777777777777778, 'vect__min_df': 0.004419929762285939, 'vect__ngram_range': (1, 2), 'vect__stop_words': None, 'vect__token_pattern': '\\w{2,}'}, {'clf__base_estimator': Pipeline(memory=None,
         steps=[('vect',
                 TfidfVectorizer(analyzer='word', binary=False,
                                 decode_error='strict',
                                 dtype=<class 'numpy.float64'>,
                                 encoding='utf-8', input='content',
                                 lowercase=True, max_df=0.6333333333333333,
                                 max_features=None,
                                 min_df=0.0010083260374073057,
                                 ngram_range=(1, 1), norm='l2',
                                 preprocessor=None, smooth_idf=True,
                                 stop_words='english', strip_accents=...
                                 tokenizer=None, use_idf=True,
                                 vocabulary=None)),
                ('clf',
                 AdaBoostClassifier(algorithm='SAMME',
                                    base_estimator=LinearSVC(C=1.0,
                                                             class_weight=None,
                                                             dual=True,
                                                             fit_intercept=True,
                                                             intercept_scaling=1,
                                                             loss='squared_hinge',
                                                             max_iter=1000,
                                                             multi_class='ovr',
                                                             penalty='l2',
                                                             random_state=None,
                                                             tol=0.0001,
                                                             verbose=0),
                                    learning_rate=0.40053644159285917,
                                    n_estimators=190, random_state=0))],
         verbose=False), 'clf__bootstrap': False, 'clf__bootstrap_features': True, 'clf__max_features': 0.18049796499991688, 'clf__max_samples': 0.11256828259155949, 'clf__n_estimators': 44, 'clf__oob_score': False, 'clf__warm_start': False, 'vect__max_df': 0.7666666666666666, 'vect__min_df': 0.0012245787062838288, 'vect__ngram_range': (1, 1), 'vect__stop_words': 'english', 'vect__token_pattern': '\\w{2,}'}, {'clf__base_estimator': Pipeline(memory=None,
         steps=[('vect',
                 TfidfVectorizer(analyzer='word', binary=False,
                                 decode_error='strict',
                                 dtype=<class 'numpy.float64'>,
                                 encoding='utf-8', input='content',
                                 lowercase=True, max_df=0.6777777777777778,
                                 max_features=None,
                                 min_df=0.0012707570261625706,
                                 ngram_range=(1, 2), norm='l2',
                                 preprocessor=None, smooth_idf=True,
                                 stop_words=None, strip_accents=None,...
                                 vocabulary=None)),
                ('clf',
                 DecisionTreeClassifier(ccp_alpha=0.0, class_weight=None,
                                        criterion='gini', max_depth=49,
                                        max_features=0.56, max_leaf_nodes=316,
                                        min_impurity_decrease=1e-07,
                                        min_impurity_split=None,
                                        min_samples_leaf=4,
                                        min_samples_split=0.015198715678453873,
                                        min_weight_fraction_leaf=0.0,
                                        presort='deprecated', random_state=0,
                                        splitter='random'))],
         verbose=False), 'clf__bootstrap': False, 'clf__bootstrap_features': False, 'clf__max_features': 0.03246607871210283, 'clf__max_samples': 0.11955216962131432, 'clf__n_estimators': 86, 'clf__oob_score': True, 'clf__warm_start': False, 'vect__max_df': 0.8111111111111111, 'vect__min_df': 0.002713421888996022, 'vect__ngram_range': (1, 2), 'vect__stop_words': None, 'vect__token_pattern': '\\w{2,}'}, {'clf__base_estimator': Pipeline(memory=None,
         steps=[('vect',
                 TfidfVectorizer(analyzer='word', binary=False,
                                 decode_error='strict',
                                 dtype=<class 'numpy.float64'>,
                                 encoding='utf-8', input='content',
                                 lowercase=True, max_df=0.6333333333333333,
                                 max_features=None,
                                 min_df=0.0010083260374073057,
                                 ngram_range=(1, 1), norm='l2',
                                 preprocessor=None, smooth_idf=True,
                                 stop_words='english', strip_accents=...
                                 tokenizer=None, use_idf=True,
                                 vocabulary=None)),
                ('clf',
                 AdaBoostClassifier(algorithm='SAMME',
                                    base_estimator=LinearSVC(C=1.0,
                                                             class_weight=None,
                                                             dual=True,
                                                             fit_intercept=True,
                                                             intercept_scaling=1,
                                                             loss='squared_hinge',
                                                             max_iter=1000,
                                                             multi_class='ovr',
                                                             penalty='l2',
                                                             random_state=None,
                                                             tol=0.0001,
                                                             verbose=0),
                                    learning_rate=0.40053644159285917,
                                    n_estimators=190, random_state=0))],
         verbose=False), 'clf__bootstrap': True, 'clf__bootstrap_features': False, 'clf__max_features': 0.5178669638726272, 'clf__max_samples': 0.06882657482202359, 'clf__n_estimators': 31, 'clf__oob_score': False, 'clf__warm_start': True, 'vect__max_df': 0.5888888888888889, 'vect__min_df': 0.006880988055417675, 'vect__ngram_range': (1, 2), 'vect__stop_words': 'english', 'vect__token_pattern': '\\w{2,}'}, {'clf__base_estimator': MultinomialNB(alpha=1.0, class_prior=None, fit_prior=True), 'clf__bootstrap': False, 'clf__bootstrap_features': False, 'clf__max_features': 0.04224561463328125, 'clf__max_samples': 0.2741235511865233, 'clf__n_estimators': 134, 'clf__oob_score': True, 'clf__warm_start': False, 'vect__max_df': 0.5444444444444444, 'vect__min_df': 0.00044294358808228686, 'vect__ngram_range': (1, 2), 'vect__stop_words': None, 'vect__token_pattern': '\\w{2,}'}, {'clf__base_estimator': Pipeline(memory=None,
         steps=[('vect',
                 TfidfVectorizer(analyzer='word', binary=False,
                                 decode_error='strict',
                                 dtype=<class 'numpy.float64'>,
                                 encoding='utf-8', input='content',
                                 lowercase=True, max_df=0.7666666666666666,
                                 max_features=None,
                                 min_df=0.00010572508827796376,
                                 ngram_range=(1, 2), norm='l2',
                                 preprocessor=None, smooth_idf=True,
                                 stop_words='english', strip_accents=None,
                                 sublinear_tf=False, token_pattern='\\w{1,}',
                                 tokenizer=None, use_idf=True,
                                 vocabulary=None)),
                ('clf',
                 LinearSVC(C=99.37702864845689, class_weight=None, dual=False,
                           fit_intercept=False, intercept_scaling=1,
                           loss='squared_hinge', max_iter=1000,
                           multi_class='ovr', penalty='l2', random_state=0,
                           tol=3.8359242531998225e-06, verbose=0))],
         verbose=False), 'clf__bootstrap': True, 'clf__bootstrap_features': True, 'clf__max_features': 0.4989348037760941, 'clf__max_samples': 0.33259806240232637, 'clf__n_estimators': 203, 'clf__oob_score': True, 'clf__warm_start': True, 'vect__max_df': 0.5444444444444444, 'vect__min_df': 0.00020962090241963318, 'vect__ngram_range': (1, 1), 'vect__stop_words': None, 'vect__token_pattern': '\\w{2,}'}, {'clf__base_estimator': Pipeline(memory=None,
         steps=[('vect',
                 TfidfVectorizer(analyzer='word', binary=False,
                                 decode_error='strict',
                                 dtype=<class 'numpy.float64'>,
                                 encoding='utf-8', input='content',
                                 lowercase=True, max_df=0.7222222222222222,
                                 max_features=None,
                                 min_df=0.00014705642569160768,
                                 ngram_range=(1, 1), norm='l2',
                                 preprocessor=None, smooth_idf=True,
                                 stop_words=None, strip_accents=None...
                                        max_features=0.01, max_leaf_nodes=None,
                                        max_samples=0.007808728196651904,
                                        min_impurity_decrease=3.1622776601683795e-10,
                                        min_impurity_split=None,
                                        min_samples_leaf=62,
                                        min_samples_split=0.0008966659727253451,
                                        min_weight_fraction_leaf=0.0007448459487372297,
                                        n_estimators=251, n_jobs=None,
                                        oob_score=False, random_state=0,
                                        verbose=True, warm_start=True))],
         verbose=False), 'clf__bootstrap': True, 'clf__bootstrap_features': True, 'clf__max_features': 0.14108123148074647, 'clf__max_samples': 0.749260650702883, 'clf__n_estimators': 213, 'clf__oob_score': True, 'clf__warm_start': False, 'vect__max_df': 0.5, 'vect__min_df': 0.000593739604302443, 'vect__ngram_range': (1, 1), 'vect__stop_words': 'english', 'vect__token_pattern': '\\w{1,}'}, {'clf__base_estimator': Pipeline(memory=None,
         steps=[('vect',
                 TfidfVectorizer(analyzer='word', binary=False,
                                 decode_error='strict',
                                 dtype=<class 'numpy.float64'>,
                                 encoding='utf-8', input='content',
                                 lowercase=True, max_df=0.6333333333333333,
                                 max_features=None,
                                 min_df=0.0010083260374073057,
                                 ngram_range=(1, 1), norm='l2',
                                 preprocessor=None, smooth_idf=True,
                                 stop_words='english', strip_accents=...
                                 tokenizer=None, use_idf=True,
                                 vocabulary=None)),
                ('clf',
                 AdaBoostClassifier(algorithm='SAMME',
                                    base_estimator=LinearSVC(C=1.0,
                                                             class_weight=None,
                                                             dual=True,
                                                             fit_intercept=True,
                                                             intercept_scaling=1,
                                                             loss='squared_hinge',
                                                             max_iter=1000,
                                                             multi_class='ovr',
                                                             penalty='l2',
                                                             random_state=None,
                                                             tol=0.0001,
                                                             verbose=0),
                                    learning_rate=0.40053644159285917,
                                    n_estimators=190, random_state=0))],
         verbose=False), 'clf__bootstrap': False, 'clf__bootstrap_features': True, 'clf__max_features': 0.11885017508589724, 'clf__max_samples': 0.8538423290162976, 'clf__n_estimators': 307, 'clf__oob_score': False, 'clf__warm_start': False, 'vect__max_df': 0.5444444444444444, 'vect__min_df': 0.00020380309208005223, 'vect__ngram_range': (1, 2), 'vect__stop_words': None, 'vect__token_pattern': '\\w{2,}'}, {'clf__base_estimator': Pipeline(memory=None,
         steps=[('vect',
                 TfidfVectorizer(analyzer='word', binary=False,
                                 decode_error='strict',
                                 dtype=<class 'numpy.float64'>,
                                 encoding='utf-8', input='content',
                                 lowercase=True, max_df=0.7666666666666666,
                                 max_features=None,
                                 min_df=0.00010572508827796376,
                                 ngram_range=(1, 2), norm='l2',
                                 preprocessor=None, smooth_idf=True,
                                 stop_words='english', strip_accents=None,
                                 sublinear_tf=False, token_pattern='\\w{1,}',
                                 tokenizer=None, use_idf=True,
                                 vocabulary=None)),
                ('clf',
                 LinearSVC(C=99.37702864845689, class_weight=None, dual=False,
                           fit_intercept=False, intercept_scaling=1,
                           loss='squared_hinge', max_iter=1000,
                           multi_class='ovr', penalty='l2', random_state=0,
                           tol=3.8359242531998225e-06, verbose=0))],
         verbose=False), 'clf__bootstrap': True, 'clf__bootstrap_features': True, 'clf__max_features': 0.472945975317727, 'clf__max_samples': 0.31760660238217986, 'clf__n_estimators': 391, 'clf__oob_score': False, 'clf__warm_start': False, 'vect__max_df': 0.8555555555555556, 'vect__min_df': 0.009502988782348575, 'vect__ngram_range': (1, 1), 'vect__stop_words': 'english', 'vect__token_pattern': '\\w{1,}'}, {'clf__base_estimator': Pipeline(memory=None,
         steps=[('vect',
                 TfidfVectorizer(analyzer='word', binary=False,
                                 decode_error='strict',
                                 dtype=<class 'numpy.float64'>,
                                 encoding='utf-8', input='content',
                                 lowercase=True, max_df=0.6333333333333333,
                                 max_features=None,
                                 min_df=0.0010083260374073057,
                                 ngram_range=(1, 1), norm='l2',
                                 preprocessor=None, smooth_idf=True,
                                 stop_words='english', strip_accents=...
                                 tokenizer=None, use_idf=True,
                                 vocabulary=None)),
                ('clf',
                 AdaBoostClassifier(algorithm='SAMME',
                                    base_estimator=LinearSVC(C=1.0,
                                                             class_weight=None,
                                                             dual=True,
                                                             fit_intercept=True,
                                                             intercept_scaling=1,
                                                             loss='squared_hinge',
                                                             max_iter=1000,
                                                             multi_class='ovr',
                                                             penalty='l2',
                                                             random_state=None,
                                                             tol=0.0001,
                                                             verbose=0),
                                    learning_rate=0.40053644159285917,
                                    n_estimators=190, random_state=0))],
         verbose=False), 'clf__bootstrap': True, 'clf__bootstrap_features': False, 'clf__max_features': 0.026169964054654993, 'clf__max_samples': 0.5616100320361351, 'clf__n_estimators': 397, 'clf__oob_score': True, 'clf__warm_start': True, 'vect__max_df': 0.5444444444444444, 'vect__min_df': 0.007205596954635388, 'vect__ngram_range': (1, 1), 'vect__stop_words': 'english', 'vect__token_pattern': '\\w{1,}'}, {'clf__base_estimator': Pipeline(memory=None,
         steps=[('vect',
                 TfidfVectorizer(analyzer='word', binary=False,
                                 decode_error='strict',
                                 dtype=<class 'numpy.float64'>,
                                 encoding='utf-8', input='content',
                                 lowercase=True, max_df=0.6333333333333333,
                                 max_features=None,
                                 min_df=0.0010083260374073057,
                                 ngram_range=(1, 1), norm='l2',
                                 preprocessor=None, smooth_idf=True,
                                 stop_words='english', strip_accents=...
                                 tokenizer=None, use_idf=True,
                                 vocabulary=None)),
                ('clf',
                 AdaBoostClassifier(algorithm='SAMME',
                                    base_estimator=LinearSVC(C=1.0,
                                                             class_weight=None,
                                                             dual=True,
                                                             fit_intercept=True,
                                                             intercept_scaling=1,
                                                             loss='squared_hinge',
                                                             max_iter=1000,
                                                             multi_class='ovr',
                                                             penalty='l2',
                                                             random_state=None,
                                                             tol=0.0001,
                                                             verbose=0),
                                    learning_rate=0.40053644159285917,
                                    n_estimators=190, random_state=0))],
         verbose=False), 'clf__bootstrap': False, 'clf__bootstrap_features': False, 'clf__max_features': 0.24416450273006962, 'clf__max_samples': 0.9314701714465692, 'clf__n_estimators': 84, 'clf__oob_score': False, 'clf__warm_start': False, 'vect__max_df': 0.5444444444444444, 'vect__min_df': 0.00047889587110035495, 'vect__ngram_range': (1, 2), 'vect__stop_words': 'english', 'vect__token_pattern': '\\w{1,}'}, {'clf__base_estimator': Pipeline(memory=None,
         steps=[('vect',
                 TfidfVectorizer(analyzer='word', binary=False,
                                 decode_error='strict',
                                 dtype=<class 'numpy.float64'>,
                                 encoding='utf-8', input='content',
                                 lowercase=True, max_df=0.8555555555555556,
                                 max_features=None,
                                 min_df=0.00016483617040605902,
                                 ngram_range=(1, 2), norm='l2',
                                 preprocessor=None, smooth_idf=True,
                                 stop_words='english', strip_accents...
                                 tokenizer=None, use_idf=True,
                                 vocabulary=None)),
                ('clf',
                 LogisticRegression(C=0.47946121772409134, class_weight=None,
                                    dual=False, fit_intercept=True,
                                    intercept_scaling=1,
                                    l1_ratio=0.9168495407673116, max_iter=75,
                                    multi_class='multinomial', n_jobs=None,
                                    penalty='none', random_state=0,
                                    solver='sag', tol=6.514227791666363e-07,
                                    verbose=0, warm_start=False))],
         verbose=False), 'clf__bootstrap': False, 'clf__bootstrap_features': True, 'clf__max_features': 0.8492396289409444, 'clf__max_samples': 0.9908035841022258, 'clf__n_estimators': 355, 'clf__oob_score': False, 'clf__warm_start': False, 'vect__max_df': 0.7222222222222222, 'vect__min_df': 0.002637023046189934, 'vect__ngram_range': (1, 1), 'vect__stop_words': 'english', 'vect__token_pattern': '\\w{2,}'}, {'clf__base_estimator': None, 'clf__bootstrap': True, 'clf__bootstrap_features': False, 'clf__max_features': 0.8508259620451911, 'clf__max_samples': 0.9423628271767283, 'clf__n_estimators': 48, 'clf__oob_score': False, 'clf__warm_start': True, 'vect__max_df': 0.6333333333333333, 'vect__min_df': 0.0008661652043599906, 'vect__ngram_range': (1, 2), 'vect__stop_words': 'english', 'vect__token_pattern': '\\w{1,}'}, {'clf__base_estimator': Pipeline(memory=None,
         steps=[('vect',
                 TfidfVectorizer(analyzer='word', binary=False,
                                 decode_error='strict',
                                 dtype=<class 'numpy.float64'>,
                                 encoding='utf-8', input='content',
                                 lowercase=True, max_df=0.6777777777777778,
                                 max_features=None,
                                 min_df=0.0012707570261625706,
                                 ngram_range=(1, 2), norm='l2',
                                 preprocessor=None, smooth_idf=True,
                                 stop_words=None, strip_accents=None,...
                                 vocabulary=None)),
                ('clf',
                 DecisionTreeClassifier(ccp_alpha=0.0, class_weight=None,
                                        criterion='gini', max_depth=49,
                                        max_features=0.56, max_leaf_nodes=316,
                                        min_impurity_decrease=1e-07,
                                        min_impurity_split=None,
                                        min_samples_leaf=4,
                                        min_samples_split=0.015198715678453873,
                                        min_weight_fraction_leaf=0.0,
                                        presort='deprecated', random_state=0,
                                        splitter='random'))],
         verbose=False), 'clf__bootstrap': False, 'clf__bootstrap_features': True, 'clf__max_features': 0.48551459314237844, 'clf__max_samples': 0.2342201903308665, 'clf__n_estimators': 329, 'clf__oob_score': False, 'clf__warm_start': False, 'vect__max_df': 0.9, 'vect__min_df': 0.005924074547707516, 'vect__ngram_range': (1, 1), 'vect__stop_words': None, 'vect__token_pattern': '\\w{1,}'}, {'clf__base_estimator': MultinomialNB(alpha=1.0, class_prior=None, fit_prior=True), 'clf__bootstrap': False, 'clf__bootstrap_features': True, 'clf__max_features': 0.39794236451718035, 'clf__max_samples': 0.4422286088110027, 'clf__n_estimators': 141, 'clf__oob_score': True, 'clf__warm_start': True, 'vect__max_df': 0.5444444444444444, 'vect__min_df': 0.009981339169120484, 'vect__ngram_range': (1, 1), 'vect__stop_words': None, 'vect__token_pattern': '\\w{1,}'}, {'clf__base_estimator': Pipeline(memory=None,
         steps=[('vect',
                 TfidfVectorizer(analyzer='word', binary=False,
                                 decode_error='strict',
                                 dtype=<class 'numpy.float64'>,
                                 encoding='utf-8', input='content',
                                 lowercase=True, max_df=0.7222222222222222,
                                 max_features=None,
                                 min_df=0.00014705642569160768,
                                 ngram_range=(1, 1), norm='l2',
                                 preprocessor=None, smooth_idf=True,
                                 stop_words=None, strip_accents=None...
                                        max_features=0.01, max_leaf_nodes=None,
                                        max_samples=0.007808728196651904,
                                        min_impurity_decrease=3.1622776601683795e-10,
                                        min_impurity_split=None,
                                        min_samples_leaf=62,
                                        min_samples_split=0.0008966659727253451,
                                        min_weight_fraction_leaf=0.0007448459487372297,
                                        n_estimators=251, n_jobs=None,
                                        oob_score=False, random_state=0,
                                        verbose=True, warm_start=True))],
         verbose=False), 'clf__bootstrap': False, 'clf__bootstrap_features': False, 'clf__max_features': 0.15526405782347297, 'clf__max_samples': 0.8708032266022915, 'clf__n_estimators': 152, 'clf__oob_score': True, 'clf__warm_start': True, 'vect__max_df': 0.7666666666666666, 'vect__min_df': 0.001147372405154575, 'vect__ngram_range': (1, 2), 'vect__stop_words': None, 'vect__token_pattern': '\\w{2,}'}, {'clf__base_estimator': Pipeline(memory=None,
         steps=[('vect',
                 TfidfVectorizer(analyzer='word', binary=False,
                                 decode_error='strict',
                                 dtype=<class 'numpy.float64'>,
                                 encoding='utf-8', input='content',
                                 lowercase=True, max_df=0.6777777777777778,
                                 max_features=None,
                                 min_df=0.0012707570261625706,
                                 ngram_range=(1, 2), norm='l2',
                                 preprocessor=None, smooth_idf=True,
                                 stop_words=None, strip_accents=None,...
                                 vocabulary=None)),
                ('clf',
                 DecisionTreeClassifier(ccp_alpha=0.0, class_weight=None,
                                        criterion='gini', max_depth=49,
                                        max_features=0.56, max_leaf_nodes=316,
                                        min_impurity_decrease=1e-07,
                                        min_impurity_split=None,
                                        min_samples_leaf=4,
                                        min_samples_split=0.015198715678453873,
                                        min_weight_fraction_leaf=0.0,
                                        presort='deprecated', random_state=0,
                                        splitter='random'))],
         verbose=False), 'clf__bootstrap': True, 'clf__bootstrap_features': True, 'clf__max_features': 0.24054441575724628, 'clf__max_samples': 0.7641143165482993, 'clf__n_estimators': 323, 'clf__oob_score': True, 'clf__warm_start': True, 'vect__max_df': 0.5888888888888889, 'vect__min_df': 0.0004446598761497269, 'vect__ngram_range': (1, 2), 'vect__stop_words': 'english', 'vect__token_pattern': '\\w{1,}'}, {'clf__base_estimator': Pipeline(memory=None,
         steps=[('vect',
                 TfidfVectorizer(analyzer='word', binary=False,
                                 decode_error='strict',
                                 dtype=<class 'numpy.float64'>,
                                 encoding='utf-8', input='content',
                                 lowercase=True, max_df=0.6777777777777778,
                                 max_features=None,
                                 min_df=0.0012707570261625706,
                                 ngram_range=(1, 2), norm='l2',
                                 preprocessor=None, smooth_idf=True,
                                 stop_words=None, strip_accents=None,...
                                 vocabulary=None)),
                ('clf',
                 DecisionTreeClassifier(ccp_alpha=0.0, class_weight=None,
                                        criterion='gini', max_depth=49,
                                        max_features=0.56, max_leaf_nodes=316,
                                        min_impurity_decrease=1e-07,
                                        min_impurity_split=None,
                                        min_samples_leaf=4,
                                        min_samples_split=0.015198715678453873,
                                        min_weight_fraction_leaf=0.0,
                                        presort='deprecated', random_state=0,
                                        splitter='random'))],
         verbose=False), 'clf__bootstrap': True, 'clf__bootstrap_features': True, 'clf__max_features': 0.030038975497580833, 'clf__max_samples': 0.8816462999279175, 'clf__n_estimators': 117, 'clf__oob_score': False, 'clf__warm_start': False, 'vect__max_df': 0.7222222222222222, 'vect__min_df': 0.0034810124102999155, 'vect__ngram_range': (1, 2), 'vect__stop_words': None, 'vect__token_pattern': '\\w{1,}'}, {'clf__base_estimator': Pipeline(memory=None,
         steps=[('vect',
                 TfidfVectorizer(analyzer='word', binary=False,
                                 decode_error='strict',
                                 dtype=<class 'numpy.float64'>,
                                 encoding='utf-8', input='content',
                                 lowercase=True, max_df=0.6777777777777778,
                                 max_features=None,
                                 min_df=0.0012707570261625706,
                                 ngram_range=(1, 2), norm='l2',
                                 preprocessor=None, smooth_idf=True,
                                 stop_words=None, strip_accents=None,...
                                 vocabulary=None)),
                ('clf',
                 DecisionTreeClassifier(ccp_alpha=0.0, class_weight=None,
                                        criterion='gini', max_depth=49,
                                        max_features=0.56, max_leaf_nodes=316,
                                        min_impurity_decrease=1e-07,
                                        min_impurity_split=None,
                                        min_samples_leaf=4,
                                        min_samples_split=0.015198715678453873,
                                        min_weight_fraction_leaf=0.0,
                                        presort='deprecated', random_state=0,
                                        splitter='random'))],
         verbose=False), 'clf__bootstrap': True, 'clf__bootstrap_features': False, 'clf__max_features': 0.9149855489411466, 'clf__max_samples': 0.20561886827850373, 'clf__n_estimators': 31, 'clf__oob_score': True, 'clf__warm_start': True, 'vect__max_df': 0.6777777777777778, 'vect__min_df': 0.00961301992181817, 'vect__ngram_range': (1, 2), 'vect__stop_words': None, 'vect__token_pattern': '\\w{2,}'}, {'clf__base_estimator': Pipeline(memory=None,
         steps=[('vect',
                 TfidfVectorizer(analyzer='word', binary=False,
                                 decode_error='strict',
                                 dtype=<class 'numpy.float64'>,
                                 encoding='utf-8', input='content',
                                 lowercase=True, max_df=0.6333333333333333,
                                 max_features=None,
                                 min_df=0.0010083260374073057,
                                 ngram_range=(1, 1), norm='l2',
                                 preprocessor=None, smooth_idf=True,
                                 stop_words='english', strip_accents=...
                                 tokenizer=None, use_idf=True,
                                 vocabulary=None)),
                ('clf',
                 AdaBoostClassifier(algorithm='SAMME',
                                    base_estimator=LinearSVC(C=1.0,
                                                             class_weight=None,
                                                             dual=True,
                                                             fit_intercept=True,
                                                             intercept_scaling=1,
                                                             loss='squared_hinge',
                                                             max_iter=1000,
                                                             multi_class='ovr',
                                                             penalty='l2',
                                                             random_state=None,
                                                             tol=0.0001,
                                                             verbose=0),
                                    learning_rate=0.40053644159285917,
                                    n_estimators=190, random_state=0))],
         verbose=False), 'clf__bootstrap': True, 'clf__bootstrap_features': False, 'clf__max_features': 0.46207474901789525, 'clf__max_samples': 0.005872825254672476, 'clf__n_estimators': 96, 'clf__oob_score': True, 'clf__warm_start': True, 'vect__max_df': 0.8555555555555556, 'vect__min_df': 0.0014749151153054764, 'vect__ngram_range': (1, 1), 'vect__stop_words': None, 'vect__token_pattern': '\\w{2,}'}, {'clf__base_estimator': Pipeline(memory=None,
         steps=[('vect',
                 TfidfVectorizer(analyzer='word', binary=False,
                                 decode_error='strict',
                                 dtype=<class 'numpy.float64'>,
                                 encoding='utf-8', input='content',
                                 lowercase=True, max_df=0.6777777777777778,
                                 max_features=None,
                                 min_df=0.0012707570261625706,
                                 ngram_range=(1, 2), norm='l2',
                                 preprocessor=None, smooth_idf=True,
                                 stop_words=None, strip_accents=None,...
                                 vocabulary=None)),
                ('clf',
                 DecisionTreeClassifier(ccp_alpha=0.0, class_weight=None,
                                        criterion='gini', max_depth=49,
                                        max_features=0.56, max_leaf_nodes=316,
                                        min_impurity_decrease=1e-07,
                                        min_impurity_split=None,
                                        min_samples_leaf=4,
                                        min_samples_split=0.015198715678453873,
                                        min_weight_fraction_leaf=0.0,
                                        presort='deprecated', random_state=0,
                                        splitter='random'))],
         verbose=False), 'clf__bootstrap': False, 'clf__bootstrap_features': False, 'clf__max_features': 0.12805494124630634, 'clf__max_samples': 0.7387372220155091, 'clf__n_estimators': 399, 'clf__oob_score': True, 'clf__warm_start': False, 'vect__max_df': 0.7222222222222222, 'vect__min_df': 0.0002304567810434819, 'vect__ngram_range': (1, 1), 'vect__stop_words': 'english', 'vect__token_pattern': '\\w{1,}'}, {'clf__base_estimator': Pipeline(memory=None,
         steps=[('vect',
                 TfidfVectorizer(analyzer='word', binary=False,
                                 decode_error='strict',
                                 dtype=<class 'numpy.float64'>,
                                 encoding='utf-8', input='content',
                                 lowercase=True, max_df=0.7222222222222222,
                                 max_features=None,
                                 min_df=0.00014705642569160768,
                                 ngram_range=(1, 1), norm='l2',
                                 preprocessor=None, smooth_idf=True,
                                 stop_words=None, strip_accents=None...
                                        max_features=0.01, max_leaf_nodes=None,
                                        max_samples=0.007808728196651904,
                                        min_impurity_decrease=3.1622776601683795e-10,
                                        min_impurity_split=None,
                                        min_samples_leaf=62,
                                        min_samples_split=0.0008966659727253451,
                                        min_weight_fraction_leaf=0.0007448459487372297,
                                        n_estimators=251, n_jobs=None,
                                        oob_score=False, random_state=0,
                                        verbose=True, warm_start=True))],
         verbose=False), 'clf__bootstrap': False, 'clf__bootstrap_features': False, 'clf__max_features': 0.7416617230448526, 'clf__max_samples': 0.5724320473455495, 'clf__n_estimators': 75, 'clf__oob_score': False, 'clf__warm_start': True, 'vect__max_df': 0.5444444444444444, 'vect__min_df': 0.008500887639270652, 'vect__ngram_range': (1, 1), 'vect__stop_words': None, 'vect__token_pattern': '\\w{2,}'}, {'clf__base_estimator': None, 'clf__bootstrap': True, 'clf__bootstrap_features': True, 'clf__max_features': 0.028609864096968196, 'clf__max_samples': 0.5436549244718809, 'clf__n_estimators': 196, 'clf__oob_score': True, 'clf__warm_start': True, 'vect__max_df': 0.5444444444444444, 'vect__min_df': 0.0001880763629327088, 'vect__ngram_range': (1, 1), 'vect__stop_words': None, 'vect__token_pattern': '\\w{1,}'}, {'clf__base_estimator': Pipeline(memory=None,
         steps=[('vect',
                 TfidfVectorizer(analyzer='word', binary=False,
                                 decode_error='strict',
                                 dtype=<class 'numpy.float64'>,
                                 encoding='utf-8', input='content',
                                 lowercase=True, max_df=0.8555555555555556,
                                 max_features=None,
                                 min_df=0.00016483617040605902,
                                 ngram_range=(1, 2), norm='l2',
                                 preprocessor=None, smooth_idf=True,
                                 stop_words='english', strip_accents...
                                 tokenizer=None, use_idf=True,
                                 vocabulary=None)),
                ('clf',
                 LogisticRegression(C=0.47946121772409134, class_weight=None,
                                    dual=False, fit_intercept=True,
                                    intercept_scaling=1,
                                    l1_ratio=0.9168495407673116, max_iter=75,
                                    multi_class='multinomial', n_jobs=None,
                                    penalty='none', random_state=0,
                                    solver='sag', tol=6.514227791666363e-07,
                                    verbose=0, warm_start=False))],
         verbose=False), 'clf__bootstrap': False, 'clf__bootstrap_features': False, 'clf__max_features': 0.21239798536546894, 'clf__max_samples': 0.9907500064232448, 'clf__n_estimators': 230, 'clf__oob_score': True, 'clf__warm_start': True, 'vect__max_df': 0.7222222222222222, 'vect__min_df': 0.0010647922993256248, 'vect__ngram_range': (1, 2), 'vect__stop_words': None, 'vect__token_pattern': '\\w{2,}'}, {'clf__base_estimator': None, 'clf__bootstrap': True, 'clf__bootstrap_features': True, 'clf__max_features': 0.9013347673197406, 'clf__max_samples': 0.5795762074280727, 'clf__n_estimators': 386, 'clf__oob_score': True, 'clf__warm_start': True, 'vect__max_df': 0.6777777777777778, 'vect__min_df': 0.0017052197067315843, 'vect__ngram_range': (1, 2), 'vect__stop_words': 'english', 'vect__token_pattern': '\\w{1,}'}, {'clf__base_estimator': Pipeline(memory=None,
         steps=[('vect',
                 TfidfVectorizer(analyzer='word', binary=False,
                                 decode_error='strict',
                                 dtype=<class 'numpy.float64'>,
                                 encoding='utf-8', input='content',
                                 lowercase=True, max_df=0.6777777777777778,
                                 max_features=None,
                                 min_df=0.0012707570261625706,
                                 ngram_range=(1, 2), norm='l2',
                                 preprocessor=None, smooth_idf=True,
                                 stop_words=None, strip_accents=None,...
                                 vocabulary=None)),
                ('clf',
                 DecisionTreeClassifier(ccp_alpha=0.0, class_weight=None,
                                        criterion='gini', max_depth=49,
                                        max_features=0.56, max_leaf_nodes=316,
                                        min_impurity_decrease=1e-07,
                                        min_impurity_split=None,
                                        min_samples_leaf=4,
                                        min_samples_split=0.015198715678453873,
                                        min_weight_fraction_leaf=0.0,
                                        presort='deprecated', random_state=0,
                                        splitter='random'))],
         verbose=False), 'clf__bootstrap': False, 'clf__bootstrap_features': False, 'clf__max_features': 0.3779847902337966, 'clf__max_samples': 0.26358137053301645, 'clf__n_estimators': 338, 'clf__oob_score': True, 'clf__warm_start': False, 'vect__max_df': 0.5, 'vect__min_df': 0.0032975676312138654, 'vect__ngram_range': (1, 2), 'vect__stop_words': None, 'vect__token_pattern': '\\w{1,}'}, {'clf__base_estimator': Pipeline(memory=None,
         steps=[('vect',
                 TfidfVectorizer(analyzer='word', binary=False,
                                 decode_error='strict',
                                 dtype=<class 'numpy.float64'>,
                                 encoding='utf-8', input='content',
                                 lowercase=True, max_df=0.6333333333333333,
                                 max_features=None,
                                 min_df=0.0010083260374073057,
                                 ngram_range=(1, 1), norm='l2',
                                 preprocessor=None, smooth_idf=True,
                                 stop_words='english', strip_accents=...
                                 tokenizer=None, use_idf=True,
                                 vocabulary=None)),
                ('clf',
                 AdaBoostClassifier(algorithm='SAMME',
                                    base_estimator=LinearSVC(C=1.0,
                                                             class_weight=None,
                                                             dual=True,
                                                             fit_intercept=True,
                                                             intercept_scaling=1,
                                                             loss='squared_hinge',
                                                             max_iter=1000,
                                                             multi_class='ovr',
                                                             penalty='l2',
                                                             random_state=None,
                                                             tol=0.0001,
                                                             verbose=0),
                                    learning_rate=0.40053644159285917,
                                    n_estimators=190, random_state=0))],
         verbose=False), 'clf__bootstrap': False, 'clf__bootstrap_features': True, 'clf__max_features': 0.39009505777512277, 'clf__max_samples': 0.9477764248603917, 'clf__n_estimators': 350, 'clf__oob_score': True, 'clf__warm_start': True, 'vect__max_df': 0.6333333333333333, 'vect__min_df': 0.00032451846073530645, 'vect__ngram_range': (1, 2), 'vect__stop_words': 'english', 'vect__token_pattern': '\\w{1,}'}, {'clf__base_estimator': MultinomialNB(alpha=1.0, class_prior=None, fit_prior=True), 'clf__bootstrap': True, 'clf__bootstrap_features': False, 'clf__max_features': 0.18322640896232567, 'clf__max_samples': 0.4847466187924927, 'clf__n_estimators': 266, 'clf__oob_score': True, 'clf__warm_start': True, 'vect__max_df': 0.7222222222222222, 'vect__min_df': 0.00011978539103282762, 'vect__ngram_range': (1, 1), 'vect__stop_words': None, 'vect__token_pattern': '\\w{1,}'}, {'clf__base_estimator': Pipeline(memory=None,
         steps=[('vect',
                 TfidfVectorizer(analyzer='word', binary=False,
                                 decode_error='strict',
                                 dtype=<class 'numpy.float64'>,
                                 encoding='utf-8', input='content',
                                 lowercase=True, max_df=0.6777777777777778,
                                 max_features=None,
                                 min_df=0.0012707570261625706,
                                 ngram_range=(1, 2), norm='l2',
                                 preprocessor=None, smooth_idf=True,
                                 stop_words=None, strip_accents=None,...
                                 vocabulary=None)),
                ('clf',
                 DecisionTreeClassifier(ccp_alpha=0.0, class_weight=None,
                                        criterion='gini', max_depth=49,
                                        max_features=0.56, max_leaf_nodes=316,
                                        min_impurity_decrease=1e-07,
                                        min_impurity_split=None,
                                        min_samples_leaf=4,
                                        min_samples_split=0.015198715678453873,
                                        min_weight_fraction_leaf=0.0,
                                        presort='deprecated', random_state=0,
                                        splitter='random'))],
         verbose=False), 'clf__bootstrap': False, 'clf__bootstrap_features': True, 'clf__max_features': 0.9312787930942861, 'clf__max_samples': 0.44603033432367944, 'clf__n_estimators': 383, 'clf__oob_score': False, 'clf__warm_start': False, 'vect__max_df': 0.5444444444444444, 'vect__min_df': 0.0013280530323949023, 'vect__ngram_range': (1, 2), 'vect__stop_words': None, 'vect__token_pattern': '\\w{2,}'}, {'clf__base_estimator': Pipeline(memory=None,
         steps=[('vect',
                 TfidfVectorizer(analyzer='word', binary=False,
                                 decode_error='strict',
                                 dtype=<class 'numpy.float64'>,
                                 encoding='utf-8', input='content',
                                 lowercase=True, max_df=0.6333333333333333,
                                 max_features=None,
                                 min_df=0.0010083260374073057,
                                 ngram_range=(1, 1), norm='l2',
                                 preprocessor=None, smooth_idf=True,
                                 stop_words='english', strip_accents=...
                                 tokenizer=None, use_idf=True,
                                 vocabulary=None)),
                ('clf',
                 AdaBoostClassifier(algorithm='SAMME',
                                    base_estimator=LinearSVC(C=1.0,
                                                             class_weight=None,
                                                             dual=True,
                                                             fit_intercept=True,
                                                             intercept_scaling=1,
                                                             loss='squared_hinge',
                                                             max_iter=1000,
                                                             multi_class='ovr',
                                                             penalty='l2',
                                                             random_state=None,
                                                             tol=0.0001,
                                                             verbose=0),
                                    learning_rate=0.40053644159285917,
                                    n_estimators=190, random_state=0))],
         verbose=False), 'clf__bootstrap': True, 'clf__bootstrap_features': True, 'clf__max_features': 0.08811614901174147, 'clf__max_samples': 0.42438906284110456, 'clf__n_estimators': 388, 'clf__oob_score': False, 'clf__warm_start': True, 'vect__max_df': 0.5888888888888889, 'vect__min_df': 0.0005981036708715185, 'vect__ngram_range': (1, 2), 'vect__stop_words': None, 'vect__token_pattern': '\\w{1,}'}, {'clf__base_estimator': Pipeline(memory=None,
         steps=[('vect',
                 TfidfVectorizer(analyzer='word', binary=False,
                                 decode_error='strict',
                                 dtype=<class 'numpy.float64'>,
                                 encoding='utf-8', input='content',
                                 lowercase=True, max_df=0.7222222222222222,
                                 max_features=None,
                                 min_df=0.00014705642569160768,
                                 ngram_range=(1, 1), norm='l2',
                                 preprocessor=None, smooth_idf=True,
                                 stop_words=None, strip_accents=None...
                                        max_features=0.01, max_leaf_nodes=None,
                                        max_samples=0.007808728196651904,
                                        min_impurity_decrease=3.1622776601683795e-10,
                                        min_impurity_split=None,
                                        min_samples_leaf=62,
                                        min_samples_split=0.0008966659727253451,
                                        min_weight_fraction_leaf=0.0007448459487372297,
                                        n_estimators=251, n_jobs=None,
                                        oob_score=False, random_state=0,
                                        verbose=True, warm_start=True))],
         verbose=False), 'clf__bootstrap': False, 'clf__bootstrap_features': False, 'clf__max_features': 0.5530024589549252, 'clf__max_samples': 0.532668748242248, 'clf__n_estimators': 214, 'clf__oob_score': False, 'clf__warm_start': True, 'vect__max_df': 0.8555555555555556, 'vect__min_df': 0.0028354733636035655, 'vect__ngram_range': (1, 2), 'vect__stop_words': None, 'vect__token_pattern': '\\w{2,}'}, {'clf__base_estimator': Pipeline(memory=None,
         steps=[('vect',
                 TfidfVectorizer(analyzer='word', binary=False,
                                 decode_error='strict',
                                 dtype=<class 'numpy.float64'>,
                                 encoding='utf-8', input='content',
                                 lowercase=True, max_df=0.8555555555555556,
                                 max_features=None,
                                 min_df=0.00016483617040605902,
                                 ngram_range=(1, 2), norm='l2',
                                 preprocessor=None, smooth_idf=True,
                                 stop_words='english', strip_accents...
                                 tokenizer=None, use_idf=True,
                                 vocabulary=None)),
                ('clf',
                 LogisticRegression(C=0.47946121772409134, class_weight=None,
                                    dual=False, fit_intercept=True,
                                    intercept_scaling=1,
                                    l1_ratio=0.9168495407673116, max_iter=75,
                                    multi_class='multinomial', n_jobs=None,
                                    penalty='none', random_state=0,
                                    solver='sag', tol=6.514227791666363e-07,
                                    verbose=0, warm_start=False))],
         verbose=False), 'clf__bootstrap': False, 'clf__bootstrap_features': False, 'clf__max_features': 0.9665716350714949, 'clf__max_samples': 0.47672671190122273, 'clf__n_estimators': 178, 'clf__oob_score': False, 'clf__warm_start': False, 'vect__max_df': 0.5444444444444444, 'vect__min_df': 0.001261008822907652, 'vect__ngram_range': (1, 1), 'vect__stop_words': 'english', 'vect__token_pattern': '\\w{1,}'}, {'clf__base_estimator': Pipeline(memory=None,
         steps=[('vect',
                 TfidfVectorizer(analyzer='word', binary=False,
                                 decode_error='strict',
                                 dtype=<class 'numpy.float64'>,
                                 encoding='utf-8', input='content',
                                 lowercase=True, max_df=0.7666666666666666,
                                 max_features=None,
                                 min_df=0.00010572508827796376,
                                 ngram_range=(1, 2), norm='l2',
                                 preprocessor=None, smooth_idf=True,
                                 stop_words='english', strip_accents=None,
                                 sublinear_tf=False, token_pattern='\\w{1,}',
                                 tokenizer=None, use_idf=True,
                                 vocabulary=None)),
                ('clf',
                 LinearSVC(C=99.37702864845689, class_weight=None, dual=False,
                           fit_intercept=False, intercept_scaling=1,
                           loss='squared_hinge', max_iter=1000,
                           multi_class='ovr', penalty='l2', random_state=0,
                           tol=3.8359242531998225e-06, verbose=0))],
         verbose=False), 'clf__bootstrap': False, 'clf__bootstrap_features': True, 'clf__max_features': 0.7992400271516518, 'clf__max_samples': 0.5164785347710172, 'clf__n_estimators': 127, 'clf__oob_score': True, 'clf__warm_start': False, 'vect__max_df': 0.8111111111111111, 'vect__min_df': 0.0027142272290598896, 'vect__ngram_range': (1, 1), 'vect__stop_words': None, 'vect__token_pattern': '\\w{2,}'}, {'clf__base_estimator': Pipeline(memory=None,
         steps=[('vect',
                 TfidfVectorizer(analyzer='word', binary=False,
                                 decode_error='strict',
                                 dtype=<class 'numpy.float64'>,
                                 encoding='utf-8', input='content',
                                 lowercase=True, max_df=0.6777777777777778,
                                 max_features=None,
                                 min_df=0.0012707570261625706,
                                 ngram_range=(1, 2), norm='l2',
                                 preprocessor=None, smooth_idf=True,
                                 stop_words=None, strip_accents=None,...
                                 vocabulary=None)),
                ('clf',
                 DecisionTreeClassifier(ccp_alpha=0.0, class_weight=None,
                                        criterion='gini', max_depth=49,
                                        max_features=0.56, max_leaf_nodes=316,
                                        min_impurity_decrease=1e-07,
                                        min_impurity_split=None,
                                        min_samples_leaf=4,
                                        min_samples_split=0.015198715678453873,
                                        min_weight_fraction_leaf=0.0,
                                        presort='deprecated', random_state=0,
                                        splitter='random'))],
         verbose=False), 'clf__bootstrap': True, 'clf__bootstrap_features': True, 'clf__max_features': 0.9802402877274728, 'clf__max_samples': 0.0724394139679907, 'clf__n_estimators': 32, 'clf__oob_score': True, 'clf__warm_start': True, 'vect__max_df': 0.6333333333333333, 'vect__min_df': 0.00745914718205243, 'vect__ngram_range': (1, 2), 'vect__stop_words': 'english', 'vect__token_pattern': '\\w{2,}'}, {'clf__base_estimator': None, 'clf__bootstrap': False, 'clf__bootstrap_features': False, 'clf__max_features': 0.17454393949296276, 'clf__max_samples': 0.9488968049234774, 'clf__n_estimators': 38, 'clf__oob_score': False, 'clf__warm_start': False, 'vect__max_df': 0.6333333333333333, 'vect__min_df': 0.0012180400671630584, 'vect__ngram_range': (1, 1), 'vect__stop_words': None, 'vect__token_pattern': '\\w{2,}'}, {'clf__base_estimator': Pipeline(memory=None,
         steps=[('vect',
                 TfidfVectorizer(analyzer='word', binary=False,
                                 decode_error='strict',
                                 dtype=<class 'numpy.float64'>,
                                 encoding='utf-8', input='content',
                                 lowercase=True, max_df=0.7222222222222222,
                                 max_features=None,
                                 min_df=0.00014705642569160768,
                                 ngram_range=(1, 1), norm='l2',
                                 preprocessor=None, smooth_idf=True,
                                 stop_words=None, strip_accents=None...
                                        max_features=0.01, max_leaf_nodes=None,
                                        max_samples=0.007808728196651904,
                                        min_impurity_decrease=3.1622776601683795e-10,
                                        min_impurity_split=None,
                                        min_samples_leaf=62,
                                        min_samples_split=0.0008966659727253451,
                                        min_weight_fraction_leaf=0.0007448459487372297,
                                        n_estimators=251, n_jobs=None,
                                        oob_score=False, random_state=0,
                                        verbose=True, warm_start=True))],
         verbose=False), 'clf__bootstrap': False, 'clf__bootstrap_features': False, 'clf__max_features': 0.4172577149382912, 'clf__max_samples': 0.6819205156481029, 'clf__n_estimators': 107, 'clf__oob_score': False, 'clf__warm_start': True, 'vect__max_df': 0.5, 'vect__min_df': 0.004558549171871819, 'vect__ngram_range': (1, 1), 'vect__stop_words': 'english', 'vect__token_pattern': '\\w{2,}'}, {'clf__base_estimator': Pipeline(memory=None,
         steps=[('vect',
                 TfidfVectorizer(analyzer='word', binary=False,
                                 decode_error='strict',
                                 dtype=<class 'numpy.float64'>,
                                 encoding='utf-8', input='content',
                                 lowercase=True, max_df=0.7666666666666666,
                                 max_features=None,
                                 min_df=0.00010572508827796376,
                                 ngram_range=(1, 2), norm='l2',
                                 preprocessor=None, smooth_idf=True,
                                 stop_words='english', strip_accents=None,
                                 sublinear_tf=False, token_pattern='\\w{1,}',
                                 tokenizer=None, use_idf=True,
                                 vocabulary=None)),
                ('clf',
                 LinearSVC(C=99.37702864845689, class_weight=None, dual=False,
                           fit_intercept=False, intercept_scaling=1,
                           loss='squared_hinge', max_iter=1000,
                           multi_class='ovr', penalty='l2', random_state=0,
                           tol=3.8359242531998225e-06, verbose=0))],
         verbose=False), 'clf__bootstrap': True, 'clf__bootstrap_features': False, 'clf__max_features': 0.12204752707809907, 'clf__max_samples': 0.39546784875406715, 'clf__n_estimators': 277, 'clf__oob_score': True, 'clf__warm_start': True, 'vect__max_df': 0.5888888888888889, 'vect__min_df': 0.003925143857322602, 'vect__ngram_range': (1, 1), 'vect__stop_words': None, 'vect__token_pattern': '\\w{1,}'}, {'clf__base_estimator': Pipeline(memory=None,
         steps=[('vect',
                 TfidfVectorizer(analyzer='word', binary=False,
                                 decode_error='strict',
                                 dtype=<class 'numpy.float64'>,
                                 encoding='utf-8', input='content',
                                 lowercase=True, max_df=0.7222222222222222,
                                 max_features=None,
                                 min_df=0.00014705642569160768,
                                 ngram_range=(1, 1), norm='l2',
                                 preprocessor=None, smooth_idf=True,
                                 stop_words=None, strip_accents=None...
                                        max_features=0.01, max_leaf_nodes=None,
                                        max_samples=0.007808728196651904,
                                        min_impurity_decrease=3.1622776601683795e-10,
                                        min_impurity_split=None,
                                        min_samples_leaf=62,
                                        min_samples_split=0.0008966659727253451,
                                        min_weight_fraction_leaf=0.0007448459487372297,
                                        n_estimators=251, n_jobs=None,
                                        oob_score=False, random_state=0,
                                        verbose=True, warm_start=True))],
         verbose=False), 'clf__bootstrap': False, 'clf__bootstrap_features': True, 'clf__max_features': 0.5057664385563188, 'clf__max_samples': 0.5145267519376905, 'clf__n_estimators': 322, 'clf__oob_score': False, 'clf__warm_start': True, 'vect__max_df': 0.5, 'vect__min_df': 0.0006241953388285983, 'vect__ngram_range': (1, 1), 'vect__stop_words': 'english', 'vect__token_pattern': '\\w{1,}'}, {'clf__base_estimator': Pipeline(memory=None,
         steps=[('vect',
                 TfidfVectorizer(analyzer='word', binary=False,
                                 decode_error='strict',
                                 dtype=<class 'numpy.float64'>,
                                 encoding='utf-8', input='content',
                                 lowercase=True, max_df=0.7666666666666666,
                                 max_features=None,
                                 min_df=0.00010572508827796376,
                                 ngram_range=(1, 2), norm='l2',
                                 preprocessor=None, smooth_idf=True,
                                 stop_words='english', strip_accents=None,
                                 sublinear_tf=False, token_pattern='\\w{1,}',
                                 tokenizer=None, use_idf=True,
                                 vocabulary=None)),
                ('clf',
                 LinearSVC(C=99.37702864845689, class_weight=None, dual=False,
                           fit_intercept=False, intercept_scaling=1,
                           loss='squared_hinge', max_iter=1000,
                           multi_class='ovr', penalty='l2', random_state=0,
                           tol=3.8359242531998225e-06, verbose=0))],
         verbose=False), 'clf__bootstrap': False, 'clf__bootstrap_features': True, 'clf__max_features': 0.5914234847917038, 'clf__max_samples': 0.6032121819431405, 'clf__n_estimators': 246, 'clf__oob_score': False, 'clf__warm_start': False, 'vect__max_df': 0.5, 'vect__min_df': 0.004200077108533546, 'vect__ngram_range': (1, 2), 'vect__stop_words': 'english', 'vect__token_pattern': '\\w{1,}'}, {'clf__base_estimator': MultinomialNB(alpha=1.0, class_prior=None, fit_prior=True), 'clf__bootstrap': True, 'clf__bootstrap_features': False, 'clf__max_features': 0.30068257675087484, 'clf__max_samples': 0.75956566277332, 'clf__n_estimators': 75, 'clf__oob_score': False, 'clf__warm_start': False, 'vect__max_df': 0.9, 'vect__min_df': 0.00015088836173661365, 'vect__ngram_range': (1, 2), 'vect__stop_words': 'english', 'vect__token_pattern': '\\w{2,}'}, {'clf__base_estimator': Pipeline(memory=None,
         steps=[('vect',
                 TfidfVectorizer(analyzer='word', binary=False,
                                 decode_error='strict',
                                 dtype=<class 'numpy.float64'>,
                                 encoding='utf-8', input='content',
                                 lowercase=True, max_df=0.8555555555555556,
                                 max_features=None,
                                 min_df=0.00016483617040605902,
                                 ngram_range=(1, 2), norm='l2',
                                 preprocessor=None, smooth_idf=True,
                                 stop_words='english', strip_accents...
                                 tokenizer=None, use_idf=True,
                                 vocabulary=None)),
                ('clf',
                 LogisticRegression(C=0.47946121772409134, class_weight=None,
                                    dual=False, fit_intercept=True,
                                    intercept_scaling=1,
                                    l1_ratio=0.9168495407673116, max_iter=75,
                                    multi_class='multinomial', n_jobs=None,
                                    penalty='none', random_state=0,
                                    solver='sag', tol=6.514227791666363e-07,
                                    verbose=0, warm_start=False))],
         verbose=False), 'clf__bootstrap': True, 'clf__bootstrap_features': True, 'clf__max_features': 0.4033915522597019, 'clf__max_samples': 0.10902863351855907, 'clf__n_estimators': 364, 'clf__oob_score': False, 'clf__warm_start': True, 'vect__max_df': 0.6777777777777778, 'vect__min_df': 0.00010056292290556832, 'vect__ngram_range': (1, 2), 'vect__stop_words': None, 'vect__token_pattern': '\\w{2,}'}], 'split0_test_score': array([       nan,        nan, 0.54441008,        nan,        nan,
       0.63632346, 0.6195316 ,        nan,        nan,        nan,
              nan,        nan, 0.59169244,        nan,        nan,
              nan, 0.59478568,        nan,        nan,        nan,
              nan, 0.57048166,        nan,        nan,        nan,
       0.60539107,        nan,        nan, 0.59611136,        nan,
              nan,        nan,        nan,        nan,        nan,
       0.56296951,        nan,        nan, 0.70216527,        nan,
              nan,        nan, 0.64560318,        nan,        nan,
              nan,        nan,        nan,        nan,        nan,
              nan, 0.6243924 ,        nan,        nan,        nan,
              nan,        nan,        nan,        nan,        nan,
              nan,        nan,        nan, 0.57711003,        nan,
              nan,        nan,        nan,        nan, 0.67211666,
              nan,        nan,        nan, 0.64118427,        nan,
              nan,        nan, 0.58771542,        nan,        nan,
       0.66195316,        nan, 0.58064516,        nan,        nan,
              nan,        nan,        nan,        nan, 0.69155988,
              nan,        nan, 0.66593018,        nan,        nan,
              nan,        nan,        nan,        nan, 0.57269112,
              nan,        nan,        nan,        nan,        nan,
              nan,        nan,        nan,        nan,        nan,
              nan,        nan, 0.53071144,        nan,        nan,
              nan,        nan,        nan,        nan,        nan,
              nan,        nan,        nan,        nan,        nan,
              nan,        nan,        nan,        nan,        nan,
              nan,        nan,        nan, 0.67609368,        nan,
              nan,        nan,        nan,        nan, 0.65267344,
              nan,        nan,        nan,        nan,        nan,
       0.48254529,        nan, 0.64206805,        nan,        nan,
              nan,        nan,        nan,        nan,        nan,
       0.60804242,        nan,        nan,        nan,        nan,
              nan,        nan,        nan,        nan,        nan,
              nan,        nan,        nan,        nan,        nan,
              nan, 0.58108705,        nan,        nan,        nan,
              nan,        nan,        nan,        nan,        nan,
              nan,        nan,        nan,        nan,        nan,
              nan,        nan,        nan,        nan,        nan,
              nan,        nan,        nan, 0.60185594,        nan,
              nan,        nan,        nan, 0.68360583,        nan]), 'split1_test_score': array([       nan,        nan, 0.53778171,        nan,        nan,
       0.62262483, 0.6190897 ,        nan,        nan,        nan,
              nan,        nan, 0.5735749 ,        nan,        nan,
              nan, 0.58197084,        nan,        nan,        nan,
              nan, 0.55810871,        nan,        nan,        nan,
       0.59125055,        nan,        nan, 0.58683164,        nan,
              nan,        nan,        nan,        nan,        nan,
       0.54441008,        nan,        nan, 0.69465312,        nan,
              nan,        nan, 0.63543968,        nan,        nan,
              nan,        nan,        nan,        nan,        nan,
              nan, 0.60273973,        nan,        nan,        nan,
              nan,        nan,        nan,        nan,        nan,
              nan,        nan,        nan, 0.5687141 ,        nan,
              nan,        nan,        nan,        nan, 0.65355722,
              nan,        nan,        nan, 0.63720725,        nan,
              nan,        nan, 0.59566947,        nan,        nan,
       0.65355722,        nan, 0.56385329,        nan,        nan,
              nan,        nan,        nan,        nan, 0.67432612,
              nan,        nan, 0.65046399,        nan,        nan,
              nan,        nan,        nan,        nan, 0.55015466,
              nan,        nan,        nan,        nan,        nan,
              nan,        nan,        nan,        nan,        nan,
              nan,        nan, 0.52319929,        nan,        nan,
              nan,        nan,        nan,        nan,        nan,
              nan,        nan,        nan,        nan,        nan,
              nan,        nan,        nan,        nan,        nan,
              nan,        nan,        nan, 0.6650464 ,        nan,
              nan,        nan,        nan,        nan, 0.64560318,
              nan,        nan,        nan,        nan,        nan,
       0.53071144,        nan, 0.62306673,        nan,        nan,
              nan,        nan,        nan,        nan,        nan,
       0.60229783,        nan,        nan,        nan,        nan,
              nan,        nan,        nan,        nan,        nan,
              nan,        nan,        nan,        nan,        nan,
              nan, 0.57534247,        nan,        nan,        nan,
              nan,        nan,        nan,        nan,        nan,
              nan,        nan,        nan,        nan,        nan,
              nan,        nan,        nan,        nan,        nan,
              nan,        nan,        nan, 0.59080866,        nan,
              nan,        nan,        nan, 0.66637207,        nan]), 'split2_test_score': array([       nan,        nan, 0.54573575,        nan,        nan,
       0.63411401, 0.64206805,        nan,        nan,        nan,
              nan,        nan, 0.5939019 ,        nan,        nan,
              nan, 0.60229783,        nan,        nan,        nan,
              nan, 0.57755192,        nan,        nan,        nan,
       0.60980999,        nan,        nan, 0.59743703,        nan,
              nan,        nan,        nan,        nan,        nan,
       0.55987627,        nan,        nan, 0.70790985,        nan,
              nan,        nan, 0.6597437 ,        nan,        nan,
              nan,        nan,        nan,        nan,        nan,
              nan, 0.6243924 ,        nan,        nan,        nan,
              nan,        nan,        nan,        nan,        nan,
              nan,        nan,        nan, 0.58285462,        nan,
              nan,        nan,        nan,        nan, 0.68360583,
              nan,        nan,        nan, 0.64206805,        nan,
              nan,        nan, 0.60318162,        nan,        nan,
       0.67697746,        nan, 0.5939019 ,        nan,        nan,
              nan,        nan,        nan,        nan, 0.69376933,
              nan,        nan, 0.67034909,        nan,        nan,
              nan,        nan,        nan,        nan, 0.57003977,
              nan,        nan,        nan,        nan,        nan,
              nan,        nan,        nan,        nan,        nan,
              nan,        nan, 0.53954927,        nan,        nan,
              nan,        nan,        nan,        nan,        nan,
              nan,        nan,        nan,        nan,        nan,
              nan,        nan,        nan,        nan,        nan,
              nan,        nan,        nan, 0.68714096,        nan,
              nan,        nan,        nan,        nan, 0.65444101,
              nan,        nan,        nan,        nan,        nan,
       0.51745471,        nan, 0.64339373,        nan,        nan,
              nan,        nan,        nan,        nan,        nan,
       0.62306673,        nan,        nan,        nan,        nan,
              nan,        nan,        nan,        nan,        nan,
              nan,        nan,        nan,        nan,        nan,
              nan, 0.58418029,        nan,        nan,        nan,
              nan,        nan,        nan,        nan,        nan,
              nan,        nan,        nan,        nan,        nan,
              nan,        nan,        nan,        nan,        nan,
              nan,        nan,        nan, 0.61378701,        nan,
              nan,        nan,        nan, 0.68316394,        nan]), 'split3_test_score': array([       nan,        nan, 0.56164384,        nan,        nan,
       0.63985859, 0.64781264,        nan,        nan,        nan,
              nan,        nan, 0.60494918,        nan,        nan,
              nan, 0.61555457,        nan,        nan,        nan,
              nan, 0.59832081,        nan,        nan,        nan,
       0.61467079,        nan,        nan, 0.60583297,        nan,
              nan,        nan,        nan,        nan,        nan,
       0.58506407,        nan,        nan, 0.71365444,        nan,
              nan,        nan, 0.65311533,        nan,        nan,
              nan,        nan,        nan,        nan,        nan,
              nan, 0.6292532 ,        nan,        nan,        nan,
              nan,        nan,        nan,        nan,        nan,
              nan,        nan,        nan, 0.58594786,        nan,
              nan,        nan,        nan,        nan, 0.67388422,
              nan,        nan,        nan, 0.64692886,        nan,
              nan,        nan, 0.6089262 ,        nan,        nan,
       0.67344233,        nan, 0.60583297,        nan,        nan,
              nan,        nan,        nan,        nan, 0.69907203,
              nan,        nan, 0.6752099 ,        nan,        nan,
              nan,        nan,        nan,        nan, 0.58727353,
              nan,        nan,        nan,        nan,        nan,
              nan,        nan,        nan,        nan,        nan,
              nan,        nan, 0.53468847,        nan,        nan,
              nan,        nan,        nan,        nan,        nan,
              nan,        nan,        nan,        nan,        nan,
              nan,        nan,        nan,        nan,        nan,
              nan,        nan,        nan, 0.69642068,        nan,
              nan,        nan,        nan,        nan, 0.66106938,
              nan,        nan,        nan,        nan,        nan,
       0.51833849,        nan, 0.64913831,        nan,        nan,
              nan,        nan,        nan,        nan,        nan,
       0.63278833,        nan,        nan,        nan,        nan,
              nan,        nan,        nan,        nan,        nan,
              nan,        nan,        nan,        nan,        nan,
              nan, 0.59478568,        nan,        nan,        nan,
              nan,        nan,        nan,        nan,        nan,
              nan,        nan,        nan,        nan,        nan,
              nan,        nan,        nan,        nan,        nan,
              nan,        nan,        nan, 0.61290323,        nan,
              nan,        nan,        nan, 0.68935042,        nan]), 'split4_test_score': array([       nan,        nan, 0.55526083,        nan,        nan,
       0.63262599, 0.63969938,        nan,        nan,        nan,
              nan,        nan, 0.59018568,        nan,        nan,
              nan, 0.59725906,        nan,        nan,        nan,
              nan, 0.5729443 ,        nan,        nan,        nan,
       0.60521662,        nan,        nan, 0.58885942,        nan,
              nan,        nan,        nan,        nan,        nan,
       0.5596817 ,        nan,        nan, 0.70733864,        nan,
              nan,        nan, 0.65384615,        nan,        nan,
              nan,        nan,        nan,        nan,        nan,
              nan, 0.61538462,        nan,        nan,        nan,
              nan,        nan,        nan,        nan,        nan,
              nan,        nan,        nan, 0.57913351,        nan,
              nan,        nan,        nan,        nan, 0.66931919,
              nan,        nan,        nan, 0.63925729,        nan,
              nan,        nan, 0.57648099,        nan,        nan,
       0.66268789,        nan, 0.5826702 ,        nan,        nan,
              nan,        nan,        nan,        nan, 0.69363395,
              nan,        nan, 0.66489832,        nan,        nan,
              nan,        nan,        nan,        nan, 0.56984969,
              nan,        nan,        nan,        nan,        nan,
              nan,        nan,        nan,        nan,        nan,
              nan,        nan, 0.53580902,        nan,        nan,
              nan,        nan,        nan,        nan,        nan,
              nan,        nan,        nan,        nan,        nan,
              nan,        nan,        nan,        nan,        nan,
              nan,        nan,        nan, 0.68081344,        nan,
              nan,        nan,        nan,        nan, 0.63748895,
              nan,        nan,        nan,        nan,        nan,
       0.50839965,        nan, 0.63925729,        nan,        nan,
              nan,        nan,        nan,        nan,        nan,
       0.622458  ,        nan,        nan,        nan,        nan,
              nan,        nan,        nan,        nan,        nan,
              nan,        nan,        nan,        nan,        nan,
              nan, 0.58178603,        nan,        nan,        nan,
              nan,        nan,        nan,        nan,        nan,
              nan,        nan,        nan,        nan,        nan,
              nan,        nan,        nan,        nan,        nan,
              nan,        nan,        nan, 0.61715296,        nan,
              nan,        nan,        nan, 0.68390805,        nan]), 'mean_test_score': array([       nan,        nan, 0.54896644,        nan,        nan,
       0.63310938, 0.63364027,        nan,        nan,        nan,
              nan,        nan, 0.59086082,        nan,        nan,
              nan, 0.5983736 ,        nan,        nan,        nan,
              nan, 0.57548148,        nan,        nan,        nan,
       0.60526781,        nan,        nan, 0.59501448,        nan,
              nan,        nan,        nan,        nan,        nan,
       0.56240033,        nan,        nan, 0.70514426,        nan,
              nan,        nan, 0.64954961,        nan,        nan,
              nan,        nan,        nan,        nan,        nan,
              nan, 0.61923247,        nan,        nan,        nan,
              nan,        nan,        nan,        nan,        nan,
              nan,        nan,        nan, 0.57875202,        nan,
              nan,        nan,        nan,        nan, 0.67049663,
              nan,        nan,        nan, 0.64132914,        nan,
              nan,        nan, 0.59439474,        nan,        nan,
       0.66572361,        nan, 0.5853807 ,        nan,        nan,
              nan,        nan,        nan,        nan, 0.69047226,
              nan,        nan, 0.6653703 ,        nan,        nan,
              nan,        nan,        nan,        nan, 0.57000175,
              nan,        nan,        nan,        nan,        nan,
              nan,        nan,        nan,        nan,        nan,
              nan,        nan, 0.5327915 ,        nan,        nan,
              nan,        nan,        nan,        nan,        nan,
              nan,        nan,        nan,        nan,        nan,
              nan,        nan,        nan,        nan,        nan,
              nan,        nan,        nan, 0.68110303,        nan,
              nan,        nan,        nan,        nan, 0.65025519,
              nan,        nan,        nan,        nan,        nan,
       0.51148992,        nan, 0.63938482,        nan,        nan,
              nan,        nan,        nan,        nan,        nan,
       0.61773066,        nan,        nan,        nan,        nan,
              nan,        nan,        nan,        nan,        nan,
              nan,        nan,        nan,        nan,        nan,
              nan, 0.5834363 ,        nan,        nan,        nan,
              nan,        nan,        nan,        nan,        nan,
              nan,        nan,        nan,        nan,        nan,
              nan,        nan,        nan,        nan,        nan,
              nan,        nan,        nan, 0.60730156,        nan,
              nan,        nan,        nan, 0.68128006,        nan]), 'std_test_score': array([       nan,        nan, 0.00844549,        nan,        nan,
       0.00578015, 0.01199475,        nan,        nan,        nan,
              nan,        nan, 0.01007651,        nan,        nan,
              nan, 0.01089726,        nan,        nan,        nan,
              nan, 0.01310855,        nan,        nan,        nan,
       0.00781727,        nan,        nan, 0.00676691,        nan,
              nan,        nan,        nan,        nan,        nan,
       0.01304899,        nan,        nan, 0.00638476,        nan,
              nan,        nan, 0.00836371,        nan,        nan,
              nan,        nan,        nan,        nan,        nan,
              nan, 0.00938596,        nan,        nan,        nan,
              nan,        nan,        nan,        nan,        nan,
              nan,        nan,        nan, 0.00586878,        nan,
              nan,        nan,        nan,        nan, 0.00973942,
              nan,        nan,        nan, 0.0032621 ,        nan,
              nan,        nan, 0.01145001,        nan,        nan,
       0.0084579 ,        nan, 0.01402931,        nan,        nan,
              nan,        nan,        nan,        nan, 0.00844635,
              nan,        nan, 0.00829868,        nan,        nan,
              nan,        nan,        nan,        nan, 0.01182775,
              nan,        nan,        nan,        nan,        nan,
              nan,        nan,        nan,        nan,        nan,
              nan,        nan, 0.00556253,        nan,        nan,
              nan,        nan,        nan,        nan,        nan,
              nan,        nan,        nan,        nan,        nan,
              nan,        nan,        nan,        nan,        nan,
              nan,        nan,        nan, 0.01052692,        nan,
              nan,        nan,        nan,        nan, 0.00806144,
              nan,        nan,        nan,        nan,        nan,
       0.01612011,        nan, 0.00877152,        nan,        nan,
              nan,        nan,        nan,        nan,        nan,
       0.01104164,        nan,        nan,        nan,        nan,
              nan,        nan,        nan,        nan,        nan,
              nan,        nan,        nan,        nan,        nan,
              nan, 0.00637361,        nan,        nan,        nan,
              nan,        nan,        nan,        nan,        nan,
              nan,        nan,        nan,        nan,        nan,
              nan,        nan,        nan,        nan,        nan,
              nan,        nan,        nan, 0.00971762,        nan,
              nan,        nan,        nan, 0.00778773,        nan]), 'rank_test_score': array([111, 135,  28, 136, 137,  13,  12, 138, 139, 140, 141, 142,  21,
       143, 144, 145,  18, 146, 147, 148, 149,  25, 150, 151, 152,  17,
       153, 134,  19, 133, 132, 121, 113, 114, 115,  27, 116, 117,   1,
       118, 119, 120,   9, 122, 131, 123, 124, 125, 126, 127, 128,  14,
       129, 130, 154, 156, 199, 157, 180, 181, 182, 183, 184,  24, 185,
       186, 187, 188, 189,   5, 190, 191, 192,  10, 193, 194, 195,  20,
       196, 197,   6, 198,  22, 179, 178, 177, 166, 158, 159,   2, 160,
       161,   7, 162, 163, 164, 165, 167, 176,  26, 168, 169, 170, 171,
       172, 173, 174, 175, 112, 155, 110,  93,  29,  43,  44,  45,  46,
        47,  48,  49,  50,  52,  61,  53,  54,  55,  56,  57,  58,  41,
        59,  60,  42,   4,  51,  37,  36,  31,  39,   8,  40,  32,  33,
        34,  35,  30,  38,  11,  62,  86,  64,  89,  90,  91,  92,  15,
        63,  94,  95,  96,  97,  99, 109, 100, 101, 102, 103, 104, 105,
       106, 107,  23, 108,  88,  98,  87,  74,  65,  66,  67,  68,  69,
        70,  71,  72,  73,  75,  85,  76,  77,  78,  79,  80,  16,  81,
        82,  83,  84,   3, 200], dtype=int32)}